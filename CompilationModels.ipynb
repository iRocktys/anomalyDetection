{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo. Abaixo, são listados os períodos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Função para criar sequências aleatórias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm1): LSTM(9, 128, num_layers=2, batch_first=True)\n",
      "  (lstm2): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (lstm3): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Configurar os parâmetros da rede LSTM\n",
    "input_size = 9         # Número de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / Nº de unidades ocultas por célula\n",
    "num_layers = 2         # Número de camadas LSTM / Nº de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 128        # Batch size / \n",
    "num_epochs = 100         # Número de epochs\n",
    "lr = 0.0011             # Learning rate\n",
    "sequence_length = 1   # Tamanho da sequência de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida\n",
    "\n",
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Criar o modelo\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.5388 Val Loss: 0.6308 Accuracy: 0.7270\n",
      "Epoch [2/100] Train Loss: 0.3928 Val Loss: 1.0530 Accuracy: 0.4878\n",
      "Epoch [3/100] Train Loss: 0.3563 Val Loss: 0.9152 Accuracy: 0.5510\n",
      "Epoch [4/100] Train Loss: 0.3443 Val Loss: 0.6847 Accuracy: 0.6159\n",
      "Epoch [5/100] Train Loss: 0.3385 Val Loss: 0.8433 Accuracy: 0.6032\n",
      "Epoch [6/100] Train Loss: 0.3312 Val Loss: 0.9238 Accuracy: 0.5672\n",
      "Epoch [7/100] Train Loss: 0.3191 Val Loss: 0.7441 Accuracy: 0.6192\n",
      "Epoch [8/100] Train Loss: 0.3183 Val Loss: 0.6064 Accuracy: 0.5786\n",
      "Epoch [9/100] Train Loss: 0.3068 Val Loss: 0.8447 Accuracy: 0.5283\n",
      "Epoch [10/100] Train Loss: 0.3045 Val Loss: 0.8962 Accuracy: 0.5107\n",
      "Epoch [11/100] Train Loss: 0.2975 Val Loss: 0.8629 Accuracy: 0.5097\n",
      "Epoch [12/100] Train Loss: 0.2902 Val Loss: 0.8187 Accuracy: 0.5219\n",
      "Epoch [13/100] Train Loss: 0.2855 Val Loss: 0.9001 Accuracy: 0.5580\n",
      "Epoch [14/100] Train Loss: 0.2860 Val Loss: 0.6318 Accuracy: 0.7244\n",
      "Epoch [15/100] Train Loss: 0.2768 Val Loss: 0.6716 Accuracy: 0.6734\n",
      "Epoch [16/100] Train Loss: 0.2753 Val Loss: 0.5958 Accuracy: 0.6902\n",
      "Epoch [17/100] Train Loss: 0.2702 Val Loss: 0.6619 Accuracy: 0.6895\n",
      "Epoch [18/100] Train Loss: 0.2695 Val Loss: 0.8215 Accuracy: 0.5929\n",
      "Epoch [19/100] Train Loss: 0.2626 Val Loss: 0.5820 Accuracy: 0.7002\n",
      "Epoch [20/100] Train Loss: 0.2613 Val Loss: 0.6787 Accuracy: 0.7737\n",
      "Epoch [21/100] Train Loss: 0.2610 Val Loss: 0.8531 Accuracy: 0.6864\n",
      "Epoch [22/100] Train Loss: 0.2599 Val Loss: 0.7170 Accuracy: 0.7098\n",
      "Epoch [23/100] Train Loss: 0.2495 Val Loss: 0.7295 Accuracy: 0.6989\n",
      "Epoch [24/100] Train Loss: 0.2472 Val Loss: 0.6038 Accuracy: 0.8034\n",
      "Melhor modelo salvo\n",
      "Epoch [25/100] Train Loss: 0.2529 Val Loss: 0.5315 Accuracy: 0.8217\n",
      "Melhor modelo salvo\n",
      "Epoch [26/100] Train Loss: 0.2428 Val Loss: 0.6034 Accuracy: 0.7945\n",
      "Epoch [27/100] Train Loss: 0.2437 Val Loss: 0.5891 Accuracy: 0.7994\n",
      "Epoch [28/100] Train Loss: 0.2381 Val Loss: 0.6170 Accuracy: 0.8034\n",
      "Epoch [29/100] Train Loss: 0.2341 Val Loss: 0.5994 Accuracy: 0.8107\n",
      "Epoch [30/100] Train Loss: 0.2356 Val Loss: 0.5859 Accuracy: 0.8041\n",
      "Epoch [31/100] Train Loss: 0.2334 Val Loss: 0.5239 Accuracy: 0.8336\n",
      "Melhor modelo salvo\n",
      "Epoch [32/100] Train Loss: 0.2302 Val Loss: 0.5843 Accuracy: 0.8282\n",
      "Epoch [33/100] Train Loss: 0.2327 Val Loss: 0.6831 Accuracy: 0.7962\n",
      "Epoch [34/100] Train Loss: 0.2351 Val Loss: 0.6568 Accuracy: 0.7959\n",
      "Epoch [35/100] Train Loss: 0.2265 Val Loss: 0.7102 Accuracy: 0.6941\n",
      "Epoch [36/100] Train Loss: 0.2262 Val Loss: 0.4677 Accuracy: 0.8456\n",
      "Melhor modelo salvo\n",
      "Epoch [37/100] Train Loss: 0.2269 Val Loss: 0.5921 Accuracy: 0.8195\n",
      "Epoch [38/100] Train Loss: 0.2298 Val Loss: 0.4946 Accuracy: 0.8310\n",
      "Epoch [39/100] Train Loss: 0.2294 Val Loss: 0.5052 Accuracy: 0.8310\n",
      "Epoch [40/100] Train Loss: 0.2261 Val Loss: 0.5585 Accuracy: 0.7968\n",
      "Epoch [41/100] Train Loss: 0.2290 Val Loss: 0.5819 Accuracy: 0.7982\n",
      "Epoch [42/100] Train Loss: 0.2253 Val Loss: 0.6614 Accuracy: 0.7367\n",
      "Epoch [43/100] Train Loss: 0.2289 Val Loss: 0.5960 Accuracy: 0.8015\n",
      "Epoch [44/100] Train Loss: 0.2271 Val Loss: 0.6353 Accuracy: 0.7697\n",
      "Epoch [45/100] Train Loss: 0.2285 Val Loss: 0.5951 Accuracy: 0.8173\n",
      "Epoch [46/100] Train Loss: 0.2228 Val Loss: 0.6179 Accuracy: 0.8059\n",
      "Epoch [47/100] Train Loss: 0.2229 Val Loss: 0.7223 Accuracy: 0.7953\n",
      "Epoch [48/100] Train Loss: 0.2290 Val Loss: 0.6350 Accuracy: 0.7991\n",
      "Epoch [49/100] Train Loss: 0.2218 Val Loss: 0.5787 Accuracy: 0.8015\n",
      "Epoch [50/100] Train Loss: 0.2254 Val Loss: 0.5669 Accuracy: 0.7737\n",
      "Epoch [51/100] Train Loss: 0.2189 Val Loss: 0.6829 Accuracy: 0.8211\n",
      "Epoch [52/100] Train Loss: 0.2247 Val Loss: 0.5985 Accuracy: 0.7744\n",
      "Epoch [53/100] Train Loss: 0.2213 Val Loss: 0.6381 Accuracy: 0.8095\n",
      "Epoch [54/100] Train Loss: 0.2224 Val Loss: 0.6111 Accuracy: 0.8008\n",
      "Epoch [55/100] Train Loss: 0.2149 Val Loss: 0.6105 Accuracy: 0.8126\n",
      "Epoch [56/100] Train Loss: 0.2217 Val Loss: 0.5863 Accuracy: 0.8025\n",
      "Epoch [57/100] Train Loss: 0.2186 Val Loss: 0.6456 Accuracy: 0.8069\n",
      "Epoch [58/100] Train Loss: 0.2275 Val Loss: 0.5672 Accuracy: 0.8199\n",
      "Epoch [59/100] Train Loss: 0.2166 Val Loss: 0.6820 Accuracy: 0.7980\n",
      "Epoch [60/100] Train Loss: 0.2172 Val Loss: 0.6838 Accuracy: 0.7864\n",
      "Epoch [61/100] Train Loss: 0.2131 Val Loss: 0.5890 Accuracy: 0.8128\n",
      "Epoch [62/100] Train Loss: 0.2225 Val Loss: 0.8193 Accuracy: 0.7351\n",
      "Epoch [63/100] Train Loss: 0.2174 Val Loss: 0.9256 Accuracy: 0.7331\n",
      "Epoch [64/100] Train Loss: 0.2132 Val Loss: 0.8786 Accuracy: 0.7450\n",
      "Epoch [65/100] Train Loss: 0.2162 Val Loss: 0.6925 Accuracy: 0.7648\n",
      "Epoch [66/100] Train Loss: 0.2138 Val Loss: 0.7195 Accuracy: 0.7583\n",
      "Epoch [67/100] Train Loss: 0.2173 Val Loss: 0.7616 Accuracy: 0.7964\n",
      "Epoch [68/100] Train Loss: 0.2147 Val Loss: 0.7350 Accuracy: 0.7800\n",
      "Epoch [69/100] Train Loss: 0.2134 Val Loss: 0.8308 Accuracy: 0.7535\n",
      "Epoch [70/100] Train Loss: 0.2101 Val Loss: 0.5554 Accuracy: 0.8246\n",
      "Epoch [71/100] Train Loss: 0.2120 Val Loss: 0.7642 Accuracy: 0.7629\n",
      "Epoch [72/100] Train Loss: 0.2120 Val Loss: 0.8301 Accuracy: 0.7621\n",
      "Epoch [73/100] Train Loss: 0.2069 Val Loss: 0.8988 Accuracy: 0.7504\n",
      "Epoch [74/100] Train Loss: 0.2101 Val Loss: 0.8322 Accuracy: 0.7658\n",
      "Epoch [75/100] Train Loss: 0.2067 Val Loss: 0.7443 Accuracy: 0.7860\n",
      "Epoch [76/100] Train Loss: 0.2136 Val Loss: 0.7854 Accuracy: 0.7554\n",
      "Epoch [77/100] Train Loss: 0.2101 Val Loss: 0.7442 Accuracy: 0.7741\n",
      "Epoch [78/100] Train Loss: 0.2097 Val Loss: 0.7644 Accuracy: 0.7647\n",
      "Epoch [79/100] Train Loss: 0.2107 Val Loss: 0.8202 Accuracy: 0.7621\n",
      "Epoch [80/100] Train Loss: 0.2126 Val Loss: 0.6102 Accuracy: 0.7900\n",
      "Epoch [81/100] Train Loss: 0.2163 Val Loss: 0.6525 Accuracy: 0.7908\n",
      "Epoch [82/100] Train Loss: 0.2126 Val Loss: 0.8085 Accuracy: 0.7772\n",
      "Epoch [83/100] Train Loss: 0.2068 Val Loss: 0.8147 Accuracy: 0.7689\n",
      "Epoch [84/100] Train Loss: 0.2039 Val Loss: 0.7197 Accuracy: 0.7725\n",
      "Epoch [85/100] Train Loss: 0.2050 Val Loss: 0.7428 Accuracy: 0.7634\n",
      "Epoch [86/100] Train Loss: 0.2058 Val Loss: 0.7888 Accuracy: 0.7728\n",
      "Epoch [87/100] Train Loss: 0.2052 Val Loss: 0.9159 Accuracy: 0.7293\n",
      "Epoch [88/100] Train Loss: 0.2052 Val Loss: 0.8545 Accuracy: 0.7605\n",
      "Epoch [89/100] Train Loss: 0.2087 Val Loss: 0.6444 Accuracy: 0.7988\n",
      "Epoch [90/100] Train Loss: 0.1982 Val Loss: 0.7909 Accuracy: 0.7752\n",
      "Epoch [91/100] Train Loss: 0.2019 Val Loss: 0.7692 Accuracy: 0.7787\n",
      "Epoch [92/100] Train Loss: 0.2043 Val Loss: 0.6632 Accuracy: 0.8056\n",
      "Epoch [93/100] Train Loss: 0.1996 Val Loss: 0.6827 Accuracy: 0.7888\n",
      "Epoch [94/100] Train Loss: 0.2028 Val Loss: 0.7449 Accuracy: 0.7710\n",
      "Epoch [95/100] Train Loss: 0.1997 Val Loss: 0.6479 Accuracy: 0.8116\n",
      "Epoch [96/100] Train Loss: 0.2050 Val Loss: 0.5850 Accuracy: 0.8173\n",
      "Epoch [97/100] Train Loss: 0.2026 Val Loss: 0.6795 Accuracy: 0.7842\n",
      "Epoch [98/100] Train Loss: 0.2024 Val Loss: 0.8095 Accuracy: 0.7695\n",
      "Epoch [99/100] Train Loss: 0.1973 Val Loss: 0.7403 Accuracy: 0.7901\n",
      "Epoch [100/100] Train Loss: 0.1985 Val Loss: 0.7741 Accuracy: 0.7861\n",
      "Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "dir_save = \"output/UEL/LSTM\"\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "best_loss = float('inf') \n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---- Treinamento ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validação ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # inputs = inputs.permute(1, 0, 2)  # se necessário\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader.dataset)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} \"\n",
    "          f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # ---- Salvar o melhor modelo ----\n",
    "    if avg_val_loss < best_loss and acc > 0.80:\n",
    "        best_loss = avg_val_loss\n",
    "        save_path = os.path.join(dir_save, f\"LSTM_Epoca-{epoch}_Acc-{acc:.2f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, save_path)\n",
    "        print(f\"Melhor modelo salvo\")\n",
    "\n",
    "print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shape: torch.Size([17144, 9, 5])\n",
      "Test Dataset Shape: torch.Size([8415, 9, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 9\n",
    "sequence_length = 5\n",
    "output_size = 2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "column_to_remove = 'attack_name'\n",
    "\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Modelo\n",
    "in_ch  = train_dataset.sequences.shape[1]   \n",
    "length = train_dataset.sequences.shape[2]\n",
    "model = CNN(input_channels=in_ch, input_length=length, num_classes=output_size).to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs [1] Train Loss: 0.3576 Val Loss: 0.7702 Accuracy: 0.7051\n",
      "num_epochs [2] Train Loss: 0.3576 Val Loss: 1.0510 Accuracy: 0.6134\n",
      "num_epochs [3] Train Loss: 0.3576 Val Loss: 1.1179 Accuracy: 0.6330\n",
      "num_epochs [4] Train Loss: 0.3576 Val Loss: 0.8157 Accuracy: 0.7176\n",
      "num_epochs [5] Train Loss: 0.3576 Val Loss: 0.9511 Accuracy: 0.7144\n",
      "num_epochs [6] Train Loss: 0.3576 Val Loss: 0.9229 Accuracy: 0.6942\n",
      "num_epochs [7] Train Loss: 0.3576 Val Loss: 0.5452 Accuracy: 0.8007\n",
      "Melhor modelo salvo\n",
      "num_epochs [8] Train Loss: 0.3576 Val Loss: 1.1932 Accuracy: 0.6270\n",
      "num_epochs [9] Train Loss: 0.3576 Val Loss: 0.8717 Accuracy: 0.7630\n",
      "num_epochs [10] Train Loss: 0.3576 Val Loss: 1.4044 Accuracy: 0.6585\n",
      "num_epochs [11] Train Loss: 0.3576 Val Loss: 3.8234 Accuracy: 0.4597\n",
      "num_epochs [12] Train Loss: 0.3576 Val Loss: 0.9889 Accuracy: 0.7616\n",
      "num_epochs [13] Train Loss: 0.3576 Val Loss: 1.6415 Accuracy: 0.6485\n",
      "num_epochs [14] Train Loss: 0.3576 Val Loss: 2.0549 Accuracy: 0.6147\n",
      "num_epochs [15] Train Loss: 0.3576 Val Loss: 1.7502 Accuracy: 0.6655\n",
      "num_epochs [16] Train Loss: 0.3576 Val Loss: 2.6815 Accuracy: 0.5469\n",
      "num_epochs [17] Train Loss: 0.3576 Val Loss: 1.1204 Accuracy: 0.7588\n",
      "num_epochs [18] Train Loss: 0.3576 Val Loss: 2.2956 Accuracy: 0.6265\n",
      "num_epochs [19] Train Loss: 0.3576 Val Loss: 2.1913 Accuracy: 0.6187\n",
      "num_epochs [20] Train Loss: 0.3576 Val Loss: 0.8827 Accuracy: 0.7779\n",
      "num_epochs [21] Train Loss: 0.3576 Val Loss: 1.8462 Accuracy: 0.6431\n",
      "num_epochs [22] Train Loss: 0.3576 Val Loss: 1.8877 Accuracy: 0.6551\n",
      "num_epochs [23] Train Loss: 0.3576 Val Loss: 2.4207 Accuracy: 0.5666\n",
      "num_epochs [24] Train Loss: 0.3576 Val Loss: 2.0236 Accuracy: 0.6181\n",
      "num_epochs [25] Train Loss: 0.3576 Val Loss: 1.9134 Accuracy: 0.6587\n",
      "num_epochs [26] Train Loss: 0.3576 Val Loss: 1.3708 Accuracy: 0.7813\n",
      "num_epochs [27] Train Loss: 0.3576 Val Loss: 3.9837 Accuracy: 0.5051\n",
      "num_epochs [28] Train Loss: 0.3576 Val Loss: 2.1952 Accuracy: 0.6286\n",
      "num_epochs [29] Train Loss: 0.3576 Val Loss: 0.5826 Accuracy: 0.8638\n",
      "num_epochs [30] Train Loss: 0.3576 Val Loss: 3.5461 Accuracy: 0.5718\n",
      "num_epochs [31] Train Loss: 0.3576 Val Loss: 2.0784 Accuracy: 0.6610\n",
      "num_epochs [32] Train Loss: 0.3576 Val Loss: 3.0549 Accuracy: 0.5944\n",
      "num_epochs [33] Train Loss: 0.3576 Val Loss: 2.0750 Accuracy: 0.6727\n",
      "num_epochs [34] Train Loss: 0.3576 Val Loss: 0.7260 Accuracy: 0.8575\n",
      "num_epochs [35] Train Loss: 0.3576 Val Loss: 2.3444 Accuracy: 0.6385\n",
      "num_epochs [36] Train Loss: 0.3576 Val Loss: 0.8839 Accuracy: 0.7939\n",
      "num_epochs [37] Train Loss: 0.3576 Val Loss: 3.2321 Accuracy: 0.5827\n",
      "num_epochs [38] Train Loss: 0.3576 Val Loss: 0.7058 Accuracy: 0.8652\n",
      "num_epochs [39] Train Loss: 0.3576 Val Loss: 1.6086 Accuracy: 0.7234\n",
      "num_epochs [40] Train Loss: 0.3576 Val Loss: 1.3145 Accuracy: 0.7851\n",
      "num_epochs [41] Train Loss: 0.3576 Val Loss: 3.9126 Accuracy: 0.5623\n",
      "num_epochs [42] Train Loss: 0.3576 Val Loss: 1.1854 Accuracy: 0.7925\n",
      "num_epochs [43] Train Loss: 0.3576 Val Loss: 2.1291 Accuracy: 0.6890\n",
      "num_epochs [44] Train Loss: 0.3576 Val Loss: 2.8088 Accuracy: 0.6458\n",
      "num_epochs [45] Train Loss: 0.3576 Val Loss: 0.8004 Accuracy: 0.8404\n",
      "num_epochs [46] Train Loss: 0.3576 Val Loss: 3.0018 Accuracy: 0.6006\n",
      "num_epochs [47] Train Loss: 0.3576 Val Loss: 3.0566 Accuracy: 0.6822\n",
      "num_epochs [48] Train Loss: 0.3576 Val Loss: 2.0696 Accuracy: 0.7106\n",
      "num_epochs [49] Train Loss: 0.3576 Val Loss: 2.6765 Accuracy: 0.6676\n",
      "num_epochs [50] Train Loss: 0.3576 Val Loss: 3.0011 Accuracy: 0.6217\n",
      "Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "dir_save = \"output/UEL/CNN\"\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "best_loss = float('inf') \n",
    "\n",
    "for num_epochs in range(1, num_epochs+1):\n",
    "    # Treino\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss, preds, trues = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            val_loss += criterion(out, y).item() * x.size(0)\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc  = accuracy_score(trues, preds)\n",
    "\n",
    "    print(f\"num_epochs [{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} \"\n",
    "          f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Salvar o melhor modelo ----\n",
    "    if val_loss < best_loss and val_acc > 0.80:\n",
    "        best_loss = val_loss\n",
    "        save_path = os.path.join(dir_save, f\"CNN_Epoca-{num_epochs}_Acc-{val_acc:.2f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, save_path)\n",
    "        print(f\"Melhor modelo salvo\")\n",
    "\n",
    "print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label', 'attack_name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 155\u001b[0m\n\u001b[0;32m    152\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 1. Train CNN-LSTM on train set\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m model, train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 2. Train SVM on train features\u001b[39;00m\n\u001b[0;32m    166\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m train_svm_on_features(model, train_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[186], line 76\u001b[0m, in \u001b[0;36mtrain_cnn_lstm\u001b[1;34m(train_df_path, test_df_path, seq_len, batch_size, epochs, lr, device)\u001b[0m\n\u001b[0;32m     73\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_df_path)\n\u001b[0;32m     74\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_df_path)\n\u001b[1;32m---> 76\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mFlowSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m FlowSequenceDataset(test_df, seq_len)\n\u001b[0;32m     78\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[186], line 18\u001b[0m, in \u001b[0;36mFlowSequenceDataset.__init__\u001b[1;34m(self, df, seq_len)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame, seq_len: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Assumes df sorted by time, with label column\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m=\u001b[39m seq_len\n\u001b[1;32m---> 18\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattack_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     19\u001b[0m     labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     20\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['label', 'attack_name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sequence_dataset import FlowSequenceDataset\n",
    "from hybrid_model import train_feature_extractor, extract_features, train_svm, evaluate_hybrid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configurações\n",
    "device   = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_csv = \"path/to/train_data.csv\"\n",
    "test_csv  = \"path/to/test_data.csv\"\n",
    "\n",
    "# Passo 1: treina o extrator CNN-LSTM\n",
    "extractor, train_loader = train_feature_extractor(\n",
    "    train_csv=train_csv,\n",
    "    seq_len=10,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    lr=1e-3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Passo 2: treina o SVM sobre features extraídas\n",
    "dados_X, dados_y = extract_features(extractor, train_loader, device)\n",
    "svm_model = train_svm(dados_X, dados_y)\n",
    "\n",
    "# Passo 3: avalia no conjunto de teste test_csv\n",
    "test_ds     = FlowSequenceDataset(test_csv, seq_len=10)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "evaluate_hybrid(extractor, test_loader, svm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
