{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntroduÃ§Ã£o aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contÃ©m mÃºltiplos cenÃ¡rios de ataques, registrados em arquivos CSV, com detalhes sobre trÃ¡fego malicioso e legÃ­timo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrÃ©-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequÃªncias aleatÃ³rias!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. FunÃ§Ã£o para criar sequÃªncias aleatÃ³rias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequÃªncias aleatÃ³rias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.LSTM.ModelLSTM import LSTM\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.LSTM.TrainerLSTM import TrainerLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Configurar os parÃ¢metros da rede LSTM\n",
    "input_size = 9         # NÃºmero de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / NÂº de unidades ocultas por cÃ©lula\n",
    "num_layers = 3         # NÃºmero de camadas LSTM / NÂº de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 128        # Batch size / \n",
    "sequence_length = 10   # Tamanho da sequÃªncia de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17139\n",
      "Total de amostras no conjunto de teste: 8410\n",
      "Train Dataset Shape: torch.Size([17139, 10, 9])\n",
      "Test Dataset Shape: torch.Size([8410, 10, 9])\n"
     ]
    }
   ],
   "source": [
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove=column_to_remove, normalize=True, mode='lstm')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Criar o modelo\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.4143 Val Loss:   0.3635 Accuracy:   0.8725\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [2/100] Train Loss: 0.1607 Val Loss:   0.3591 Accuracy:   0.8926\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [3/100] Train Loss: 0.1231 Val Loss:   0.3107 Accuracy:   0.9001\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [4/100] Train Loss: 0.0989 Val Loss:   0.2797 Accuracy:   0.8907\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [5/100] Train Loss: 0.0911 Val Loss:   0.4735 Accuracy:   0.8895\n",
      "Epoch [6/100] Train Loss: 0.0964 Val Loss:   0.2631 Accuracy:   0.9043\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [7/100] Train Loss: 0.0857 Val Loss:   0.3939 Accuracy:   0.8484\n",
      "Epoch [8/100] Train Loss: 0.0873 Val Loss:   0.2725 Accuracy:   0.8898\n",
      "Epoch [9/100] Train Loss: 0.0873 Val Loss:   0.2713 Accuracy:   0.9100\n",
      "Epoch [10/100] Train Loss: 0.0909 Val Loss:   0.2565 Accuracy:   0.8930\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [11/100] Train Loss: 0.0752 Val Loss:   0.2610 Accuracy:   0.9074\n",
      "Epoch [12/100] Train Loss: 0.0775 Val Loss:   0.2517 Accuracy:   0.9127\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [13/100] Train Loss: 0.0727 Val Loss:   0.2843 Accuracy:   0.8961\n",
      "Epoch [14/100] Train Loss: 0.0756 Val Loss:   0.2700 Accuracy:   0.9033\n",
      "Epoch [15/100] Train Loss: 0.0739 Val Loss:   0.2420 Accuracy:   0.9090\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [16/100] Train Loss: 0.0765 Val Loss:   0.2673 Accuracy:   0.9078\n",
      "Epoch [17/100] Train Loss: 0.0742 Val Loss:   0.3314 Accuracy:   0.8807\n",
      "Epoch [18/100] Train Loss: 0.0792 Val Loss:   0.2696 Accuracy:   0.9118\n",
      "Epoch [19/100] Train Loss: 0.0740 Val Loss:   0.2456 Accuracy:   0.9042\n",
      "Epoch [20/100] Train Loss: 0.0780 Val Loss:   0.3812 Accuracy:   0.8981\n",
      "Epoch [21/100] Train Loss: 0.0688 Val Loss:   0.2402 Accuracy:   0.9164\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [22/100] Train Loss: 0.0682 Val Loss:   0.2660 Accuracy:   0.9149\n",
      "Epoch [23/100] Train Loss: 0.0665 Val Loss:   0.2856 Accuracy:   0.8973\n",
      "Epoch [24/100] Train Loss: 0.0775 Val Loss:   0.2314 Accuracy:   0.9119\n",
      "ðŸ”– Melhor modelo salvo!\n",
      "Epoch [25/100] Train Loss: 0.0670 Val Loss:   0.3108 Accuracy:   0.8987\n",
      "Epoch [26/100] Train Loss: 0.0689 Val Loss:   0.2468 Accuracy:   0.9151\n",
      "Epoch [27/100] Train Loss: 0.0673 Val Loss:   0.2877 Accuracy:   0.8860\n",
      "Epoch [28/100] Train Loss: 0.0639 Val Loss:   0.3360 Accuracy:   0.8700\n",
      "Epoch [29/100] Train Loss: 0.0579 Val Loss:   0.2506 Accuracy:   0.9108\n",
      "Epoch [30/100] Train Loss: 0.0593 Val Loss:   0.3183 Accuracy:   0.9021\n",
      "Epoch [31/100] Train Loss: 0.0583 Val Loss:   0.2439 Accuracy:   0.9064\n",
      "Epoch [32/100] Train Loss: 0.0535 Val Loss:   0.2950 Accuracy:   0.9093\n",
      "Epoch [33/100] Train Loss: 0.0565 Val Loss:   0.2550 Accuracy:   0.9106\n",
      "Epoch [34/100] Train Loss: 0.0559 Val Loss:   0.3880 Accuracy:   0.8694\n",
      "Epoch [35/100] Train Loss: 0.0562 Val Loss:   0.2846 Accuracy:   0.9168\n",
      "Epoch [36/100] Train Loss: 0.0528 Val Loss:   0.2605 Accuracy:   0.9155\n",
      "Epoch [37/100] Train Loss: 0.0559 Val Loss:   0.2615 Accuracy:   0.9122\n",
      "Epoch [38/100] Train Loss: 0.0555 Val Loss:   0.3790 Accuracy:   0.9005\n",
      "Epoch [39/100] Train Loss: 0.0501 Val Loss:   0.2864 Accuracy:   0.8948\n",
      "Epoch [40/100] Train Loss: 0.0478 Val Loss:   0.3286 Accuracy:   0.9076\n",
      "Epoch [41/100] Train Loss: 0.0475 Val Loss:   0.2854 Accuracy:   0.8964\n",
      "Epoch [42/100] Train Loss: 0.0458 Val Loss:   0.2498 Accuracy:   0.9233\n",
      "Epoch [43/100] Train Loss: 0.0511 Val Loss:   0.2740 Accuracy:   0.9045\n",
      "Epoch [44/100] Train Loss: 0.0467 Val Loss:   0.2770 Accuracy:   0.9150\n",
      "Epoch [45/100] Train Loss: 0.0463 Val Loss:   0.2758 Accuracy:   0.9206\n",
      "Epoch [46/100] Train Loss: 0.0435 Val Loss:   0.3112 Accuracy:   0.9043\n",
      "Epoch [47/100] Train Loss: 0.0409 Val Loss:   0.2945 Accuracy:   0.9219\n",
      "Epoch [48/100] Train Loss: 0.0485 Val Loss:   0.3155 Accuracy:   0.9146\n",
      "Epoch [49/100] Train Loss: 0.0410 Val Loss:   0.3210 Accuracy:   0.9057\n",
      "Epoch [50/100] Train Loss: 0.0451 Val Loss:   0.2645 Accuracy:   0.9181\n",
      "Epoch [51/100] Train Loss: 0.0382 Val Loss:   0.2908 Accuracy:   0.9133\n",
      "Epoch [52/100] Train Loss: 0.0379 Val Loss:   0.3492 Accuracy:   0.9151\n",
      "Epoch [53/100] Train Loss: 0.0359 Val Loss:   0.2920 Accuracy:   0.9120\n",
      "Epoch [54/100] Train Loss: 0.0401 Val Loss:   0.3510 Accuracy:   0.9080\n",
      "Epoch [55/100] Train Loss: 0.0399 Val Loss:   0.2807 Accuracy:   0.9226\n",
      "Epoch [56/100] Train Loss: 0.0360 Val Loss:   0.3267 Accuracy:   0.9077\n",
      "Epoch [57/100] Train Loss: 0.0373 Val Loss:   0.3068 Accuracy:   0.9162\n",
      "Epoch [58/100] Train Loss: 0.0355 Val Loss:   0.3315 Accuracy:   0.9175\n",
      "Epoch [59/100] Train Loss: 0.0305 Val Loss:   0.3241 Accuracy:   0.9196\n",
      "Epoch [60/100] Train Loss: 0.0280 Val Loss:   0.3741 Accuracy:   0.9159\n",
      "Epoch [61/100] Train Loss: 0.0342 Val Loss:   0.4321 Accuracy:   0.8917\n",
      "Epoch [62/100] Train Loss: 0.0392 Val Loss:   0.3009 Accuracy:   0.9206\n",
      "Epoch [63/100] Train Loss: 0.0289 Val Loss:   0.3414 Accuracy:   0.9137\n",
      "Epoch [64/100] Train Loss: 0.0286 Val Loss:   0.3834 Accuracy:   0.8932\n",
      "Epoch [65/100] Train Loss: 0.0249 Val Loss:   0.3926 Accuracy:   0.9064\n",
      "Epoch [66/100] Train Loss: 0.0268 Val Loss:   0.3995 Accuracy:   0.8941\n",
      "Epoch [67/100] Train Loss: 0.0221 Val Loss:   0.3355 Accuracy:   0.9165\n",
      "Epoch [68/100] Train Loss: 0.0274 Val Loss:   0.3862 Accuracy:   0.9057\n",
      "Epoch [69/100] Train Loss: 0.0238 Val Loss:   0.4770 Accuracy:   0.8697\n",
      "Epoch [70/100] Train Loss: 0.0258 Val Loss:   0.3735 Accuracy:   0.8969\n",
      "Epoch [71/100] Train Loss: 0.0224 Val Loss:   0.3459 Accuracy:   0.9121\n",
      "Epoch [72/100] Train Loss: 0.0243 Val Loss:   0.3723 Accuracy:   0.9171\n",
      "Epoch [73/100] Train Loss: 0.0261 Val Loss:   0.3642 Accuracy:   0.8863\n",
      "Epoch [74/100] Train Loss: 0.0302 Val Loss:   0.3420 Accuracy:   0.9165\n",
      "Epoch [75/100] Train Loss: 0.0182 Val Loss:   0.4696 Accuracy:   0.9090\n",
      "Epoch [76/100] Train Loss: 0.0293 Val Loss:   0.4105 Accuracy:   0.8931\n",
      "Epoch [77/100] Train Loss: 0.0244 Val Loss:   0.3619 Accuracy:   0.9131\n",
      "Epoch [78/100] Train Loss: 0.0147 Val Loss:   0.3828 Accuracy:   0.9189\n",
      "Epoch [79/100] Train Loss: 0.0151 Val Loss:   0.3506 Accuracy:   0.9239\n",
      "Epoch [80/100] Train Loss: 0.0186 Val Loss:   0.3567 Accuracy:   0.9147\n",
      "Epoch [81/100] Train Loss: 0.0120 Val Loss:   0.4150 Accuracy:   0.9190\n",
      "Epoch [82/100] Train Loss: 0.0138 Val Loss:   0.4675 Accuracy:   0.9118\n",
      "Epoch [83/100] Train Loss: 0.0212 Val Loss:   0.4207 Accuracy:   0.9137\n",
      "Epoch [84/100] Train Loss: 0.0157 Val Loss:   0.4640 Accuracy:   0.9062\n",
      "Epoch [85/100] Train Loss: 0.0208 Val Loss:   0.4428 Accuracy:   0.9117\n",
      "Epoch [86/100] Train Loss: 0.0277 Val Loss:   0.8959 Accuracy:   0.8673\n",
      "Epoch [87/100] Train Loss: 0.0426 Val Loss:   0.3018 Accuracy:   0.9169\n",
      "Epoch [88/100] Train Loss: 0.0114 Val Loss:   0.3735 Accuracy:   0.9240\n",
      "Epoch [89/100] Train Loss: 0.0129 Val Loss:   0.3879 Accuracy:   0.9162\n",
      "Epoch [90/100] Train Loss: 0.0162 Val Loss:   0.6334 Accuracy:   0.8883\n",
      "Epoch [91/100] Train Loss: 0.0338 Val Loss:   0.3934 Accuracy:   0.9138\n",
      "Epoch [92/100] Train Loss: 0.0124 Val Loss:   0.3653 Accuracy:   0.9080\n",
      "Epoch [93/100] Train Loss: 0.0143 Val Loss:   0.3859 Accuracy:   0.9161\n",
      "Epoch [94/100] Train Loss: 0.0145 Val Loss:   0.3890 Accuracy:   0.9228\n",
      "Epoch [95/100] Train Loss: 0.0116 Val Loss:   0.4495 Accuracy:   0.9069\n",
      "Epoch [96/100] Train Loss: 0.0127 Val Loss:   0.4295 Accuracy:   0.9156\n",
      "Epoch [97/100] Train Loss: 0.0087 Val Loss:   0.4397 Accuracy:   0.9145\n",
      "Epoch [98/100] Train Loss: 0.0610 Val Loss:   0.3068 Accuracy:   0.9166\n",
      "Epoch [99/100] Train Loss: 0.0123 Val Loss:   0.3736 Accuracy:   0.9168\n",
      "Epoch [100/100] Train Loss: 0.0121 Val Loss:   0.3907 Accuracy:   0.9139\n",
      "âœ… Treinamento concluÃ­do.\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerLSTM(dir_save=\"output/LSTM\", num_epochs=100)\n",
    "trainer.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.CNN import CNN\n",
    "from models.Sequence import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "input_size = 9\n",
    "sequence_length = 70\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "column_to_remove = 'attack_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17079\n",
      "Total de amostras no conjunto de teste: 8350\n",
      "Train Dataset Shape: torch.Size([17079, 9, 70])\n",
      "Test Dataset Shape: torch.Size([8350, 9, 70])\n",
      "Total de batches no conjunto de treino: 267\n",
      "Total de batches no conjunto de teste: 131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=17920, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Modelo\n",
    "n_feat = train_dataset.sequences.shape[1]\n",
    "model = CNN(\n",
    "    input_channels=n_feat,\n",
    "    input_length=sequence_length,\n",
    "    num_classes=output_size\n",
    ").to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.1187\n",
      "Epoch 2/20 - Loss: 0.0512\n",
      "Epoch 3/20 - Loss: 0.0381\n",
      "Epoch 4/20 - Loss: 0.0284\n",
      "Epoch 5/20 - Loss: 0.0195\n",
      "Epoch 6/20 - Loss: 0.0175\n",
      "Epoch 7/20 - Loss: 0.0139\n",
      "Epoch 8/20 - Loss: 0.0104\n",
      "Epoch 9/20 - Loss: 0.0119\n",
      "Epoch 10/20 - Loss: 0.0090\n",
      "Epoch 11/20 - Loss: 0.0113\n",
      "Epoch 12/20 - Loss: 0.0066\n",
      "Epoch 13/20 - Loss: 0.0086\n",
      "Epoch 14/20 - Loss: 0.0047\n",
      "Epoch 15/20 - Loss: 0.0032\n",
      "Epoch 16/20 - Loss: 0.0089\n",
      "Epoch 17/20 - Loss: 0.0072\n",
      "Epoch 18/20 - Loss: 0.0053\n",
      "Epoch 19/20 - Loss: 0.0042\n",
      "Epoch 20/20 - Loss: 0.0046\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      4965\n",
      "           1       0.95      0.90      0.92      3385\n",
      "\n",
      "    accuracy                           0.94      8350\n",
      "   macro avg       0.94      0.93      0.93      8350\n",
      "weighted avg       0.94      0.94      0.94      8350\n",
      "\n",
      "Accuracy: 0.9377245508982036\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso CNN\n",
    "# treinar\n",
    "model.train_model(train_loader, device=device, epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar\n",
    "model.evaluate(test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17099\n",
      "Total de amostras no conjunto de teste:  8370\n",
      "Train Dataset Shape: torch.Size([17099, 50, 9])\n",
      "Test  Dataset Shape: torch.Size([8370, 50, 9])\n",
      "Total de batches no treino: 268\n",
      "Total de batches no teste:  131\n",
      "ModelHybridAttnSVM(\n",
      "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(128, 64, num_layers=3, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.Hybrid.ModelHybridAttnSVM import ModelHybridAttnSVM\n",
    "from torch.utils.data import DataLoader\n",
    "from models.Sequence import SequenceDataset\n",
    "import torch\n",
    "\n",
    "# ParÃ¢metros gerais\n",
    "sequence_length   = 50\n",
    "column_to_remove  = 'attack_name'\n",
    "batch_size        = 64\n",
    "hidden_size       = 64\n",
    "num_layers        = 3\n",
    "num_classes       = 2\n",
    "device            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs            = 100\n",
    "learning_rate     = 1e-4\n",
    "\n",
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste:  {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test  Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no teste:  {len(test_loader)}\")\n",
    "\n",
    "# Definir n_features a partir do dataset\n",
    "n_features = train_dataset.sequences.shape[2]\n",
    "\n",
    "# Instanciar o modelo hÃ­brido\n",
    "model = ModelHybridAttnSVM(\n",
    "    seq_len     = sequence_length,\n",
    "    n_features  = n_features,\n",
    "    lstm_hidden = hidden_size,\n",
    "    lstm_layers = num_layers,\n",
    "    num_classes = num_classes\n",
    ").to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Treinando CNN+LSTM ###\n",
      "Epoch 1/100 - Loss: 0.5094\n",
      "Epoch 2/100 - Loss: 0.1693\n",
      "Epoch 3/100 - Loss: 0.1238\n",
      "Epoch 4/100 - Loss: 0.1104\n",
      "Epoch 5/100 - Loss: 0.1021\n",
      "Epoch 6/100 - Loss: 0.0973\n",
      "Epoch 7/100 - Loss: 0.0935\n",
      "Epoch 8/100 - Loss: 0.0871\n",
      "Epoch 9/100 - Loss: 0.0841\n",
      "Epoch 10/100 - Loss: 0.0828\n",
      "Epoch 11/100 - Loss: 0.0799\n",
      "Epoch 12/100 - Loss: 0.0743\n",
      "Epoch 13/100 - Loss: 0.0736\n",
      "Epoch 14/100 - Loss: 0.0723\n",
      "Epoch 15/100 - Loss: 0.0697\n",
      "Epoch 16/100 - Loss: 0.0683\n",
      "Epoch 17/100 - Loss: 0.0664\n",
      "Epoch 18/100 - Loss: 0.0668\n",
      "Epoch 19/100 - Loss: 0.0656\n",
      "Epoch 20/100 - Loss: 0.0629\n",
      "Epoch 21/100 - Loss: 0.0624\n",
      "Epoch 22/100 - Loss: 0.0608\n",
      "Epoch 23/100 - Loss: 0.0598\n",
      "Epoch 24/100 - Loss: 0.0604\n",
      "Epoch 25/100 - Loss: 0.0587\n",
      "Epoch 26/100 - Loss: 0.0586\n",
      "Epoch 27/100 - Loss: 0.0566\n",
      "Epoch 28/100 - Loss: 0.0559\n",
      "Epoch 29/100 - Loss: 0.0554\n",
      "Epoch 30/100 - Loss: 0.0541\n",
      "Epoch 31/100 - Loss: 0.0520\n",
      "Epoch 32/100 - Loss: 0.0527\n",
      "Epoch 33/100 - Loss: 0.0517\n",
      "Epoch 34/100 - Loss: 0.0511\n",
      "Epoch 35/100 - Loss: 0.0483\n",
      "Epoch 36/100 - Loss: 0.0493\n",
      "Epoch 37/100 - Loss: 0.0487\n",
      "Epoch 38/100 - Loss: 0.0480\n",
      "Epoch 39/100 - Loss: 0.0460\n",
      "Epoch 40/100 - Loss: 0.0450\n",
      "Epoch 41/100 - Loss: 0.0447\n",
      "Epoch 42/100 - Loss: 0.0438\n",
      "Epoch 43/100 - Loss: 0.0441\n",
      "Epoch 44/100 - Loss: 0.0418\n",
      "Epoch 45/100 - Loss: 0.0419\n",
      "Epoch 46/100 - Loss: 0.0421\n",
      "Epoch 47/100 - Loss: 0.0408\n",
      "Epoch 48/100 - Loss: 0.0412\n",
      "Epoch 49/100 - Loss: 0.0401\n",
      "Epoch 50/100 - Loss: 0.0413\n",
      "Epoch 51/100 - Loss: 0.0387\n",
      "Epoch 52/100 - Loss: 0.0384\n",
      "Epoch 53/100 - Loss: 0.0384\n",
      "Epoch 54/100 - Loss: 0.0361\n",
      "Epoch 55/100 - Loss: 0.0362\n",
      "Epoch 56/100 - Loss: 0.0360\n",
      "Epoch 57/100 - Loss: 0.0348\n",
      "Epoch 58/100 - Loss: 0.0337\n",
      "Epoch 59/100 - Loss: 0.0328\n",
      "Epoch 60/100 - Loss: 0.0356\n",
      "Epoch 61/100 - Loss: 0.0351\n",
      "Epoch 62/100 - Loss: 0.0325\n",
      "Epoch 63/100 - Loss: 0.0315\n",
      "Epoch 64/100 - Loss: 0.0322\n",
      "Epoch 65/100 - Loss: 0.0308\n",
      "Epoch 66/100 - Loss: 0.0315\n",
      "Epoch 67/100 - Loss: 0.0322\n",
      "Epoch 68/100 - Loss: 0.0296\n",
      "Epoch 69/100 - Loss: 0.0286\n",
      "Epoch 70/100 - Loss: 0.0284\n",
      "Epoch 71/100 - Loss: 0.0282\n",
      "Epoch 72/100 - Loss: 0.0264\n",
      "Epoch 73/100 - Loss: 0.0273\n",
      "Epoch 74/100 - Loss: 0.0257\n",
      "Epoch 75/100 - Loss: 0.0267\n",
      "Epoch 76/100 - Loss: 0.0255\n",
      "Epoch 77/100 - Loss: 0.0263\n",
      "Epoch 78/100 - Loss: 0.0254\n",
      "Epoch 79/100 - Loss: 0.0248\n",
      "Epoch 80/100 - Loss: 0.0230\n",
      "Epoch 81/100 - Loss: 0.0237\n",
      "Epoch 82/100 - Loss: 0.0253\n",
      "Epoch 83/100 - Loss: 0.0230\n",
      "Epoch 84/100 - Loss: 0.0223\n",
      "Epoch 85/100 - Loss: 0.0225\n",
      "Epoch 86/100 - Loss: 0.0215\n",
      "Epoch 87/100 - Loss: 0.0217\n",
      "Epoch 88/100 - Loss: 0.0216\n",
      "Epoch 89/100 - Loss: 0.0207\n",
      "Epoch 90/100 - Loss: 0.0193\n",
      "Epoch 91/100 - Loss: 0.0189\n",
      "Epoch 92/100 - Loss: 0.0213\n",
      "Epoch 93/100 - Loss: 0.0199\n",
      "Epoch 94/100 - Loss: 0.0198\n",
      "Epoch 95/100 - Loss: 0.0195\n",
      "Epoch 96/100 - Loss: 0.0185\n",
      "Epoch 97/100 - Loss: 0.0209\n",
      "Epoch 98/100 - Loss: 0.0181\n",
      "Epoch 99/100 - Loss: 0.0180\n",
      "Epoch 100/100 - Loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelHybridAttnSVM(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(128, 64, num_layers=3, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento\n",
    "print(\"### Treinando CNN+LSTM ###\")\n",
    "model.train_model(\n",
    "    train_loader,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    lr=learning_rate,\n",
    "    save_path='output/Hybrid/hybrid_attn.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Treinando PCA + SVM ###\n",
      "PCA salvo em output/Hybrid/pca.joblib\n",
      "SVM salvo em output/Hybrid/hybrid_svm.joblib\n",
      "\n",
      "### Avaliando Modelo HÃ­brido no Conjunto de Teste ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      4965\n",
      "           1       0.90      0.90      0.90      3405\n",
      "\n",
      "    accuracy                           0.92      8370\n",
      "   macro avg       0.92      0.92      0.92      8370\n",
      "weighted avg       0.92      0.92      0.92      8370\n",
      "\n",
      "Accuracy: 0.9200716845878136\n"
     ]
    }
   ],
   "source": [
    "# 2) Treinar PCA + SVM sobre as features extraÃ­das\n",
    "print(\"\\n### Treinando PCA + SVM ###\")\n",
    "svm = model.train_svm(\n",
    "    train_loader,\n",
    "    pca_path='output/Hybrid/pca.joblib',\n",
    "    svm_path='output/Hybrid/hybrid_svm.joblib'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Avaliar todo o pipeline (CNN+LSTM â†’ PCA â†’ SVM) no conjunto de teste\n",
    "print(\"\\n### Avaliando Modelo HÃ­brido no Conjunto de Teste ###\")\n",
    "model.evaluate(\n",
    "    test_loader,\n",
    "    pca_path='output/Hybrid/pca.joblib',\n",
    "    svm_path='output/Hybrid/hybrid_svm.joblib'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
