{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Função para criar sequências aleatórias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Total de amostras no treino: 17139\n",
      "Total de amostras no teste:  8410\n",
      "Train Shape: torch.Size([17139, 10, 9])\n",
      "Test  Shape: torch.Size([8410, 10, 9])\n",
      "Batches treino: 134, teste: 66\n",
      "LSTM(\n",
      "  (lstm1): LSTM(9, 128, num_layers=3, batch_first=True)\n",
      "  (lstm2): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (lstm3): LSTM(256, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.LSTM import LSTM\n",
    "from models.Sequence import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Configurações gerais\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Parâmetros do dataset e modelo\n",
    "input_size      = 9\n",
    "hidden_size     = 256\n",
    "num_layers      = 3\n",
    "output_size     = 2\n",
    "batch_size      = 128\n",
    "sequence_length = 10\n",
    "column_to_remove= 'attack_name'\n",
    "\n",
    "# 1) Criar os datasets\n",
    "train_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "\n",
    "print(f\"Total de amostras no treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no teste:  {len(test_dataset)}\")\n",
    "print(\"Train Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test  Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# 2) DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "print(f\"Batches treino: {len(train_loader)}, teste: {len(test_loader)}\")\n",
    "\n",
    "# 3) Instanciar e treinar\n",
    "model = LSTM(input_size=input_size,\n",
    "             hidden_size=hidden_size,\n",
    "             num_layers=num_layers,\n",
    "             output_size=output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 – Loss: 0.4092\n",
      "Epoch 2/10 – Loss: 0.1853\n",
      "Epoch 3/10 – Loss: 0.1209\n",
      "Epoch 4/10 – Loss: 0.0983\n",
      "Epoch 5/10 – Loss: 0.0942\n",
      "Epoch 6/10 – Loss: 0.0870\n",
      "Epoch 7/10 – Loss: 0.0827\n",
      "Epoch 8/10 – Loss: 0.0824\n",
      "Epoch 9/10 – Loss: 0.0859\n",
      "Epoch 10/10 – Loss: 0.0801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm1): LSTM(9, 128, num_layers=3, batch_first=True)\n",
       "  (lstm2): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (lstm3): LSTM(256, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treina e salva\n",
    "model.train_model(\n",
    "    train_loader,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    save_path='output/LSTM/lstm.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87      4991\n",
      "           1       0.76      0.92      0.84      3419\n",
      "\n",
      "    accuracy                           0.85      8410\n",
      "   macro avg       0.85      0.86      0.85      8410\n",
      "weighted avg       0.87      0.85      0.85      8410\n",
      "\n",
      "Accuracy: 0.8530321046373365\n"
     ]
    }
   ],
   "source": [
    "# Avalia\n",
    "model.evaluate(test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.CNN import CNN\n",
    "from models.Sequence import SequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "input_size = 9\n",
    "sequence_length = 70\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "column_to_remove = 'attack_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17079\n",
      "Total de amostras no conjunto de teste: 8350\n",
      "Train Dataset Shape: torch.Size([17079, 9, 70])\n",
      "Test Dataset Shape: torch.Size([8350, 9, 70])\n",
      "Total de batches no conjunto de treino: 267\n",
      "Total de batches no conjunto de teste: 131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=17920, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Modelo\n",
    "n_feat = train_dataset.sequences.shape[1]\n",
    "model = CNN(\n",
    "    input_channels=n_feat,\n",
    "    input_length=sequence_length,\n",
    "    num_classes=output_size\n",
    ").to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.1187\n",
      "Epoch 2/20 - Loss: 0.0512\n",
      "Epoch 3/20 - Loss: 0.0381\n",
      "Epoch 4/20 - Loss: 0.0284\n",
      "Epoch 5/20 - Loss: 0.0195\n",
      "Epoch 6/20 - Loss: 0.0175\n",
      "Epoch 7/20 - Loss: 0.0139\n",
      "Epoch 8/20 - Loss: 0.0104\n",
      "Epoch 9/20 - Loss: 0.0119\n",
      "Epoch 10/20 - Loss: 0.0090\n",
      "Epoch 11/20 - Loss: 0.0113\n",
      "Epoch 12/20 - Loss: 0.0066\n",
      "Epoch 13/20 - Loss: 0.0086\n",
      "Epoch 14/20 - Loss: 0.0047\n",
      "Epoch 15/20 - Loss: 0.0032\n",
      "Epoch 16/20 - Loss: 0.0089\n",
      "Epoch 17/20 - Loss: 0.0072\n",
      "Epoch 18/20 - Loss: 0.0053\n",
      "Epoch 19/20 - Loss: 0.0042\n",
      "Epoch 20/20 - Loss: 0.0046\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      4965\n",
      "           1       0.95      0.90      0.92      3385\n",
      "\n",
      "    accuracy                           0.94      8350\n",
      "   macro avg       0.94      0.93      0.93      8350\n",
      "weighted avg       0.94      0.94      0.94      8350\n",
      "\n",
      "Accuracy: 0.9377245508982036\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso CNN\n",
    "# treinar\n",
    "model.train_model(train_loader, device=device, epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliar\n",
    "model.evaluate(test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17099\n",
      "Total de amostras no conjunto de teste:  8370\n",
      "Train Dataset Shape: torch.Size([17099, 50, 9])\n",
      "Test  Dataset Shape: torch.Size([8370, 50, 9])\n",
      "Total de batches no treino: 268\n",
      "Total de batches no teste:  131\n",
      "ModelHybridAttnSVM(\n",
      "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(128, 64, num_layers=3, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.Hybrid.ModelHybridAttnSVM import ModelHybridAttnSVM\n",
    "from torch.utils.data import DataLoader\n",
    "from models.Sequence import SequenceDataset\n",
    "import torch\n",
    "\n",
    "# Parâmetros gerais\n",
    "sequence_length   = 50\n",
    "column_to_remove  = 'attack_name'\n",
    "batch_size        = 64\n",
    "hidden_size       = 64\n",
    "num_layers        = 3\n",
    "num_classes       = 2\n",
    "device            = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs            = 100\n",
    "learning_rate     = 1e-4\n",
    "\n",
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste:  {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test  Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no teste:  {len(test_loader)}\")\n",
    "\n",
    "# Definir n_features a partir do dataset\n",
    "n_features = train_dataset.sequences.shape[2]\n",
    "\n",
    "# Instanciar o modelo híbrido\n",
    "model = ModelHybridAttnSVM(\n",
    "    seq_len     = sequence_length,\n",
    "    n_features  = n_features,\n",
    "    lstm_hidden = hidden_size,\n",
    "    lstm_layers = num_layers,\n",
    "    num_classes = num_classes\n",
    ").to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Treinando CNN+LSTM ###\n",
      "Epoch 1/100 - Loss: 0.5094\n",
      "Epoch 2/100 - Loss: 0.1693\n",
      "Epoch 3/100 - Loss: 0.1238\n",
      "Epoch 4/100 - Loss: 0.1104\n",
      "Epoch 5/100 - Loss: 0.1021\n",
      "Epoch 6/100 - Loss: 0.0973\n",
      "Epoch 7/100 - Loss: 0.0935\n",
      "Epoch 8/100 - Loss: 0.0871\n",
      "Epoch 9/100 - Loss: 0.0841\n",
      "Epoch 10/100 - Loss: 0.0828\n",
      "Epoch 11/100 - Loss: 0.0799\n",
      "Epoch 12/100 - Loss: 0.0743\n",
      "Epoch 13/100 - Loss: 0.0736\n",
      "Epoch 14/100 - Loss: 0.0723\n",
      "Epoch 15/100 - Loss: 0.0697\n",
      "Epoch 16/100 - Loss: 0.0683\n",
      "Epoch 17/100 - Loss: 0.0664\n",
      "Epoch 18/100 - Loss: 0.0668\n",
      "Epoch 19/100 - Loss: 0.0656\n",
      "Epoch 20/100 - Loss: 0.0629\n",
      "Epoch 21/100 - Loss: 0.0624\n",
      "Epoch 22/100 - Loss: 0.0608\n",
      "Epoch 23/100 - Loss: 0.0598\n",
      "Epoch 24/100 - Loss: 0.0604\n",
      "Epoch 25/100 - Loss: 0.0587\n",
      "Epoch 26/100 - Loss: 0.0586\n",
      "Epoch 27/100 - Loss: 0.0566\n",
      "Epoch 28/100 - Loss: 0.0559\n",
      "Epoch 29/100 - Loss: 0.0554\n",
      "Epoch 30/100 - Loss: 0.0541\n",
      "Epoch 31/100 - Loss: 0.0520\n",
      "Epoch 32/100 - Loss: 0.0527\n",
      "Epoch 33/100 - Loss: 0.0517\n",
      "Epoch 34/100 - Loss: 0.0511\n",
      "Epoch 35/100 - Loss: 0.0483\n",
      "Epoch 36/100 - Loss: 0.0493\n",
      "Epoch 37/100 - Loss: 0.0487\n",
      "Epoch 38/100 - Loss: 0.0480\n",
      "Epoch 39/100 - Loss: 0.0460\n",
      "Epoch 40/100 - Loss: 0.0450\n",
      "Epoch 41/100 - Loss: 0.0447\n",
      "Epoch 42/100 - Loss: 0.0438\n",
      "Epoch 43/100 - Loss: 0.0441\n",
      "Epoch 44/100 - Loss: 0.0418\n",
      "Epoch 45/100 - Loss: 0.0419\n",
      "Epoch 46/100 - Loss: 0.0421\n",
      "Epoch 47/100 - Loss: 0.0408\n",
      "Epoch 48/100 - Loss: 0.0412\n",
      "Epoch 49/100 - Loss: 0.0401\n",
      "Epoch 50/100 - Loss: 0.0413\n",
      "Epoch 51/100 - Loss: 0.0387\n",
      "Epoch 52/100 - Loss: 0.0384\n",
      "Epoch 53/100 - Loss: 0.0384\n",
      "Epoch 54/100 - Loss: 0.0361\n",
      "Epoch 55/100 - Loss: 0.0362\n",
      "Epoch 56/100 - Loss: 0.0360\n",
      "Epoch 57/100 - Loss: 0.0348\n",
      "Epoch 58/100 - Loss: 0.0337\n",
      "Epoch 59/100 - Loss: 0.0328\n",
      "Epoch 60/100 - Loss: 0.0356\n",
      "Epoch 61/100 - Loss: 0.0351\n",
      "Epoch 62/100 - Loss: 0.0325\n",
      "Epoch 63/100 - Loss: 0.0315\n",
      "Epoch 64/100 - Loss: 0.0322\n",
      "Epoch 65/100 - Loss: 0.0308\n",
      "Epoch 66/100 - Loss: 0.0315\n",
      "Epoch 67/100 - Loss: 0.0322\n",
      "Epoch 68/100 - Loss: 0.0296\n",
      "Epoch 69/100 - Loss: 0.0286\n",
      "Epoch 70/100 - Loss: 0.0284\n",
      "Epoch 71/100 - Loss: 0.0282\n",
      "Epoch 72/100 - Loss: 0.0264\n",
      "Epoch 73/100 - Loss: 0.0273\n",
      "Epoch 74/100 - Loss: 0.0257\n",
      "Epoch 75/100 - Loss: 0.0267\n",
      "Epoch 76/100 - Loss: 0.0255\n",
      "Epoch 77/100 - Loss: 0.0263\n",
      "Epoch 78/100 - Loss: 0.0254\n",
      "Epoch 79/100 - Loss: 0.0248\n",
      "Epoch 80/100 - Loss: 0.0230\n",
      "Epoch 81/100 - Loss: 0.0237\n",
      "Epoch 82/100 - Loss: 0.0253\n",
      "Epoch 83/100 - Loss: 0.0230\n",
      "Epoch 84/100 - Loss: 0.0223\n",
      "Epoch 85/100 - Loss: 0.0225\n",
      "Epoch 86/100 - Loss: 0.0215\n",
      "Epoch 87/100 - Loss: 0.0217\n",
      "Epoch 88/100 - Loss: 0.0216\n",
      "Epoch 89/100 - Loss: 0.0207\n",
      "Epoch 90/100 - Loss: 0.0193\n",
      "Epoch 91/100 - Loss: 0.0189\n",
      "Epoch 92/100 - Loss: 0.0213\n",
      "Epoch 93/100 - Loss: 0.0199\n",
      "Epoch 94/100 - Loss: 0.0198\n",
      "Epoch 95/100 - Loss: 0.0195\n",
      "Epoch 96/100 - Loss: 0.0185\n",
      "Epoch 97/100 - Loss: 0.0209\n",
      "Epoch 98/100 - Loss: 0.0181\n",
      "Epoch 99/100 - Loss: 0.0180\n",
      "Epoch 100/100 - Loss: 0.0202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelHybridAttnSVM(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(128, 64, num_layers=3, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento\n",
    "print(\"### Treinando CNN+LSTM ###\")\n",
    "model.train_model(\n",
    "    train_loader,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    lr=learning_rate,\n",
    "    save_path='output/Hybrid/hybrid_attn.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Treinando PCA + SVM ###\n",
      "PCA salvo em output/Hybrid/pca.joblib\n",
      "SVM salvo em output/Hybrid/hybrid_svm.joblib\n",
      "\n",
      "### Avaliando Modelo Híbrido no Conjunto de Teste ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      4965\n",
      "           1       0.90      0.90      0.90      3405\n",
      "\n",
      "    accuracy                           0.92      8370\n",
      "   macro avg       0.92      0.92      0.92      8370\n",
      "weighted avg       0.92      0.92      0.92      8370\n",
      "\n",
      "Accuracy: 0.9200716845878136\n"
     ]
    }
   ],
   "source": [
    "# 2) Treinar PCA + SVM sobre as features extraídas\n",
    "print(\"\\n### Treinando PCA + SVM ###\")\n",
    "svm = model.train_svm(\n",
    "    train_loader,\n",
    "    pca_path='output/Hybrid/pca.joblib',\n",
    "    svm_path='output/Hybrid/hybrid_svm.joblib'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Avaliar todo o pipeline (CNN+LSTM → PCA → SVM) no conjunto de teste\n",
    "print(\"\\n### Avaliando Modelo Híbrido no Conjunto de Teste ###\")\n",
    "model.evaluate(\n",
    "    test_loader,\n",
    "    pca_path='output/Hybrid/pca.joblib',\n",
    "    svm_path='output/Hybrid/hybrid_svm.joblib'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
