{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo. Abaixo, são listados os períodos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Função para criar sequências aleatórias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.LSTM.ModelLSTM import LSTM\n",
    "from models.LSTM.SequenceLSTM import SequenceDataset\n",
    "from models.LSTM.TrainerLSTM import TrainerLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Configurar os parâmetros da rede LSTM\n",
    "input_size = 9         # Número de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / Nº de unidades ocultas por célula\n",
    "num_layers = 3         # Número de camadas LSTM / Nº de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 128        # Batch size / \n",
    "sequence_length = 5   # Tamanho da sequência de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17144\n",
      "Total de amostras no conjunto de teste: 8415\n"
     ]
    }
   ],
   "source": [
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de batches no conjunto de treino: 134\n",
      "Total de batches no conjunto de teste: 66\n"
     ]
    }
   ],
   "source": [
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm1): LSTM(9, 128, num_layers=3, batch_first=True)\n",
      "  (lstm2): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (lstm3): LSTM(256, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.4158 Val Loss:   0.4716 Accuracy:   0.8449\n",
      "🔖 Melhor modelo salvo em: output/UEL/LSTM\\LSTM_Epoca-1_Acc-0.84.pth\n",
      "Epoch [2/100] Train Loss: 0.2251 Val Loss:   0.4735 Accuracy:   0.8323\n",
      "Epoch [3/100] Train Loss: 0.1760 Val Loss:   0.4217 Accuracy:   0.8532\n",
      "🔖 Melhor modelo salvo em: output/UEL/LSTM\\LSTM_Epoca-3_Acc-0.85.pth\n",
      "Epoch [4/100] Train Loss: 0.1607 Val Loss:   0.7465 Accuracy:   0.7450\n",
      "Epoch [5/100] Train Loss: 0.1494 Val Loss:   1.0015 Accuracy:   0.6927\n",
      "Epoch [6/100] Train Loss: 0.1420 Val Loss:   0.6754 Accuracy:   0.5468\n",
      "Epoch [7/100] Train Loss: 0.1308 Val Loss:   0.8149 Accuracy:   0.6101\n",
      "Epoch [8/100] Train Loss: 0.1247 Val Loss:   0.5917 Accuracy:   0.6872\n",
      "Epoch [9/100] Train Loss: 0.1247 Val Loss:   0.6217 Accuracy:   0.7777\n",
      "Epoch [10/100] Train Loss: 0.1187 Val Loss:   0.5170 Accuracy:   0.7728\n",
      "Epoch [11/100] Train Loss: 0.1149 Val Loss:   0.6521 Accuracy:   0.7243\n",
      "Epoch [12/100] Train Loss: 0.1101 Val Loss:   0.5270 Accuracy:   0.7348\n",
      "Epoch [13/100] Train Loss: 0.1056 Val Loss:   0.4343 Accuracy:   0.8147\n",
      "Epoch [14/100] Train Loss: 0.1054 Val Loss:   0.7186 Accuracy:   0.7444\n",
      "Epoch [15/100] Train Loss: 0.1001 Val Loss:   0.5092 Accuracy:   0.8147\n",
      "Epoch [16/100] Train Loss: 0.0945 Val Loss:   0.5627 Accuracy:   0.7975\n",
      "Epoch [17/100] Train Loss: 0.0928 Val Loss:   0.6497 Accuracy:   0.7832\n",
      "Epoch [18/100] Train Loss: 0.0923 Val Loss:   0.6020 Accuracy:   0.7740\n",
      "Epoch [19/100] Train Loss: 0.0889 Val Loss:   0.5149 Accuracy:   0.8077\n",
      "Epoch [20/100] Train Loss: 0.0874 Val Loss:   0.7608 Accuracy:   0.7746\n",
      "Epoch [21/100] Train Loss: 0.0868 Val Loss:   0.6705 Accuracy:   0.7821\n",
      "Epoch [22/100] Train Loss: 0.0820 Val Loss:   0.5716 Accuracy:   0.8143\n",
      "Epoch [23/100] Train Loss: 0.0808 Val Loss:   0.4486 Accuracy:   0.8668\n",
      "Epoch [24/100] Train Loss: 0.0805 Val Loss:   0.5556 Accuracy:   0.8109\n",
      "Epoch [25/100] Train Loss: 0.0823 Val Loss:   0.5201 Accuracy:   0.8471\n",
      "Epoch [26/100] Train Loss: 0.0743 Val Loss:   0.6060 Accuracy:   0.7976\n",
      "Epoch [27/100] Train Loss: 0.0723 Val Loss:   0.6114 Accuracy:   0.8327\n",
      "Epoch [28/100] Train Loss: 0.0696 Val Loss:   0.4779 Accuracy:   0.8532\n",
      "Epoch [29/100] Train Loss: 0.0711 Val Loss:   0.5067 Accuracy:   0.8321\n",
      "Epoch [30/100] Train Loss: 0.0707 Val Loss:   0.5647 Accuracy:   0.8374\n",
      "Epoch [31/100] Train Loss: 0.0662 Val Loss:   0.5005 Accuracy:   0.8496\n",
      "Epoch [32/100] Train Loss: 0.0652 Val Loss:   0.5141 Accuracy:   0.8513\n",
      "Epoch [33/100] Train Loss: 0.0642 Val Loss:   0.6170 Accuracy:   0.8474\n",
      "Epoch [34/100] Train Loss: 0.0598 Val Loss:   0.5419 Accuracy:   0.8551\n",
      "Epoch [35/100] Train Loss: 0.0580 Val Loss:   0.5727 Accuracy:   0.8573\n",
      "Epoch [36/100] Train Loss: 0.0565 Val Loss:   0.6526 Accuracy:   0.8231\n",
      "Epoch [37/100] Train Loss: 0.0531 Val Loss:   0.5567 Accuracy:   0.8625\n",
      "Epoch [38/100] Train Loss: 0.0559 Val Loss:   0.6426 Accuracy:   0.8304\n",
      "Epoch [39/100] Train Loss: 0.0524 Val Loss:   0.5774 Accuracy:   0.8498\n",
      "Epoch [40/100] Train Loss: 0.0515 Val Loss:   0.6110 Accuracy:   0.8530\n",
      "Epoch [41/100] Train Loss: 0.0517 Val Loss:   0.6293 Accuracy:   0.8631\n",
      "Epoch [42/100] Train Loss: 0.0485 Val Loss:   0.6460 Accuracy:   0.8540\n",
      "Epoch [43/100] Train Loss: 0.0515 Val Loss:   0.5651 Accuracy:   0.8671\n",
      "Epoch [44/100] Train Loss: 0.0433 Val Loss:   0.5947 Accuracy:   0.8550\n",
      "Epoch [45/100] Train Loss: 0.0468 Val Loss:   0.6778 Accuracy:   0.8354\n",
      "Epoch [46/100] Train Loss: 0.0413 Val Loss:   0.7092 Accuracy:   0.8509\n",
      "Epoch [47/100] Train Loss: 0.0449 Val Loss:   0.6156 Accuracy:   0.8468\n",
      "Epoch [48/100] Train Loss: 0.0426 Val Loss:   0.7175 Accuracy:   0.8440\n",
      "Epoch [49/100] Train Loss: 0.0414 Val Loss:   0.6585 Accuracy:   0.8545\n",
      "Epoch [50/100] Train Loss: 0.0416 Val Loss:   0.6536 Accuracy:   0.8500\n",
      "Epoch [51/100] Train Loss: 0.0385 Val Loss:   0.6723 Accuracy:   0.8494\n",
      "Epoch [52/100] Train Loss: 0.0420 Val Loss:   0.6353 Accuracy:   0.8612\n",
      "Epoch [53/100] Train Loss: 0.0376 Val Loss:   0.6695 Accuracy:   0.8481\n",
      "Epoch [54/100] Train Loss: 0.0354 Val Loss:   0.6257 Accuracy:   0.8589\n",
      "Epoch [55/100] Train Loss: 0.0352 Val Loss:   0.7031 Accuracy:   0.8412\n",
      "Epoch [56/100] Train Loss: 0.0338 Val Loss:   0.7034 Accuracy:   0.8448\n",
      "Epoch [57/100] Train Loss: 0.0365 Val Loss:   0.6219 Accuracy:   0.8525\n",
      "Epoch [58/100] Train Loss: 0.0329 Val Loss:   0.5510 Accuracy:   0.8752\n",
      "Epoch [59/100] Train Loss: 0.0311 Val Loss:   0.7037 Accuracy:   0.8524\n",
      "Epoch [60/100] Train Loss: 0.0330 Val Loss:   0.7544 Accuracy:   0.8599\n",
      "Epoch [61/100] Train Loss: 0.0352 Val Loss:   0.7741 Accuracy:   0.8540\n",
      "Epoch [62/100] Train Loss: 0.0332 Val Loss:   0.7026 Accuracy:   0.8479\n",
      "Epoch [63/100] Train Loss: 0.0300 Val Loss:   0.6672 Accuracy:   0.8528\n",
      "Epoch [64/100] Train Loss: 0.0320 Val Loss:   0.6937 Accuracy:   0.8551\n",
      "Epoch [65/100] Train Loss: 0.0350 Val Loss:   0.6203 Accuracy:   0.8561\n",
      "Epoch [66/100] Train Loss: 0.0277 Val Loss:   0.6191 Accuracy:   0.8591\n",
      "Epoch [67/100] Train Loss: 0.0268 Val Loss:   0.7390 Accuracy:   0.8580\n",
      "Epoch [68/100] Train Loss: 0.0311 Val Loss:   0.5898 Accuracy:   0.8718\n",
      "Epoch [69/100] Train Loss: 0.0292 Val Loss:   0.7041 Accuracy:   0.8548\n",
      "Epoch [70/100] Train Loss: 0.0257 Val Loss:   0.7077 Accuracy:   0.8597\n",
      "Epoch [71/100] Train Loss: 0.0254 Val Loss:   0.7583 Accuracy:   0.8580\n",
      "Epoch [72/100] Train Loss: 0.0248 Val Loss:   0.6956 Accuracy:   0.8544\n",
      "Epoch [73/100] Train Loss: 0.0238 Val Loss:   0.8282 Accuracy:   0.8505\n",
      "Epoch [74/100] Train Loss: 0.0294 Val Loss:   0.7792 Accuracy:   0.8437\n",
      "Epoch [75/100] Train Loss: 0.0239 Val Loss:   0.7682 Accuracy:   0.8484\n",
      "Epoch [76/100] Train Loss: 0.0243 Val Loss:   0.7382 Accuracy:   0.8562\n",
      "Epoch [77/100] Train Loss: 0.0205 Val Loss:   0.7617 Accuracy:   0.8610\n",
      "Epoch [78/100] Train Loss: 0.0229 Val Loss:   0.7492 Accuracy:   0.8671\n",
      "Epoch [79/100] Train Loss: 0.0235 Val Loss:   0.6573 Accuracy:   0.8686\n",
      "Epoch [80/100] Train Loss: 0.0236 Val Loss:   0.6605 Accuracy:   0.8713\n",
      "Epoch [81/100] Train Loss: 0.0212 Val Loss:   0.7275 Accuracy:   0.8656\n",
      "Epoch [82/100] Train Loss: 0.0250 Val Loss:   0.6206 Accuracy:   0.8680\n",
      "Epoch [83/100] Train Loss: 0.0206 Val Loss:   0.7229 Accuracy:   0.8570\n",
      "Epoch [84/100] Train Loss: 0.0198 Val Loss:   0.8112 Accuracy:   0.8522\n",
      "Epoch [85/100] Train Loss: 0.0249 Val Loss:   0.8604 Accuracy:   0.8435\n",
      "Epoch [86/100] Train Loss: 0.0313 Val Loss:   0.7505 Accuracy:   0.8632\n",
      "Epoch [87/100] Train Loss: 0.0226 Val Loss:   0.7765 Accuracy:   0.8525\n",
      "Epoch [88/100] Train Loss: 0.0254 Val Loss:   0.7470 Accuracy:   0.8667\n",
      "Epoch [89/100] Train Loss: 0.0306 Val Loss:   0.6375 Accuracy:   0.8694\n",
      "Epoch [90/100] Train Loss: 0.0192 Val Loss:   0.7567 Accuracy:   0.8560\n",
      "Epoch [91/100] Train Loss: 0.0172 Val Loss:   0.6991 Accuracy:   0.8613\n",
      "Epoch [92/100] Train Loss: 0.0203 Val Loss:   0.7427 Accuracy:   0.8622\n",
      "Epoch [93/100] Train Loss: 0.0219 Val Loss:   0.8337 Accuracy:   0.8506\n",
      "Epoch [94/100] Train Loss: 0.0194 Val Loss:   0.8092 Accuracy:   0.8536\n",
      "Epoch [95/100] Train Loss: 0.0174 Val Loss:   0.6852 Accuracy:   0.8744\n",
      "Epoch [96/100] Train Loss: 0.0212 Val Loss:   0.7383 Accuracy:   0.8644\n",
      "Epoch [97/100] Train Loss: 0.0163 Val Loss:   0.8300 Accuracy:   0.8568\n",
      "Epoch [98/100] Train Loss: 0.0164 Val Loss:   0.8026 Accuracy:   0.8594\n",
      "Epoch [99/100] Train Loss: 0.0196 Val Loss:   0.8486 Accuracy:   0.8424\n",
      "Epoch [100/100] Train Loss: 0.0211 Val Loss:   0.7915 Accuracy:   0.8582\n",
      "✅ Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerLSTM(dir_save=\"output/UEL/LSTM\", num_epochs=100)\n",
    "trainer.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shape: torch.Size([17144, 9, 5])\n",
      "Test Dataset Shape: torch.Size([8415, 9, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(9, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 9\n",
    "sequence_length = 5\n",
    "output_size = 2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "column_to_remove = 'attack_name'\n",
    "\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Modelo\n",
    "in_ch  = train_dataset.sequences.shape[1]   \n",
    "length = train_dataset.sequences.shape[2]\n",
    "model = CNN(input_channels=in_ch, input_length=length, num_classes=output_size).to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs [1] Train Loss: 0.3576 Val Loss: 0.7702 Accuracy: 0.7051\n",
      "num_epochs [2] Train Loss: 0.3576 Val Loss: 1.0510 Accuracy: 0.6134\n",
      "num_epochs [3] Train Loss: 0.3576 Val Loss: 1.1179 Accuracy: 0.6330\n",
      "num_epochs [4] Train Loss: 0.3576 Val Loss: 0.8157 Accuracy: 0.7176\n",
      "num_epochs [5] Train Loss: 0.3576 Val Loss: 0.9511 Accuracy: 0.7144\n",
      "num_epochs [6] Train Loss: 0.3576 Val Loss: 0.9229 Accuracy: 0.6942\n",
      "num_epochs [7] Train Loss: 0.3576 Val Loss: 0.5452 Accuracy: 0.8007\n",
      "Melhor modelo salvo\n",
      "num_epochs [8] Train Loss: 0.3576 Val Loss: 1.1932 Accuracy: 0.6270\n",
      "num_epochs [9] Train Loss: 0.3576 Val Loss: 0.8717 Accuracy: 0.7630\n",
      "num_epochs [10] Train Loss: 0.3576 Val Loss: 1.4044 Accuracy: 0.6585\n",
      "num_epochs [11] Train Loss: 0.3576 Val Loss: 3.8234 Accuracy: 0.4597\n",
      "num_epochs [12] Train Loss: 0.3576 Val Loss: 0.9889 Accuracy: 0.7616\n",
      "num_epochs [13] Train Loss: 0.3576 Val Loss: 1.6415 Accuracy: 0.6485\n",
      "num_epochs [14] Train Loss: 0.3576 Val Loss: 2.0549 Accuracy: 0.6147\n",
      "num_epochs [15] Train Loss: 0.3576 Val Loss: 1.7502 Accuracy: 0.6655\n",
      "num_epochs [16] Train Loss: 0.3576 Val Loss: 2.6815 Accuracy: 0.5469\n",
      "num_epochs [17] Train Loss: 0.3576 Val Loss: 1.1204 Accuracy: 0.7588\n",
      "num_epochs [18] Train Loss: 0.3576 Val Loss: 2.2956 Accuracy: 0.6265\n",
      "num_epochs [19] Train Loss: 0.3576 Val Loss: 2.1913 Accuracy: 0.6187\n",
      "num_epochs [20] Train Loss: 0.3576 Val Loss: 0.8827 Accuracy: 0.7779\n",
      "num_epochs [21] Train Loss: 0.3576 Val Loss: 1.8462 Accuracy: 0.6431\n",
      "num_epochs [22] Train Loss: 0.3576 Val Loss: 1.8877 Accuracy: 0.6551\n",
      "num_epochs [23] Train Loss: 0.3576 Val Loss: 2.4207 Accuracy: 0.5666\n",
      "num_epochs [24] Train Loss: 0.3576 Val Loss: 2.0236 Accuracy: 0.6181\n",
      "num_epochs [25] Train Loss: 0.3576 Val Loss: 1.9134 Accuracy: 0.6587\n",
      "num_epochs [26] Train Loss: 0.3576 Val Loss: 1.3708 Accuracy: 0.7813\n",
      "num_epochs [27] Train Loss: 0.3576 Val Loss: 3.9837 Accuracy: 0.5051\n",
      "num_epochs [28] Train Loss: 0.3576 Val Loss: 2.1952 Accuracy: 0.6286\n",
      "num_epochs [29] Train Loss: 0.3576 Val Loss: 0.5826 Accuracy: 0.8638\n",
      "num_epochs [30] Train Loss: 0.3576 Val Loss: 3.5461 Accuracy: 0.5718\n",
      "num_epochs [31] Train Loss: 0.3576 Val Loss: 2.0784 Accuracy: 0.6610\n",
      "num_epochs [32] Train Loss: 0.3576 Val Loss: 3.0549 Accuracy: 0.5944\n",
      "num_epochs [33] Train Loss: 0.3576 Val Loss: 2.0750 Accuracy: 0.6727\n",
      "num_epochs [34] Train Loss: 0.3576 Val Loss: 0.7260 Accuracy: 0.8575\n",
      "num_epochs [35] Train Loss: 0.3576 Val Loss: 2.3444 Accuracy: 0.6385\n",
      "num_epochs [36] Train Loss: 0.3576 Val Loss: 0.8839 Accuracy: 0.7939\n",
      "num_epochs [37] Train Loss: 0.3576 Val Loss: 3.2321 Accuracy: 0.5827\n",
      "num_epochs [38] Train Loss: 0.3576 Val Loss: 0.7058 Accuracy: 0.8652\n",
      "num_epochs [39] Train Loss: 0.3576 Val Loss: 1.6086 Accuracy: 0.7234\n",
      "num_epochs [40] Train Loss: 0.3576 Val Loss: 1.3145 Accuracy: 0.7851\n",
      "num_epochs [41] Train Loss: 0.3576 Val Loss: 3.9126 Accuracy: 0.5623\n",
      "num_epochs [42] Train Loss: 0.3576 Val Loss: 1.1854 Accuracy: 0.7925\n",
      "num_epochs [43] Train Loss: 0.3576 Val Loss: 2.1291 Accuracy: 0.6890\n",
      "num_epochs [44] Train Loss: 0.3576 Val Loss: 2.8088 Accuracy: 0.6458\n",
      "num_epochs [45] Train Loss: 0.3576 Val Loss: 0.8004 Accuracy: 0.8404\n",
      "num_epochs [46] Train Loss: 0.3576 Val Loss: 3.0018 Accuracy: 0.6006\n",
      "num_epochs [47] Train Loss: 0.3576 Val Loss: 3.0566 Accuracy: 0.6822\n",
      "num_epochs [48] Train Loss: 0.3576 Val Loss: 2.0696 Accuracy: 0.7106\n",
      "num_epochs [49] Train Loss: 0.3576 Val Loss: 2.6765 Accuracy: 0.6676\n",
      "num_epochs [50] Train Loss: 0.3576 Val Loss: 3.0011 Accuracy: 0.6217\n",
      "Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "dir_save = \"output/UEL/CNN\"\n",
    "os.makedirs(dir_save, exist_ok=True)\n",
    "best_loss = float('inf') \n",
    "\n",
    "for num_epochs in range(1, num_epochs+1):\n",
    "    # Treino\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss, preds, trues = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            val_loss += criterion(out, y).item() * x.size(0)\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            trues.extend(y.cpu().numpy())\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc  = accuracy_score(trues, preds)\n",
    "\n",
    "    print(f\"num_epochs [{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} \"\n",
    "          f\"Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Salvar o melhor modelo ----\n",
    "    if val_loss < best_loss and val_acc > 0.80:\n",
    "        best_loss = val_loss\n",
    "        save_path = os.path.join(dir_save, f\"CNN_Epoca-{num_epochs}_Acc-{val_acc:.2f}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss\n",
    "        }, save_path)\n",
    "        print(f\"Melhor modelo salvo\")\n",
    "\n",
    "print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label', 'attack_name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 155\u001b[0m\n\u001b[0;32m    152\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# 1. Train CNN-LSTM on train set\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m model, train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 2. Train SVM on train features\u001b[39;00m\n\u001b[0;32m    166\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m train_svm_on_features(model, train_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[186], line 76\u001b[0m, in \u001b[0;36mtrain_cnn_lstm\u001b[1;34m(train_df_path, test_df_path, seq_len, batch_size, epochs, lr, device)\u001b[0m\n\u001b[0;32m     73\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_df_path)\n\u001b[0;32m     74\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_df_path)\n\u001b[1;32m---> 76\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mFlowSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m FlowSequenceDataset(test_df, seq_len)\n\u001b[0;32m     78\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[186], line 18\u001b[0m, in \u001b[0;36mFlowSequenceDataset.__init__\u001b[1;34m(self, df, seq_len)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame, seq_len: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Assumes df sorted by time, with label column\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m=\u001b[39m seq_len\n\u001b[1;32m---> 18\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattack_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     19\u001b[0m     labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     20\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['label', 'attack_name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sequence_dataset import FlowSequenceDataset\n",
    "from hybrid_model import train_feature_extractor, extract_features, train_svm, evaluate_hybrid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configurações\n",
    "device   = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_csv = \"path/to/train_data.csv\"\n",
    "test_csv  = \"path/to/test_data.csv\"\n",
    "\n",
    "# Passo 1: treina o extrator CNN-LSTM\n",
    "extractor, train_loader = train_feature_extractor(\n",
    "    train_csv=train_csv,\n",
    "    seq_len=10,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    lr=1e-3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Passo 2: treina o SVM sobre features extraídas\n",
    "dados_X, dados_y = extract_features(extractor, train_loader, device)\n",
    "svm_model = train_svm(dados_X, dados_y)\n",
    "\n",
    "# Passo 3: avalia no conjunto de teste test_csv\n",
    "test_ds     = FlowSequenceDataset(test_csv, seq_len=10)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "evaluate_hybrid(extractor, test_loader, svm_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
