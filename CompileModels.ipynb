{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Função para criar sequências aleatórias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequências aleatórias!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.LSTM.ModelLSTM import LSTM\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.LSTM.TrainerLSTM import TrainerLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Configurar os parâmetros da rede LSTM\n",
    "input_size = 9         # Número de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / Nº de unidades ocultas por célula\n",
    "num_layers = 3         # Número de camadas LSTM / Nº de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 128        # Batch size / \n",
    "sequence_length = 10   # Tamanho da sequência de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17139\n",
      "Total de amostras no conjunto de teste: 8410\n",
      "Train Dataset Shape: torch.Size([17139, 10, 9])\n",
      "Test Dataset Shape: torch.Size([8410, 10, 9])\n"
     ]
    }
   ],
   "source": [
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove=column_to_remove, normalize=True, mode='lstm')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='lstm')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Criar o modelo\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.4143 Val Loss:   0.3635 Accuracy:   0.8725\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [2/100] Train Loss: 0.1607 Val Loss:   0.3591 Accuracy:   0.8926\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [3/100] Train Loss: 0.1231 Val Loss:   0.3107 Accuracy:   0.9001\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [4/100] Train Loss: 0.0989 Val Loss:   0.2797 Accuracy:   0.8907\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [5/100] Train Loss: 0.0911 Val Loss:   0.4735 Accuracy:   0.8895\n",
      "Epoch [6/100] Train Loss: 0.0964 Val Loss:   0.2631 Accuracy:   0.9043\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [7/100] Train Loss: 0.0857 Val Loss:   0.3939 Accuracy:   0.8484\n",
      "Epoch [8/100] Train Loss: 0.0873 Val Loss:   0.2725 Accuracy:   0.8898\n",
      "Epoch [9/100] Train Loss: 0.0873 Val Loss:   0.2713 Accuracy:   0.9100\n",
      "Epoch [10/100] Train Loss: 0.0909 Val Loss:   0.2565 Accuracy:   0.8930\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [11/100] Train Loss: 0.0752 Val Loss:   0.2610 Accuracy:   0.9074\n",
      "Epoch [12/100] Train Loss: 0.0775 Val Loss:   0.2517 Accuracy:   0.9127\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [13/100] Train Loss: 0.0727 Val Loss:   0.2843 Accuracy:   0.8961\n",
      "Epoch [14/100] Train Loss: 0.0756 Val Loss:   0.2700 Accuracy:   0.9033\n",
      "Epoch [15/100] Train Loss: 0.0739 Val Loss:   0.2420 Accuracy:   0.9090\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [16/100] Train Loss: 0.0765 Val Loss:   0.2673 Accuracy:   0.9078\n",
      "Epoch [17/100] Train Loss: 0.0742 Val Loss:   0.3314 Accuracy:   0.8807\n",
      "Epoch [18/100] Train Loss: 0.0792 Val Loss:   0.2696 Accuracy:   0.9118\n",
      "Epoch [19/100] Train Loss: 0.0740 Val Loss:   0.2456 Accuracy:   0.9042\n",
      "Epoch [20/100] Train Loss: 0.0780 Val Loss:   0.3812 Accuracy:   0.8981\n",
      "Epoch [21/100] Train Loss: 0.0688 Val Loss:   0.2402 Accuracy:   0.9164\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [22/100] Train Loss: 0.0682 Val Loss:   0.2660 Accuracy:   0.9149\n",
      "Epoch [23/100] Train Loss: 0.0665 Val Loss:   0.2856 Accuracy:   0.8973\n",
      "Epoch [24/100] Train Loss: 0.0775 Val Loss:   0.2314 Accuracy:   0.9119\n",
      "🔖 Melhor modelo salvo!\n",
      "Epoch [25/100] Train Loss: 0.0670 Val Loss:   0.3108 Accuracy:   0.8987\n",
      "Epoch [26/100] Train Loss: 0.0689 Val Loss:   0.2468 Accuracy:   0.9151\n",
      "Epoch [27/100] Train Loss: 0.0673 Val Loss:   0.2877 Accuracy:   0.8860\n",
      "Epoch [28/100] Train Loss: 0.0639 Val Loss:   0.3360 Accuracy:   0.8700\n",
      "Epoch [29/100] Train Loss: 0.0579 Val Loss:   0.2506 Accuracy:   0.9108\n",
      "Epoch [30/100] Train Loss: 0.0593 Val Loss:   0.3183 Accuracy:   0.9021\n",
      "Epoch [31/100] Train Loss: 0.0583 Val Loss:   0.2439 Accuracy:   0.9064\n",
      "Epoch [32/100] Train Loss: 0.0535 Val Loss:   0.2950 Accuracy:   0.9093\n",
      "Epoch [33/100] Train Loss: 0.0565 Val Loss:   0.2550 Accuracy:   0.9106\n",
      "Epoch [34/100] Train Loss: 0.0559 Val Loss:   0.3880 Accuracy:   0.8694\n",
      "Epoch [35/100] Train Loss: 0.0562 Val Loss:   0.2846 Accuracy:   0.9168\n",
      "Epoch [36/100] Train Loss: 0.0528 Val Loss:   0.2605 Accuracy:   0.9155\n",
      "Epoch [37/100] Train Loss: 0.0559 Val Loss:   0.2615 Accuracy:   0.9122\n",
      "Epoch [38/100] Train Loss: 0.0555 Val Loss:   0.3790 Accuracy:   0.9005\n",
      "Epoch [39/100] Train Loss: 0.0501 Val Loss:   0.2864 Accuracy:   0.8948\n",
      "Epoch [40/100] Train Loss: 0.0478 Val Loss:   0.3286 Accuracy:   0.9076\n",
      "Epoch [41/100] Train Loss: 0.0475 Val Loss:   0.2854 Accuracy:   0.8964\n",
      "Epoch [42/100] Train Loss: 0.0458 Val Loss:   0.2498 Accuracy:   0.9233\n",
      "Epoch [43/100] Train Loss: 0.0511 Val Loss:   0.2740 Accuracy:   0.9045\n",
      "Epoch [44/100] Train Loss: 0.0467 Val Loss:   0.2770 Accuracy:   0.9150\n",
      "Epoch [45/100] Train Loss: 0.0463 Val Loss:   0.2758 Accuracy:   0.9206\n",
      "Epoch [46/100] Train Loss: 0.0435 Val Loss:   0.3112 Accuracy:   0.9043\n",
      "Epoch [47/100] Train Loss: 0.0409 Val Loss:   0.2945 Accuracy:   0.9219\n",
      "Epoch [48/100] Train Loss: 0.0485 Val Loss:   0.3155 Accuracy:   0.9146\n",
      "Epoch [49/100] Train Loss: 0.0410 Val Loss:   0.3210 Accuracy:   0.9057\n",
      "Epoch [50/100] Train Loss: 0.0451 Val Loss:   0.2645 Accuracy:   0.9181\n",
      "Epoch [51/100] Train Loss: 0.0382 Val Loss:   0.2908 Accuracy:   0.9133\n",
      "Epoch [52/100] Train Loss: 0.0379 Val Loss:   0.3492 Accuracy:   0.9151\n",
      "Epoch [53/100] Train Loss: 0.0359 Val Loss:   0.2920 Accuracy:   0.9120\n",
      "Epoch [54/100] Train Loss: 0.0401 Val Loss:   0.3510 Accuracy:   0.9080\n",
      "Epoch [55/100] Train Loss: 0.0399 Val Loss:   0.2807 Accuracy:   0.9226\n",
      "Epoch [56/100] Train Loss: 0.0360 Val Loss:   0.3267 Accuracy:   0.9077\n",
      "Epoch [57/100] Train Loss: 0.0373 Val Loss:   0.3068 Accuracy:   0.9162\n",
      "Epoch [58/100] Train Loss: 0.0355 Val Loss:   0.3315 Accuracy:   0.9175\n",
      "Epoch [59/100] Train Loss: 0.0305 Val Loss:   0.3241 Accuracy:   0.9196\n",
      "Epoch [60/100] Train Loss: 0.0280 Val Loss:   0.3741 Accuracy:   0.9159\n",
      "Epoch [61/100] Train Loss: 0.0342 Val Loss:   0.4321 Accuracy:   0.8917\n",
      "Epoch [62/100] Train Loss: 0.0392 Val Loss:   0.3009 Accuracy:   0.9206\n",
      "Epoch [63/100] Train Loss: 0.0289 Val Loss:   0.3414 Accuracy:   0.9137\n",
      "Epoch [64/100] Train Loss: 0.0286 Val Loss:   0.3834 Accuracy:   0.8932\n",
      "Epoch [65/100] Train Loss: 0.0249 Val Loss:   0.3926 Accuracy:   0.9064\n",
      "Epoch [66/100] Train Loss: 0.0268 Val Loss:   0.3995 Accuracy:   0.8941\n",
      "Epoch [67/100] Train Loss: 0.0221 Val Loss:   0.3355 Accuracy:   0.9165\n",
      "Epoch [68/100] Train Loss: 0.0274 Val Loss:   0.3862 Accuracy:   0.9057\n",
      "Epoch [69/100] Train Loss: 0.0238 Val Loss:   0.4770 Accuracy:   0.8697\n",
      "Epoch [70/100] Train Loss: 0.0258 Val Loss:   0.3735 Accuracy:   0.8969\n",
      "Epoch [71/100] Train Loss: 0.0224 Val Loss:   0.3459 Accuracy:   0.9121\n",
      "Epoch [72/100] Train Loss: 0.0243 Val Loss:   0.3723 Accuracy:   0.9171\n",
      "Epoch [73/100] Train Loss: 0.0261 Val Loss:   0.3642 Accuracy:   0.8863\n",
      "Epoch [74/100] Train Loss: 0.0302 Val Loss:   0.3420 Accuracy:   0.9165\n",
      "Epoch [75/100] Train Loss: 0.0182 Val Loss:   0.4696 Accuracy:   0.9090\n",
      "Epoch [76/100] Train Loss: 0.0293 Val Loss:   0.4105 Accuracy:   0.8931\n",
      "Epoch [77/100] Train Loss: 0.0244 Val Loss:   0.3619 Accuracy:   0.9131\n",
      "Epoch [78/100] Train Loss: 0.0147 Val Loss:   0.3828 Accuracy:   0.9189\n",
      "Epoch [79/100] Train Loss: 0.0151 Val Loss:   0.3506 Accuracy:   0.9239\n",
      "Epoch [80/100] Train Loss: 0.0186 Val Loss:   0.3567 Accuracy:   0.9147\n",
      "Epoch [81/100] Train Loss: 0.0120 Val Loss:   0.4150 Accuracy:   0.9190\n",
      "Epoch [82/100] Train Loss: 0.0138 Val Loss:   0.4675 Accuracy:   0.9118\n",
      "Epoch [83/100] Train Loss: 0.0212 Val Loss:   0.4207 Accuracy:   0.9137\n",
      "Epoch [84/100] Train Loss: 0.0157 Val Loss:   0.4640 Accuracy:   0.9062\n",
      "Epoch [85/100] Train Loss: 0.0208 Val Loss:   0.4428 Accuracy:   0.9117\n",
      "Epoch [86/100] Train Loss: 0.0277 Val Loss:   0.8959 Accuracy:   0.8673\n",
      "Epoch [87/100] Train Loss: 0.0426 Val Loss:   0.3018 Accuracy:   0.9169\n",
      "Epoch [88/100] Train Loss: 0.0114 Val Loss:   0.3735 Accuracy:   0.9240\n",
      "Epoch [89/100] Train Loss: 0.0129 Val Loss:   0.3879 Accuracy:   0.9162\n",
      "Epoch [90/100] Train Loss: 0.0162 Val Loss:   0.6334 Accuracy:   0.8883\n",
      "Epoch [91/100] Train Loss: 0.0338 Val Loss:   0.3934 Accuracy:   0.9138\n",
      "Epoch [92/100] Train Loss: 0.0124 Val Loss:   0.3653 Accuracy:   0.9080\n",
      "Epoch [93/100] Train Loss: 0.0143 Val Loss:   0.3859 Accuracy:   0.9161\n",
      "Epoch [94/100] Train Loss: 0.0145 Val Loss:   0.3890 Accuracy:   0.9228\n",
      "Epoch [95/100] Train Loss: 0.0116 Val Loss:   0.4495 Accuracy:   0.9069\n",
      "Epoch [96/100] Train Loss: 0.0127 Val Loss:   0.4295 Accuracy:   0.9156\n",
      "Epoch [97/100] Train Loss: 0.0087 Val Loss:   0.4397 Accuracy:   0.9145\n",
      "Epoch [98/100] Train Loss: 0.0610 Val Loss:   0.3068 Accuracy:   0.9166\n",
      "Epoch [99/100] Train Loss: 0.0123 Val Loss:   0.3736 Accuracy:   0.9168\n",
      "Epoch [100/100] Train Loss: 0.0121 Val Loss:   0.3907 Accuracy:   0.9139\n",
      "✅ Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerLSTM(dir_save=\"output/LSTM\", num_epochs=100)\n",
    "trainer.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.CNN.ModelCNN import CNN\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.CNN.TrainerCNN import TrainerCNN\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "input_size = 9\n",
    "sequence_length = 5\n",
    "output_size = 2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "column_to_remove = 'attack_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17144\n",
      "Total de amostras no conjunto de teste: 8415\n",
      "Train Dataset Shape: torch.Size([17144, 9, 5])\n",
      "Test Dataset Shape: torch.Size([8415, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn1d')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Modelo\n",
    "model = CNN(input_channels=input_size, input_length=sequence_length, num_classes=output_size).to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Train Loss: 0.1655 Val Loss:   0.3660 Accuracy:   0.8696\n",
      "🔖 Melhor modelo salvo em: output/UEL/CNN\\CNN_Epoca-1_Acc-0.87.pth\n",
      "Epoch [2/50] Train Loss: 0.1161 Val Loss:   0.2794 Accuracy:   0.8695\n",
      "🔖 Melhor modelo salvo em: output/UEL/CNN\\CNN_Epoca-2_Acc-0.87.pth\n",
      "Epoch [3/50] Train Loss: 0.1100 Val Loss:   0.4129 Accuracy:   0.8719\n",
      "Epoch [4/50] Train Loss: 0.1049 Val Loss:   0.3277 Accuracy:   0.8734\n",
      "Epoch [5/50] Train Loss: 0.1028 Val Loss:   0.3409 Accuracy:   0.8276\n",
      "Epoch [6/50] Train Loss: 0.0964 Val Loss:   0.5265 Accuracy:   0.8618\n",
      "Epoch [7/50] Train Loss: 0.0934 Val Loss:   0.2951 Accuracy:   0.8806\n",
      "Epoch [8/50] Train Loss: 0.0877 Val Loss:   0.2777 Accuracy:   0.8864\n",
      "🔖 Melhor modelo salvo em: output/UEL/CNN\\CNN_Epoca-8_Acc-0.89.pth\n",
      "Epoch [9/50] Train Loss: 0.0849 Val Loss:   0.3299 Accuracy:   0.8739\n",
      "Epoch [10/50] Train Loss: 0.0807 Val Loss:   0.2871 Accuracy:   0.8788\n",
      "Epoch [11/50] Train Loss: 0.0800 Val Loss:   0.4255 Accuracy:   0.8639\n",
      "Epoch [12/50] Train Loss: 0.0773 Val Loss:   0.2952 Accuracy:   0.8791\n",
      "Epoch [13/50] Train Loss: 0.0731 Val Loss:   0.3054 Accuracy:   0.8882\n",
      "Epoch [14/50] Train Loss: 0.0719 Val Loss:   0.3367 Accuracy:   0.8865\n",
      "Epoch [15/50] Train Loss: 0.0678 Val Loss:   0.3765 Accuracy:   0.8795\n",
      "Epoch [16/50] Train Loss: 0.0667 Val Loss:   0.3557 Accuracy:   0.8850\n",
      "Epoch [17/50] Train Loss: 0.0653 Val Loss:   0.3159 Accuracy:   0.8929\n",
      "Epoch [18/50] Train Loss: 0.0610 Val Loss:   0.3675 Accuracy:   0.8787\n",
      "Epoch [19/50] Train Loss: 0.0595 Val Loss:   0.3417 Accuracy:   0.8807\n",
      "Epoch [20/50] Train Loss: 0.0595 Val Loss:   0.4532 Accuracy:   0.8704\n",
      "Epoch [21/50] Train Loss: 0.0569 Val Loss:   0.3693 Accuracy:   0.8708\n",
      "Epoch [22/50] Train Loss: 0.0557 Val Loss:   0.3636 Accuracy:   0.8907\n",
      "Epoch [23/50] Train Loss: 0.0550 Val Loss:   0.3755 Accuracy:   0.8946\n",
      "Epoch [24/50] Train Loss: 0.0495 Val Loss:   0.4632 Accuracy:   0.8757\n",
      "Epoch [25/50] Train Loss: 0.0530 Val Loss:   0.3768 Accuracy:   0.9118\n",
      "Epoch [26/50] Train Loss: 0.0505 Val Loss:   0.4971 Accuracy:   0.8406\n",
      "Epoch [27/50] Train Loss: 0.0489 Val Loss:   0.6068 Accuracy:   0.8690\n",
      "Epoch [28/50] Train Loss: 0.0486 Val Loss:   0.4964 Accuracy:   0.8553\n",
      "Epoch [29/50] Train Loss: 0.0435 Val Loss:   0.4882 Accuracy:   0.8706\n",
      "Epoch [30/50] Train Loss: 0.0463 Val Loss:   0.3570 Accuracy:   0.9036\n",
      "Epoch [31/50] Train Loss: 0.0435 Val Loss:   0.6225 Accuracy:   0.8391\n",
      "Epoch [32/50] Train Loss: 0.0406 Val Loss:   0.4922 Accuracy:   0.9024\n",
      "Epoch [33/50] Train Loss: 0.0434 Val Loss:   0.4298 Accuracy:   0.8857\n",
      "Epoch [34/50] Train Loss: 0.0431 Val Loss:   0.6585 Accuracy:   0.8329\n",
      "Epoch [35/50] Train Loss: 0.0383 Val Loss:   0.4941 Accuracy:   0.8639\n",
      "Epoch [36/50] Train Loss: 0.0354 Val Loss:   0.4973 Accuracy:   0.8742\n",
      "Epoch [37/50] Train Loss: 0.0409 Val Loss:   0.5033 Accuracy:   0.8651\n",
      "Epoch [38/50] Train Loss: 0.0440 Val Loss:   0.3754 Accuracy:   0.8920\n",
      "Epoch [39/50] Train Loss: 0.0353 Val Loss:   0.5331 Accuracy:   0.8742\n",
      "Epoch [40/50] Train Loss: 0.0379 Val Loss:   0.4311 Accuracy:   0.8922\n",
      "Epoch [41/50] Train Loss: 0.0342 Val Loss:   0.4922 Accuracy:   0.8960\n",
      "Epoch [42/50] Train Loss: 0.0321 Val Loss:   0.5727 Accuracy:   0.8736\n",
      "Epoch [43/50] Train Loss: 0.0300 Val Loss:   0.5919 Accuracy:   0.8826\n",
      "Epoch [44/50] Train Loss: 0.0316 Val Loss:   0.6259 Accuracy:   0.8960\n",
      "Epoch [45/50] Train Loss: 0.0300 Val Loss:   0.6645 Accuracy:   0.8857\n",
      "Epoch [46/50] Train Loss: 0.0324 Val Loss:   0.5113 Accuracy:   0.8770\n",
      "Epoch [47/50] Train Loss: 0.0317 Val Loss:   0.6427 Accuracy:   0.8636\n",
      "Epoch [48/50] Train Loss: 0.0296 Val Loss:   0.4858 Accuracy:   0.8890\n",
      "Epoch [49/50] Train Loss: 0.0297 Val Loss:   0.5235 Accuracy:   0.8846\n",
      "Epoch [50/50] Train Loss: 0.0277 Val Loss:   0.6757 Accuracy:   0.8895\n",
      "✅ Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso CNN\n",
    "trainer_cnn  = TrainerCNN(dir_save=\"output/CNN\", num_epochs=50)\n",
    "trainer_cnn.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from models.CNN_2D.ModelCNN_2D import CNN_2D\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.CNN_2D.TrainerCNN_2D import TrainerCNN_2D\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "input_size = 9\n",
    "sequence_length = 5\n",
    "output_size = 2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "column_to_remove = 'attack_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de treino: 17144\n",
      "Total de amostras no conjunto de teste: 8415\n",
      "Train Dataset Shape: torch.Size([17144, 9, 5, 1])\n",
      "Test Dataset Shape: torch.Size([8415, 9, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn2d')\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove, normalize=True, mode='cnn2d')\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no conjunto de treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no conjunto de teste: {len(test_loader)}\")\n",
    "\n",
    "# Modelo CNN_2D\n",
    "model = CNN_2D(input_channels=input_size, input_length=sequence_length, num_classes=output_size).to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Train Loss: 0.1535 Val Loss:   0.2818 Accuracy:   0.8742\n",
      "🔖 Melhor modelo salvo em: output/UEL/CNN_2D\\CNN_Epoca-1_Acc-0.87.pth\n",
      "Epoch [2/50] Train Loss: 0.1194 Val Loss:   0.3322 Accuracy:   0.8781\n",
      "Epoch [3/50] Train Loss: 0.1090 Val Loss:   0.2977 Accuracy:   0.8751\n",
      "Epoch [4/50] Train Loss: 0.1045 Val Loss:   0.2516 Accuracy:   0.8837\n",
      "🔖 Melhor modelo salvo em: output/UEL/CNN_2D\\CNN_Epoca-4_Acc-0.88.pth\n",
      "Epoch [5/50] Train Loss: 0.0987 Val Loss:   0.2976 Accuracy:   0.8759\n",
      "Epoch [6/50] Train Loss: 0.1008 Val Loss:   0.4745 Accuracy:   0.8689\n",
      "Epoch [7/50] Train Loss: 0.0929 Val Loss:   0.4517 Accuracy:   0.8764\n",
      "Epoch [8/50] Train Loss: 0.0893 Val Loss:   0.2662 Accuracy:   0.8843\n",
      "Epoch [9/50] Train Loss: 0.0874 Val Loss:   0.2693 Accuracy:   0.8844\n",
      "Epoch [10/50] Train Loss: 0.0859 Val Loss:   0.3042 Accuracy:   0.8816\n",
      "Epoch [11/50] Train Loss: 0.0868 Val Loss:   0.3121 Accuracy:   0.8875\n",
      "Epoch [12/50] Train Loss: 0.0811 Val Loss:   0.3228 Accuracy:   0.8799\n",
      "Epoch [13/50] Train Loss: 0.0825 Val Loss:   0.3739 Accuracy:   0.8736\n",
      "Epoch [14/50] Train Loss: 0.0804 Val Loss:   0.3797 Accuracy:   0.8837\n",
      "Epoch [15/50] Train Loss: 0.0750 Val Loss:   0.3979 Accuracy:   0.8632\n",
      "Epoch [16/50] Train Loss: 0.0752 Val Loss:   0.3587 Accuracy:   0.8511\n",
      "Epoch [17/50] Train Loss: 0.0730 Val Loss:   0.3222 Accuracy:   0.8775\n",
      "Epoch [18/50] Train Loss: 0.0716 Val Loss:   0.6597 Accuracy:   0.7056\n",
      "Epoch [19/50] Train Loss: 0.0700 Val Loss:   0.7259 Accuracy:   0.7153\n",
      "Epoch [20/50] Train Loss: 0.0677 Val Loss:   0.3275 Accuracy:   0.8723\n",
      "Epoch [21/50] Train Loss: 0.0672 Val Loss:   0.3383 Accuracy:   0.8898\n",
      "Epoch [22/50] Train Loss: 0.0670 Val Loss:   0.4202 Accuracy:   0.8482\n",
      "Epoch [23/50] Train Loss: 0.0669 Val Loss:   0.4374 Accuracy:   0.8712\n",
      "Epoch [24/50] Train Loss: 0.0618 Val Loss:   0.4993 Accuracy:   0.8471\n",
      "Epoch [25/50] Train Loss: 0.0609 Val Loss:   0.3626 Accuracy:   0.8707\n",
      "Epoch [26/50] Train Loss: 0.0599 Val Loss:   0.3597 Accuracy:   0.8668\n",
      "Epoch [27/50] Train Loss: 0.0574 Val Loss:   0.3588 Accuracy:   0.8812\n",
      "Epoch [28/50] Train Loss: 0.0583 Val Loss:   0.4543 Accuracy:   0.8509\n",
      "Epoch [29/50] Train Loss: 0.0554 Val Loss:   0.5567 Accuracy:   0.8329\n",
      "Epoch [30/50] Train Loss: 0.0540 Val Loss:   0.3965 Accuracy:   0.8630\n",
      "Epoch [31/50] Train Loss: 0.0539 Val Loss:   0.5835 Accuracy:   0.8635\n",
      "Epoch [32/50] Train Loss: 0.0553 Val Loss:   0.3859 Accuracy:   0.8831\n",
      "Epoch [33/50] Train Loss: 0.0521 Val Loss:   0.4987 Accuracy:   0.8632\n",
      "Epoch [34/50] Train Loss: 0.0503 Val Loss:   0.8683 Accuracy:   0.7919\n",
      "Epoch [35/50] Train Loss: 0.0508 Val Loss:   0.9992 Accuracy:   0.7346\n",
      "Epoch [36/50] Train Loss: 0.0491 Val Loss:   2.0701 Accuracy:   0.5942\n",
      "Epoch [37/50] Train Loss: 0.0460 Val Loss:   0.5516 Accuracy:   0.8248\n",
      "Epoch [38/50] Train Loss: 0.0482 Val Loss:   0.5162 Accuracy:   0.8557\n",
      "Epoch [39/50] Train Loss: 0.0495 Val Loss:   0.9497 Accuracy:   0.7569\n",
      "Epoch [40/50] Train Loss: 0.0454 Val Loss:   0.8492 Accuracy:   0.7958\n",
      "Epoch [41/50] Train Loss: 0.0396 Val Loss:   1.0965 Accuracy:   0.7110\n",
      "Epoch [42/50] Train Loss: 0.0445 Val Loss:   0.4740 Accuracy:   0.8841\n",
      "Epoch [43/50] Train Loss: 0.0410 Val Loss:   0.9466 Accuracy:   0.7872\n",
      "Epoch [44/50] Train Loss: 0.0417 Val Loss:   0.8933 Accuracy:   0.7988\n",
      "Epoch [45/50] Train Loss: 0.0384 Val Loss:   0.5856 Accuracy:   0.8731\n",
      "Epoch [46/50] Train Loss: 0.0410 Val Loss:   1.9058 Accuracy:   0.5952\n",
      "Epoch [47/50] Train Loss: 0.0381 Val Loss:   1.0861 Accuracy:   0.7659\n",
      "Epoch [48/50] Train Loss: 0.0356 Val Loss:   2.6956 Accuracy:   0.5879\n",
      "Epoch [49/50] Train Loss: 0.0370 Val Loss:   0.7433 Accuracy:   0.8248\n",
      "Epoch [50/50] Train Loss: 0.0353 Val Loss:   0.5593 Accuracy:   0.8385\n",
      "✅ Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "# Treinar CNN_2D\n",
    "trainer_cnn  = TrainerCNN_2D(dir_save=\"output/CNN_2D\", num_epochs=50)\n",
    "trainer_cnn.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Total de amostras no conjunto de treino: 17139\n",
      "Total de amostras no conjunto de teste: 8410\n",
      "Train Dataset Shape: torch.Size([17139, 10, 9])\n",
      "Test Dataset Shape: torch.Size([8410, 10, 9])\n",
      "Total de batches no treino: 134\n",
      "Total de batches no teste:  66\n",
      "ModelHybrid(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout(p=0.4, inplace=False)\n",
      "  (adapt): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (lstm): LSTM(9, 256, num_layers=3, batch_first=True)\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.Hybrid.ModelHybrid import ModelHybrid\n",
    "from models.Hybrid.TrainerHybrid import TrainerHybrid\n",
    "\n",
    "# Seed e dispositivo\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Hiperparâmetros\n",
    "input_size      = 9    # n_features\n",
    "hidden_size     = 256  # hidden_size da LSTM\n",
    "num_layers      = 3    # camadas LSTM\n",
    "output_size     = 2    # classes (normal, anomalia)\n",
    "batch_size      = 128\n",
    "sequence_length = 10\n",
    "column_to_remove = 'attack_name'\n",
    "\n",
    "# Datasets\n",
    "train_dataset = SequenceDataset(\n",
    "    path               = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length    = sequence_length,\n",
    "    column_to_remove   = column_to_remove,\n",
    "    normalize          = True,\n",
    "    mode               = 'lstm'\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    path               = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length    = sequence_length,\n",
    "    column_to_remove   = column_to_remove,\n",
    "    normalize          = True,\n",
    "    mode               = 'lstm'\n",
    ")\n",
    "\n",
    "print(f\"Total de amostras no conjunto de treino: {len(train_dataset)}\")\n",
    "print(f\"Total de amostras no conjunto de teste: {len(test_dataset)}\")\n",
    "print(\"Train Dataset Shape:\", train_dataset.sequences.shape)\n",
    "print(\"Test Dataset Shape:\", test_dataset.sequences.shape)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no teste:  {len(test_loader)}\")\n",
    "\n",
    "# Modelo híbrido \n",
    "model = ModelHybrid(\n",
    "    seq_len           = sequence_length,\n",
    "    n_features        = input_size,\n",
    "    lstm_hidden_size  = hidden_size,\n",
    "    lstm_num_layers   = num_layers,\n",
    "    num_classes       = output_size\n",
    ").to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar Hybrid\n",
    "trainer = TrainerHybrid(dir_save=\"output/Hybrid\", num_epochs=50)\n",
    "trainer.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint carregado: output/Hybrid\\Hybrid_Ep3_Acc0.89.pth\n",
      "SVM Accuracy: 0.8990487514863258\n"
     ]
    }
   ],
   "source": [
    "# Jupyter cell: carregar checkpoint do Hybrid e treinar SVM\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.Hybrid.ModelHybrid import ModelHybrid\n",
    "\n",
    "# --- 1) Configurações ---\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sequence_length = 10\n",
    "column_to_remove = 'attack_name'\n",
    "batch_size = 128\n",
    "input_size = 9\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "output_size = 2\n",
    "\n",
    "# --- 2) Datasets & Loaders (mesmo janelamento LSTM) ---\n",
    "train_ds = SequenceDataset(\n",
    "    path             = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "test_ds = SequenceDataset(\n",
    "    path             = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "# --- 3) Instancia e carrega checkpoint Hybrid ---\n",
    "model = ModelHybrid(\n",
    "    seq_len           = sequence_length,\n",
    "    n_features        = input_size,\n",
    "    lstm_hidden_size  = hidden_size,\n",
    "    lstm_num_layers   = num_layers,\n",
    "    num_classes       = output_size\n",
    ")\n",
    "ckpt_dir = \"output/Hybrid\"\n",
    "# seleciona último .pth salvo\n",
    "ckpts = sorted([f for f in os.listdir(ckpt_dir) if f.endswith(\".pth\")])\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpts[-1])\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device).eval()\n",
    "print(\"Checkpoint carregado:\", ckpt_path)\n",
    "\n",
    "# --- 4) Extrai features do train e do test ---\n",
    "def extract_all(loader):\n",
    "    feats, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            f = model.extract_features(X)\n",
    "            feats.append(f.cpu().numpy())\n",
    "            labs.append(y.numpy())\n",
    "    return np.vstack(feats), np.hstack(labs)\n",
    "\n",
    "X_train, y_train = extract_all(train_loader)\n",
    "X_test,  y_test  = extract_all(test_loader)\n",
    "\n",
    "# --- 5) (Opcional) PCA para reduzir dimensão ---\n",
    "pca = PCA(n_components=128, random_state=SEED)\n",
    "X_train_p = pca.fit_transform(X_train)\n",
    "X_test_p  = pca.transform (X_test)\n",
    "\n",
    "# --- 6) Treina o SVM e avalia ---\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=SEED)\n",
    "svm.fit(X_train_p, y_train)\n",
    "y_pred = svm.predict(X_test_p)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.Hybrid.ModelHybrid import ModelHybrid\n",
    "from models.Hybrid.TrainerHybridSVM import TrainerHybridSVM\n",
    "\n",
    "# seed & device\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parâmetros\n",
    "sequence_length = 10\n",
    "column_to_remove = 'attack_name'\n",
    "batch_size      = 128\n",
    "input_size      = 9\n",
    "hidden_size     = 256\n",
    "num_layers      = 3\n",
    "output_size     = 2\n",
    "\n",
    "# datasets & loaders\n",
    "train_ds = SequenceDataset(\n",
    "    'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length,\n",
    "    column_to_remove=column_to_remove,\n",
    "    normalize=True,\n",
    "    mode='lstm'\n",
    ")\n",
    "test_ds = SequenceDataset(\n",
    "    'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length,\n",
    "    column_to_remove=column_to_remove,\n",
    "    normalize=True,\n",
    "    mode='lstm'\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "# modelo & trainer\n",
    "model   = ModelHybrid(\n",
    "    seq_len           = sequence_length,\n",
    "    n_features        = input_size,\n",
    "    lstm_hidden_size  = hidden_size,\n",
    "    lstm_num_layers   = num_layers,\n",
    "    num_classes       = output_size\n",
    ")\n",
    "trainer = TrainerHybridSVM(\n",
    "    dir_save   = \"output/HybridSVM\",\n",
    "    num_epochs = 50,\n",
    "    C          = 1000,\n",
    "    margin     = 0.5,\n",
    "    lr         = 1e-4\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] TrainLoss=0.1252 ValLoss=0.2794 ValAcc=0.8549\n",
      "🔖 Salvo: output/HybridSVM\\HybridSVM_Ep1_Val0.2794.pth\n",
      "[2/50] TrainLoss=0.0911 ValLoss=0.3805 ValAcc=0.8765\n",
      "[3/50] TrainLoss=0.0778 ValLoss=0.3045 ValAcc=0.8678\n",
      "[4/50] TrainLoss=0.0796 ValLoss=0.3018 ValAcc=0.8816\n",
      "[5/50] TrainLoss=0.0822 ValLoss=0.4925 ValAcc=0.4075\n",
      "[6/50] TrainLoss=0.0833 ValLoss=0.7966 ValAcc=0.4065\n",
      "[7/50] TrainLoss=0.0857 ValLoss=0.1424 ValAcc=0.8636\n",
      "🔖 Salvo: output/HybridSVM\\HybridSVM_Ep7_Val0.1424.pth\n",
      "[8/50] TrainLoss=0.0948 ValLoss=0.4233 ValAcc=0.8836\n",
      "[9/50] TrainLoss=0.1004 ValLoss=0.7050 ValAcc=0.4065\n",
      "[10/50] TrainLoss=0.1579 ValLoss=0.4022 ValAcc=0.5357\n",
      "[11/50] TrainLoss=0.4153 ValLoss=0.5044 ValAcc=0.4065\n",
      "[12/50] TrainLoss=0.4985 ValLoss=0.5053 ValAcc=0.4065\n",
      "[13/50] TrainLoss=0.4984 ValLoss=0.5053 ValAcc=0.4065\n",
      "[14/50] TrainLoss=0.4984 ValLoss=0.5056 ValAcc=0.4065\n",
      "[15/50] TrainLoss=0.4984 ValLoss=0.5054 ValAcc=0.4065\n",
      "[16/50] TrainLoss=0.4983 ValLoss=0.5056 ValAcc=0.4065\n",
      "[17/50] TrainLoss=0.4983 ValLoss=0.5053 ValAcc=0.4065\n",
      "[18/50] TrainLoss=0.4984 ValLoss=0.5054 ValAcc=0.4065\n",
      "[19/50] TrainLoss=0.4984 ValLoss=0.5054 ValAcc=0.4065\n",
      "[20/50] TrainLoss=0.4984 ValLoss=0.5058 ValAcc=0.4065\n",
      "[21/50] TrainLoss=0.4983 ValLoss=0.5059 ValAcc=0.4065\n",
      "[22/50] TrainLoss=0.4983 ValLoss=0.5054 ValAcc=0.4065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# executar treinamento\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\OneDrive\\Documentos\\GitHub\\anomalyDetection\\models\\Hybrid\\TrainerHybridSVM.py:62\u001b[0m, in \u001b[0;36mTrainerHybridSVM.fit\u001b[1;34m(self, model, train_loader, test_loader, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     61\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 62\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     total_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(scores, y)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     64\u001b[0m     preds \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\OneDrive\\Documentos\\GitHub\\anomalyDetection\\models\\Hybrid\\ModelHybrid.py:50\u001b[0m, in \u001b[0;36mModelHybrid.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# CNN2D branch\u001b[39;00m\n\u001b[0;32m     49\u001b[0m x_c \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)                  \u001b[38;5;66;03m# [B,1,seq_len,n_features]\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m x_c \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     51\u001b[0m x_c \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x_c)))\n\u001b[0;32m     52\u001b[0m x_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x_c)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# executar treinamento\n",
    "trainer.fit(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Total de batches no treino: 134\n",
      "Total de batches no teste:  66\n",
      "[1/50] TrLoss=0.1272 ValLoss=0.0903 ValAcc=0.8459\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep1_Val0.0903.pth\n",
      "[2/50] TrLoss=0.0661 ValLoss=0.0817 ValAcc=0.8536\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep2_Val0.0817.pth\n",
      "[3/50] TrLoss=0.0548 ValLoss=0.0768 ValAcc=0.8629\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep3_Val0.0768.pth\n",
      "[4/50] TrLoss=0.0480 ValLoss=0.0738 ValAcc=0.8690\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep4_Val0.0738.pth\n",
      "[5/50] TrLoss=0.0422 ValLoss=0.0792 ValAcc=0.8521\n",
      "[6/50] TrLoss=0.0379 ValLoss=0.0749 ValAcc=0.8805\n",
      "[7/50] TrLoss=0.0346 ValLoss=0.0700 ValAcc=0.8809\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep7_Val0.0700.pth\n",
      "[8/50] TrLoss=0.0329 ValLoss=0.0782 ValAcc=0.8579\n",
      "[9/50] TrLoss=0.0306 ValLoss=0.1010 ValAcc=0.8798\n",
      "[10/50] TrLoss=0.0302 ValLoss=0.0819 ValAcc=0.8546\n",
      "[11/50] TrLoss=0.0311 ValLoss=0.0676 ValAcc=0.8728\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep11_Val0.0676.pth\n",
      "[12/50] TrLoss=0.0286 ValLoss=0.0985 ValAcc=0.8927\n",
      "[13/50] TrLoss=0.0289 ValLoss=0.0829 ValAcc=0.8937\n",
      "[14/50] TrLoss=0.0288 ValLoss=0.0823 ValAcc=0.8912\n",
      "[15/50] TrLoss=0.0281 ValLoss=0.0833 ValAcc=0.8929\n",
      "[16/50] TrLoss=0.0283 ValLoss=0.0659 ValAcc=0.8762\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep16_Val0.0659.pth\n",
      "[17/50] TrLoss=0.0279 ValLoss=0.0653 ValAcc=0.8778\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep17_Val0.0653.pth\n",
      "[18/50] TrLoss=0.0285 ValLoss=0.0647 ValAcc=0.8847\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep18_Val0.0647.pth\n",
      "[19/50] TrLoss=0.0269 ValLoss=0.0716 ValAcc=0.8879\n",
      "[20/50] TrLoss=0.0273 ValLoss=0.0669 ValAcc=0.8860\n",
      "[21/50] TrLoss=0.0264 ValLoss=0.0808 ValAcc=0.8952\n",
      "[22/50] TrLoss=0.0268 ValLoss=0.0647 ValAcc=0.8772\n",
      "[23/50] TrLoss=0.0260 ValLoss=0.0661 ValAcc=0.8835\n",
      "[24/50] TrLoss=0.0264 ValLoss=0.0657 ValAcc=0.8853\n",
      "[25/50] TrLoss=0.0267 ValLoss=0.0906 ValAcc=0.8956\n",
      "[26/50] TrLoss=0.0261 ValLoss=0.0671 ValAcc=0.8728\n",
      "[27/50] TrLoss=0.0259 ValLoss=0.0691 ValAcc=0.8757\n",
      "[28/50] TrLoss=0.0263 ValLoss=0.0984 ValAcc=0.9040\n",
      "[29/50] TrLoss=0.0262 ValLoss=0.0774 ValAcc=0.8555\n",
      "[30/50] TrLoss=0.0259 ValLoss=0.0747 ValAcc=0.8610\n",
      "[31/50] TrLoss=0.0264 ValLoss=0.0865 ValAcc=0.8420\n",
      "[32/50] TrLoss=0.0256 ValLoss=0.0749 ValAcc=0.8595\n",
      "[33/50] TrLoss=0.0258 ValLoss=0.0981 ValAcc=0.8325\n",
      "[34/50] TrLoss=0.0267 ValLoss=0.0857 ValAcc=0.8996\n",
      "[35/50] TrLoss=0.0259 ValLoss=0.0664 ValAcc=0.8855\n",
      "[36/50] TrLoss=0.0258 ValLoss=0.0803 ValAcc=0.8509\n",
      "[37/50] TrLoss=0.0261 ValLoss=0.0666 ValAcc=0.8725\n",
      "[38/50] TrLoss=0.0258 ValLoss=0.0640 ValAcc=0.8807\n",
      "🔖 Salvo: output/HybridAttnSVM\\HybridAttnSVM_Ep38_Val0.0640.pth\n",
      "[39/50] TrLoss=0.0267 ValLoss=0.0719 ValAcc=0.8895\n",
      "[40/50] TrLoss=0.0260 ValLoss=0.0854 ValAcc=0.8463\n",
      "[41/50] TrLoss=0.0265 ValLoss=0.0858 ValAcc=0.8447\n",
      "[42/50] TrLoss=0.0268 ValLoss=0.0669 ValAcc=0.8732\n",
      "[43/50] TrLoss=0.0266 ValLoss=0.0670 ValAcc=0.8730\n",
      "[44/50] TrLoss=0.0274 ValLoss=0.0691 ValAcc=0.8875\n",
      "[45/50] TrLoss=0.0265 ValLoss=0.1046 ValAcc=0.8243\n",
      "[46/50] TrLoss=0.0279 ValLoss=0.0698 ValAcc=0.8675\n",
      "[47/50] TrLoss=0.0278 ValLoss=0.0997 ValAcc=0.9076\n",
      "[48/50] TrLoss=0.0276 ValLoss=0.0733 ValAcc=0.8611\n",
      "[49/50] TrLoss=0.0283 ValLoss=0.0833 ValAcc=0.8931\n",
      "[50/50] TrLoss=0.0282 ValLoss=0.0656 ValAcc=0.8798\n",
      "✅ Treino finalizado.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Imports do seu projeto\n",
    "from models.Sequence import SequenceDataset\n",
    "from models.Hybrid.ModelHybridAttnSVM   import ModelHybridAttnSVM\n",
    "from models.Hybrid.TrainerHybridAttnSVM import TrainerHybridAttnSVM\n",
    "\n",
    "# 2) Seed e dispositivo\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# 3) Hiperparâmetros\n",
    "sequence_length  = 10\n",
    "column_to_remove = 'attack_name'\n",
    "batch_size       = 128\n",
    "n_features       = 9\n",
    "hidden_size      = 256\n",
    "num_layers       = 3\n",
    "num_classes      = 2\n",
    "num_epochs       = 50\n",
    "\n",
    "# 4) Criar os datasets (mesmos nomes de variável anteriores)\n",
    "train_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/treino_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    path             = 'data/cic_puro/teste_final_estratificado_random.csv',\n",
    "    sequence_length  = sequence_length,\n",
    "    column_to_remove = column_to_remove,\n",
    "    normalize        = True,\n",
    "    mode             = 'lstm'\n",
    ")\n",
    "\n",
    "# 5) Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "print(f\"Total de batches no treino: {len(train_loader)}\")\n",
    "print(f\"Total de batches no teste:  {len(test_loader)}\")\n",
    "\n",
    "# 6) Instanciar modelo e treinador\n",
    "model = ModelHybridAttnSVM(\n",
    "    seq_len       = sequence_length,\n",
    "    n_features    = n_features,\n",
    "    lstm_hidden   = hidden_size,\n",
    "    lstm_layers   = num_layers,\n",
    "    num_classes   = num_classes\n",
    ")\n",
    "trainer = TrainerHybridAttnSVM(\n",
    "    dir_save   = \"output/HybridAttnSVM\",\n",
    "    num_epochs = num_epochs,\n",
    "    C          = 10.0,\n",
    "    margin     = 0.5,\n",
    "    lr         = 1e-4\n",
    ")\n",
    "\n",
    "# 7) Executar treinamento\n",
    "trainer.fit(model, train_loader, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
