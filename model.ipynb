{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo. Abaixo, são listados os períodos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "DrDos_NTP.csv, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 12:45 - 13:09\n",
    "UDPLag.csv, 13:11 - 13:15\n",
    "Syn.csv, 13:29 - 13:34\n",
    "TFTP.csv, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```csv\n",
    "PortMap.csv, 09:43 - 09:51\n",
    "DrDos_NetBIOS.csv, 10:00 - 10:09\n",
    "DrDos_LDAP.csv, 10:21 - 10:30\n",
    "DrDos_MSSQL.csv, 10:33 - 10:42\n",
    "DrDos_UDP.csv, 10:53 - 11:03\n",
    "DrDos_UDP-Lag.csv, 11:14 - 11:24\n",
    "Syn.csv, 11:28 - 17:35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatena os dias da coleta em um único arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erro ao processar DrDos_DNS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "✔ DrDos_LDAP.csv processado com 2181542 linhas.\n",
      "✔ DrDos_MSSQL.csv processado com 4524498 linhas.\n",
      "❌ Erro ao processar DrDos_NetBIOS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "✔ DrDos_NTP.csv processado com 1217007 linhas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = \"data/01-12\"\n",
    "\n",
    "files = [\n",
    "    \"DrDos_DNS.csv\", \"DrDos_LDAP.csv\", \"DrDos_MSSQL.csv\",\n",
    "    \"DrDos_NetBIOS.csv\", \"DrDos_NTP.csv\", \"DrDos_SNMP.csv\",\n",
    "    \"DrDos_SSDP.csv\", \"DrDos_UDP.csv\", \"Syn.csv\",\n",
    "    \"TFTP.csv\", \"UDPLag.csv\"\n",
    "]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "            df = df.rename(columns=lambda x: x.strip())\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "            all_data.append(df)\n",
    "            print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "    output_file = os.path.join(data_path, \"combined_attacks_01_12.csv\")\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Arquivo combinado salvo corretamente em ordem cronológica: {output_file}\")\n",
    "else:\n",
    "    print(\"Nenhum dado válido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/01-12/combined_attacks_01_12.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = \"data/03-11\"\n",
    "\n",
    "files = [\n",
    "    \"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\",\n",
    "    \"Syn.csv\", \"UDP.csv\", \"UDPLag.csv\"\n",
    "]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "            df = df.rename(columns=lambda x: x.strip())\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "            all_data.append(df)\n",
    "            print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "    output_file = os.path.join(data_path, \"combined_attacks_03_11.csv\")\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Arquivo combinado salvo corretamente em ordem cronológica: {output_file}\")\n",
    "else:\n",
    "    print(\"Nenhum dado válido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/03-11/combined_attacks_03_11.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização e criar sequência "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados preparados para LSTM salvos em: data/03-11/03_11_processado.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_path = \"data/03-11/combined_attacks_03_11.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "def create_sequences(data, seq_length=10):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        labels.append(data[i+seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "data_values = df[features].values\n",
    "seq_length = 10\n",
    "X, y = create_sequences(data_values, seq_length)\n",
    "\n",
    "processed_file = \"data/03-11/03_11_processado.csv\"\n",
    "pd.DataFrame(X.reshape(X.shape[0], -1)).to_csv(processed_file, index=False)\n",
    "\n",
    "print(f\"Dados preparados para LSTM salvos em: {processed_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas:\n",
      "['Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packets']\n",
      "                                  Flow ID       Source IP  Source Port  \\\n",
      "0          192.168.50.254-224.0.0.5-0-0-0  192.168.50.254            0   \n",
      "1          192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "2  172.217.10.98-192.168.50.6-443-54799-6    192.168.50.6        54799   \n",
      "3    172.217.7.2-192.168.50.6-443-54800-6    192.168.50.6        54800   \n",
      "4  172.217.10.98-192.168.50.6-443-54801-6    192.168.50.6        54801   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0      224.0.0.5                 0  0.000000  2018-11-03 09:18:16.964447   \n",
      "1      224.0.0.5                 0  0.000000  2018-11-03 09:18:18.506537   \n",
      "2  172.217.10.98               443  0.352941  2018-11-03 09:18:18.610576   \n",
      "3    172.217.7.2               443  0.352941  2018-11-03 09:18:18.610579   \n",
      "4  172.217.10.98               443  0.352941  2018-11-03 09:18:18.610581   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0       0.953828           0.000505  \n",
      "1       0.952916           0.000631  \n",
      "2       0.303635           0.000057  \n",
      "3       0.303629           0.000057  \n",
      "4       0.303628           0.000057  \n"
     ]
    }
   ],
   "source": [
    "print(\"Nomes das colunas:\")\n",
    "print(df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# 📌 1️⃣ Carregar Dataset Já Processado (sequenciado e normalizado)\n",
    "data_path = \"data/01-12/03_11_processado.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 📌 2️⃣ Definir Features e Rótulos\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]  \n",
    "label = \"Attack Type\"  \n",
    "\n",
    "X = df[features].values  \n",
    "y = df[label].values  \n",
    "\n",
    "# 📌 3️⃣ Separar Treino e Teste (80% treino, 20% teste)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# 📌 4️⃣ Criar Modelo Híbrido CNN + LSTM\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 📌 5️⃣ Treinar Modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 📌 6️⃣ Extração de Características da CNN+LSTM para o SVM\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "# 📌 7️⃣ Treinar o SVM\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# 📌 8️⃣ Fazer Predições\n",
    "y_pred = svm.predict(X_test_features)\n",
    "\n",
    "# 📌 9️⃣ Avaliação do Modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"🔹 Precisão do Modelo Híbrido: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n🔹 Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 🔟 Matriz de Confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Ataque\"], yticklabels=[\"Normal\", \"Ataque\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusão - CNN+LSTM+SVM\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 1️⃣1️⃣ Gráfico: Comparação Predições vs Reais\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test[:100], label=\"Real\", linestyle=\"dashed\")\n",
    "plt.plot(y_pred[:100], label=\"Previsto\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"🔹 Predições vs Valores Reais\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 1️⃣2️⃣ Salvar Modelos\n",
    "joblib.dump(svm, \"svm_model.pkl\")\n",
    "model.save(\"cnn_lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Abrir aquivo e fazer o plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(\"data\", \"01-12\")\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    plt.plot(df[\"Timestamp\"], df[\" Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Pacotes Enviados\")\n",
    "    plt.title(f\"Tráfego de Pacotes - {file_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
