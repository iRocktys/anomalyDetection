{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset cont√©m m√∫ltiplos cen√°rios de ataques, registrados em arquivos CSV, com detalhes sobre tr√°fego malicioso e leg√≠timo. Abaixo, s√£o listados os per√≠odos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "DrDos_NTP.csv, 1217007, 88, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 5074413, 88, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 2181542, 88, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 4524498, 88, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 4094986, 88, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 5161377, 88, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 2611374, 88, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 3136802, 88, 12:45 - 13:09\n",
    "UDPLag.csv, 370605, 88, 13:11 - 13:15\n",
    "Syn.csv, 1582681, 88, 13:29 - 13:34\n",
    "TFTP.csv, 20107827, 88, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "NetBIOS.csv, 3455899, 88, 10:00 - 10:09\n",
    "LDAP.csv, 2113234, 88, 10:21 - 10:30\n",
    "MSSQL.csv, 5775786, 88, 10:33 - 10:42\n",
    "UDP.csv, 3782206, 88,  10:53 - 11:03\n",
    "UDPLag.csv, 725165, 88, 11:14 - 11:24\n",
    "Syn.csv, 4320541, 88, 11:28 - 17:35\n",
    "```\n",
    "\n",
    "Portmap.csv n√£o tem per√≠odo de ataque "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequ√™ncias aleat√≥rias!\n"
     ]
    }
   ],
   "source": [
    "# selected_columns = [\n",
    "#     ' Source IP',\n",
    "#     ' Source Port',\n",
    "#     ' Destination IP',\n",
    "#     ' Destination Port',\n",
    "#     ' Timestamp',\n",
    "#     ' Flow Duration',\n",
    "#     ' Total Fwd Packets',\n",
    "#     ' Total Backward Packets',\n",
    "#     'Total Length of Fwd Packets',\n",
    "#     ' Total Length of Bwd Packets',\n",
    "#     'Flow Bytes/s',\n",
    "#     ' Flow Packets/s',\n",
    "#     'Fwd Packets/s',\n",
    "#     ' Bwd Packets/s',\n",
    "# ]\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Fun√ß√£o para criar sequ√™ncias aleat√≥rias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequ√™ncias aleat√≥rias!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes CIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def contar_rotulos_csv(lista_arquivos, coluna_label=' Label'):\n",
    "    for arquivo in lista_arquivos:\n",
    "        try:\n",
    "            print(f\"\\nüìÑ Analisando: {arquivo}\")\n",
    "            df = pd.read_csv(arquivo, usecols=[coluna_label], low_memory=False)\n",
    "            \n",
    "            total_linhas = len(df)\n",
    "            contagem_rotulos = df[coluna_label].value_counts(dropna=False)\n",
    "            soma_rotulos = contagem_rotulos.sum()\n",
    "\n",
    "            print(f\"  ‚û§ Total de linhas: {total_linhas}\")\n",
    "            print(\"  ‚û§ Contagem de r√≥tulos:\")\n",
    "            for rotulo, qtd in contagem_rotulos.items():\n",
    "                print(f\"     - {rotulo}: {qtd}\")\n",
    "\n",
    "            if soma_rotulos == total_linhas:\n",
    "                print(\"  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\")\n",
    "            else:\n",
    "                print(\"  ‚ùå A soma de todos os r√≥tulos N√ÉO √© igual ao total de linhas.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao ler {arquivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/LDAP.csv\n",
      "  ‚û§ Total de linhas: 2113234\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - LDAP: 1905191\n",
      "     - NetBIOS: 202919\n",
      "     - BENIGN: 5124\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/Mssql.csv\n",
      "  ‚û§ Total de linhas: 5775786\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - MSSQL: 5763061\n",
      "     - LDAP: 9931\n",
      "     - BENIGN: 2794\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/NetBIOS.csv\n",
      "  ‚û§ Total de linhas: 3455899\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - NetBIOS: 3454578\n",
      "     - BENIGN: 1321\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/Syn.csv\n",
      "  ‚û§ Total de linhas: 4320541\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - Syn: 4284751\n",
      "     - BENIGN: 35790\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/UDP.csv\n",
      "  ‚û§ Total de linhas: 3782206\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - UDP: 3754680\n",
      "     - MSSQL: 24392\n",
      "     - BENIGN: 3134\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/UDPLag.csv\n",
      "  ‚û§ Total de linhas: 725165\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - Syn: 606749\n",
      "     - UDP: 112475\n",
      "     - BENIGN: 4068\n",
      "     - UDPLag: 1873\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n"
     ]
    }
   ],
   "source": [
    "csv_files1 = [\n",
    "    'data/03-11/attacks_labeled/LDAP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Mssql_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/NetBIOS_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Syn_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDPLag_labeled.csv'\n",
    "]\n",
    "\n",
    "csv_files2 = [\n",
    "    'data/01-12/attacks_labeled/DrDos_DNS_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_LDAP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_MSSQL_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NetBIOS_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NTP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SNMP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SSDP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_UDP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/Syn_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/UDPLag_labeled.csv'   \n",
    "]\n",
    "\n",
    "csv_files3 = [\n",
    "    'data/Full_dataset/03-11/LDAP.csv',\n",
    "    'data/Full_dataset/03-11/Mssql.csv',\n",
    "    'data/Full_dataset/03-11/NetBIOS.csv',\n",
    "    'data/Full_dataset/03-11/Syn.csv',\n",
    "    'data/Full_dataset/03-11/UDP.csv',\n",
    "    'data/Full_dataset/03-11/UDPLag.csv'\n",
    "]\n",
    "\n",
    "csv_files4 = [\n",
    "    'data/Full_dataset/01-12/DrDos_DNS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_LDAP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_MSSQL.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NetBIOS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NTP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SNMP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SSDP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_UDP.csv',\n",
    "    'data/Full_dataset/01-12/Syn.csv',\n",
    "    'data/Full_dataset/01-12/UDPLag.csv'   \n",
    "]\n",
    "    \n",
    "contar_rotulos_csv(csv_files3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def visualizar_primeiras_linhas_formatado(caminho_csv, n_linhas=3):\n",
    "    try:\n",
    "        print(f\"\\nüìÇ Lendo primeiras {n_linhas} linhas do arquivo: {caminho_csv}\")\n",
    "        df = pd.read_csv(caminho_csv, nrows=n_linhas, low_memory=False)\n",
    "\n",
    "        for idx, linha in df.iterrows():\n",
    "            print(f\"\\nüßæ Linha {idx + 1}:\")\n",
    "            for coluna, valor in linha.items():\n",
    "                print(f\"  {coluna:<30} ‚ûú {valor}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao ler o arquivo {caminho_csv}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Lendo primeiras 3 linhas do arquivo: saida_por_segundo.csv\n",
      "\n",
      "üßæ Linha 1:\n",
      "  Protocol                       ‚ûú 0.0\n",
      "  Flow Duration                  ‚ûú 114456999.0\n",
      "  Total Fwd Packets              ‚ûú 45.0\n",
      "  Total Backward Packets         ‚ûú 0.0\n",
      "  Total Length of Fwd Packets    ‚ûú 0.0\n",
      "  Total Length of Bwd Packets    ‚ûú 0.0\n",
      "  Fwd Packet Length Max          ‚ûú 0.0\n",
      "  Fwd Packet Length Min          ‚ûú 0.0\n",
      "  Fwd Packet Length Mean         ‚ûú 0.0\n",
      "  Fwd Packet Length Std          ‚ûú 0.0\n",
      "  Bwd Packet Length Max          ‚ûú 0.0\n",
      "  Bwd Packet Length Min          ‚ûú 0.0\n",
      "  Bwd Packet Length Mean         ‚ûú 0.0\n",
      "  Bwd Packet Length Std          ‚ûú 0.0\n",
      "  Flow Bytes/s                   ‚ûú 0.0\n",
      "  Flow Packets/s                 ‚ûú 0.3931607537604581\n",
      "  Flow IAT Mean                  ‚ûú 2601295.4318181816\n",
      "  Flow IAT Std                   ‚ûú 4295632.000149808\n",
      "  Flow IAT Max                   ‚ûú 10001143.0\n",
      "  Flow IAT Min                   ‚ûú 1.0\n",
      "  Fwd IAT Total                  ‚ûú 114456999.0\n",
      "  Fwd IAT Mean                   ‚ûú 2601295.4318181816\n",
      "  Fwd IAT Std                    ‚ûú 4295632.000149808\n",
      "  Fwd IAT Max                    ‚ûú 10001143.0\n",
      "  Fwd IAT Min                    ‚ûú 1.0\n",
      "  Bwd IAT Total                  ‚ûú 0.0\n",
      "  Bwd IAT Mean                   ‚ûú 0.0\n",
      "  Bwd IAT Std                    ‚ûú 0.0\n",
      "  Bwd IAT Max                    ‚ûú 0.0\n",
      "  Bwd IAT Min                    ‚ûú 0.0\n",
      "  Fwd PSH Flags                  ‚ûú 0.0\n",
      "  Bwd PSH Flags                  ‚ûú 0.0\n",
      "  Fwd URG Flags                  ‚ûú 0.0\n",
      "  Bwd URG Flags                  ‚ûú 0.0\n",
      "  Fwd Header Length              ‚ûú 0.0\n",
      "  Bwd Header Length              ‚ûú 0.0\n",
      "  Fwd Packets/s                  ‚ûú 0.3931607537604581\n",
      "  Bwd Packets/s                  ‚ûú 0.0\n",
      "  Min Packet Length              ‚ûú 0.0\n",
      "  Max Packet Length              ‚ûú 0.0\n",
      "  Packet Length Mean             ‚ûú 0.0\n",
      "  Packet Length Std              ‚ûú 0.0\n",
      "  Packet Length Variance         ‚ûú 0.0\n",
      "  FIN Flag Count                 ‚ûú 0.0\n",
      "  SYN Flag Count                 ‚ûú 0.0\n",
      "  RST Flag Count                 ‚ûú 0.0\n",
      "  PSH Flag Count                 ‚ûú 0.0\n",
      "  ACK Flag Count                 ‚ûú 0.0\n",
      "  URG Flag Count                 ‚ûú 0.0\n",
      "  CWE Flag Count                 ‚ûú 0.0\n",
      "  ECE Flag Count                 ‚ûú 0.0\n",
      "  Down/Up Ratio                  ‚ûú 0.0\n",
      "  Average Packet Size            ‚ûú 0.0\n",
      "  Avg Fwd Segment Size           ‚ûú 0.0\n",
      "  Avg Bwd Segment Size           ‚ûú 0.0\n",
      "  Fwd Header Length.1            ‚ûú 0.0\n",
      "  Fwd Avg Bytes/Bulk             ‚ûú 0.0\n",
      "  Fwd Avg Packets/Bulk           ‚ûú 0.0\n",
      "  Fwd Avg Bulk Rate              ‚ûú 0.0\n",
      "  Bwd Avg Bytes/Bulk             ‚ûú 0.0\n",
      "  Bwd Avg Packets/Bulk           ‚ûú 0.0\n",
      "  Bwd Avg Bulk Rate              ‚ûú 0.0\n",
      "  Subflow Fwd Packets            ‚ûú 45.0\n",
      "  Subflow Fwd Bytes              ‚ûú 0.0\n",
      "  Subflow Bwd Packets            ‚ûú 0.0\n",
      "  Subflow Bwd Bytes              ‚ûú 0.0\n",
      "  Init_Win_bytes_forward         ‚ûú -1.0\n",
      "  Init_Win_bytes_backward        ‚ûú -1.0\n",
      "  act_data_pkt_fwd               ‚ûú 0.0\n",
      "  min_seg_size_forward           ‚ûú 0.0\n",
      "  Active Mean                    ‚ûú 8185.583333333334\n",
      "  Active Std                     ‚ûú 28337.11228778624\n",
      "  Active Max                     ‚ûú 98168.0\n",
      "  Active Min                     ‚ûú 3.0\n",
      "  Idle Mean                      ‚ûú 9529897.25\n",
      "  Idle Std                       ‚ûú 351582.63126924314\n",
      "  Idle Max                       ‚ûú 10001143.0\n",
      "  Idle Min                       ‚ûú 9048097.0\n",
      "  SimillarHTTP                   ‚ûú 0.0\n",
      "  Inbound                        ‚ûú 0.0\n",
      "  Attack_Label                   ‚ûú 0.0\n",
      "  entropy_source_ip              ‚ûú 0.0\n",
      "  entropy_destination_ip         ‚ûú 0.0\n",
      "  entropy_source_port            ‚ûú 0.0\n",
      "  entropy_destination_port       ‚ûú 0.0\n"
     ]
    }
   ],
   "source": [
    "# Caminho para o arquivo CSV processado\n",
    "caminho_saida = 'saida_por_segundo.csv'\n",
    "\n",
    "# Visualizar as primeiras linhas do dataset formatado\n",
    "visualizar_primeiras_linhas_formatado(caminho_saida, n_linhas=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Lendo primeiras 1 linhas do arquivo: data/Full_dataset/01-12/DrDos_LDAP.csv\n",
      "‚ùå Erro ao ler o arquivo data/Full_dataset/01-12/DrDos_LDAP.csv: [Errno 2] No such file or directory: 'data/Full_dataset/01-12/DrDos_LDAP.csv'\n"
     ]
    }
   ],
   "source": [
    "csv_files4 = [\n",
    "    'data/Full_dataset/01-12/DrDos_DNS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_LDAP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_MSSQL.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NetBIOS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NTP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SNMP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SSDP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_UDP.csv',\n",
    "    'data/Full_dataset/01-12/Syn.csv',\n",
    "    'data/Full_dataset/01-12/UDPLag.csv'   \n",
    "]\n",
    "\n",
    "visualizar_primeiras_linhas_formatado('data/Full_dataset/01-12/DrDos_LDAP.csv', n_linhas=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corre√ß√£o do CICDDoS - Calculo da entropia e concatena√ß√£o de tempo em 1 segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Fun√ß√£o para calcular entropia\n",
    "def calcular_entropia(valores):\n",
    "    contagem = Counter(valores)\n",
    "    probs = [v / sum(contagem.values()) for v in contagem.values()]\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "# Fun√ß√£o para processar dataset individual\n",
    "def processar_dataset(caminho_csv, linhas=None, caminho_saida=\"dataset_processado.csv\"):\n",
    "    try:\n",
    "        print(f\"\\nüì• Lendo o arquivo: {caminho_csv}\")\n",
    "        df = pd.read_csv(caminho_csv, nrows=linhas, low_memory=False)\n",
    "\n",
    "        # Calcular entropias\n",
    "        df['src_ip_entropy'] = calcular_entropia(df[' Source IP']) if ' Source IP' in df.columns else 0\n",
    "        df['dst_ip_entropy'] = calcular_entropia(df[' Destination IP']) if ' Destination IP' in df.columns else 0\n",
    "        df['src_port_entropy'] = calcular_entropia(df[' Source Port']) if ' Source Port' in df.columns else 0\n",
    "        df['dst_port_entropy'] = calcular_entropia(df[' Destination Port']) if ' Destination Port' in df.columns else 0\n",
    "\n",
    "        # Arredondar timestamp\n",
    "        if ' Timestamp' in df.columns:\n",
    "            df['segundo'] = pd.to_datetime(df[' Timestamp']).dt.floor('s')\n",
    "        else:\n",
    "            print(\"‚ùå Coluna ' Timestamp' n√£o encontrada.\")\n",
    "            return\n",
    "\n",
    "        # Converter label\n",
    "        if ' Label' in df.columns:\n",
    "            df['attack_label'] = df[' Label'].apply(lambda x: 0 if str(x).strip().upper() == 'BENIGN' else 1)\n",
    "\n",
    "        # Agrupar por segundo e calcular m√©dias\n",
    "        colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "        df_agrupado = df.groupby('segundo')[colunas_numericas].mean().reset_index()\n",
    "\n",
    "        # M√©todos para determinar a defini√ß√£o de ataque\n",
    "\n",
    "        # 1. M√©dia e Desvio Padr√£o\n",
    "        mean = df_agrupado['attack_label'].mean()\n",
    "        std_dev = df_agrupado['attack_label'].std()\n",
    "        print(f\"\\nüìä M√©dia: {mean}, Desvio Padr√£o: {std_dev}\")\n",
    "\n",
    "        # 2. Coeficiente de Varia√ß√£o (CV)\n",
    "        cv = std_dev / mean if mean != 0 else 0\n",
    "        print(f\"Coeficiente de Varia√ß√£o (CV): {cv}\")\n",
    "\n",
    "        # 3. Percentil 75 para determinar o limiar de ataque\n",
    "        percentil_75 = df_agrupado['attack_label'].quantile(0.75)\n",
    "        print(f\"Percentil 75 (Limiar de ataque): {percentil_75}\")\n",
    "\n",
    "        # Definir o limiar de ataque como sendo o percentil 75\n",
    "        limiar_ataque = percentil_75\n",
    "        df_agrupado['definicao_ataque'] = df_agrupado['attack_label'].apply(lambda x: 1 if x >= limiar_ataque else 0)\n",
    "\n",
    "        # Exibir resumo de ataques e benignos\n",
    "        benignos = df_agrupado[df_agrupado['definicao_ataque'] == 0].shape[0]\n",
    "        ataques = df_agrupado[df_agrupado['definicao_ataque'] == 1].shape[0]\n",
    "        print(f\"\\nüë• Benignos: {benignos}, Ataques: {ataques}\")\n",
    "\n",
    "        # Corrigir attack_label\n",
    "        if 'attack_label' in df_agrupado.columns:\n",
    "            df_agrupado['attack_label'] = df_agrupado['attack_label'].apply(lambda x: 1 if x > 0.6 else 0)\n",
    "\n",
    "        # Adicionar nome do ataque\n",
    "        if ' Label' in df.columns:\n",
    "            df_nome_ataque = df[['segundo', ' Label']].drop_duplicates()\n",
    "            df_agrupado = df_agrupado.merge(df_nome_ataque, on='segundo', how='left')\n",
    "            df_agrupado.rename(columns={' Label': 'attack_name'}, inplace=True)\n",
    "\n",
    "        # Remover colunas desnecess√°rias\n",
    "        colunas_remover = ['segundo', 'Unnamed: 0', 'Flow ID', 'Source IP', 'Destination IP', ' Timestamp',\n",
    "                           ' Source Port', ' Destination Port', ' Protocol', 'attack_name']\n",
    "        df_agrupado.drop(columns=[col for col in colunas_remover if col in df_agrupado.columns], errors='ignore', inplace=True)\n",
    "\n",
    "        # Salvar\n",
    "        df_agrupado.to_csv(caminho_saida, index=False)\n",
    "        print(f\"‚úÖ Salvo: {caminho_saida}\")\n",
    "        return df_agrupado\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar o arquivo {caminho_csv}: {e}\")\n",
    "\n",
    "# Fun√ß√£o para processar m√∫ltiplos CSVs\n",
    "def processar_varios_csvs(pasta_entrada, pasta_saida, linhas=None):\n",
    "    arquivos_csv = glob(os.path.join(pasta_entrada, \"*.csv\"))\n",
    "    os.makedirs(pasta_saida, exist_ok=True)\n",
    "\n",
    "    for caminho_csv in arquivos_csv:\n",
    "        nome_arquivo = os.path.basename(caminho_csv).replace(\".csv\", \"_processado.csv\")\n",
    "        caminho_saida = os.path.join(pasta_saida, nome_arquivo)\n",
    "        processar_dataset(caminho_csv, linhas=linhas, caminho_saida=caminho_saida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\LDAP.csv\n",
      "\n",
      "üìä M√©dia: 0.6700977912512468, Desvio Padr√£o: 0.46287904616275516\n",
      "Coeficiente de Varia√ß√£o (CV): 0.6907634261835719\n",
      "Percentil 75 (Limiar de ataque): 0.9997529644268774\n",
      "\n",
      "üë• Benignos: 634, Ataques: 215\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\LDAP_processado.csv\n",
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\MSSQL.csv\n",
      "\n",
      "üìä M√©dia: 0.9095790316551348, Desvio Padr√£o: 0.24588941688490515\n",
      "Coeficiente de Varia√ß√£o (CV): 0.2703332072612396\n",
      "Percentil 75 (Limiar de ataque): 1.0\n",
      "\n",
      "üë• Benignos: 445, Ataques: 265\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\MSSQL_processado.csv\n",
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\NetBIOS.csv\n",
      "\n",
      "üìä M√©dia: 0.9996172652785831, Desvio Padr√£o: 0.000984025821502838\n",
      "Coeficiente de Varia√ß√£o (CV): 0.0009844025865525642\n",
      "Percentil 75 (Limiar de ataque): 1.0\n",
      "\n",
      "üë• Benignos: 174, Ataques: 259\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\NetBIOS_processado.csv\n",
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\Syn.csv\n",
      "\n",
      "üìä M√©dia: 0.4694747803844438, Desvio Padr√£o: 0.48310873035199237\n",
      "Coeficiente de Varia√ß√£o (CV): 1.029040857011284\n",
      "Percentil 75 (Limiar de ataque): 1.0\n",
      "\n",
      "üë• Benignos: 4115, Ataques: 2971\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\Syn_processado.csv\n",
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\UDP.csv\n",
      "\n",
      "üìä M√©dia: 0.9107268668750302, Desvio Padr√£o: 0.24561875297807614\n",
      "Coeficiente de Varia√ß√£o (CV): 0.2696952971431114\n",
      "Percentil 75 (Limiar de ataque): 1.0\n",
      "\n",
      "üë• Benignos: 511, Ataques: 546\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\UDP_processado.csv\n",
      "\n",
      "üì• Lendo o arquivo: data/03-11/attacks\\UDPLag.csv\n",
      "\n",
      "üìä M√©dia: 0.667629897581233, Desvio Padr√£o: 0.42311950420084476\n",
      "Coeficiente de Varia√ß√£o (CV): 0.6337635653133138\n",
      "Percentil 75 (Limiar de ataque): 1.0\n",
      "\n",
      "üë• Benignos: 581, Ataques: 585\n",
      "‚úÖ Salvo: data/03-11/attacks_labeled\\UDPLag_processado.csv\n"
     ]
    }
   ],
   "source": [
    "# Processar no m√°ximo 10000 linhas de cada arquivo\n",
    "# processar_varios_csvs(\"data/01-12/attacks\", \"data/01-12/attacks_labeled\", linhas=None)\n",
    "processar_varios_csvs(\"data/03-11/attacks\", \"data/03-11/attacks_labeled\", linhas=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Analisando 10 arquivos na pasta: data/01-12/attacks_labeled\n",
      "\n",
      "üìÑ DrDoS_DNS_processado.csv\n",
      "   ‚úÖ Benignos:   192\n",
      "   ‚ùå Maliciosos: 1960\n",
      "\n",
      "üìÑ DrDoS_LDAP_processado.csv\n",
      "   ‚úÖ Benignos:   0\n",
      "   ‚ùå Maliciosos: 919\n",
      "\n",
      "üìÑ DrDoS_MSSQL_processado.csv\n",
      "   ‚úÖ Benignos:   92\n",
      "   ‚ùå Maliciosos: 893\n",
      "\n",
      "üìÑ DrDoS_NetBIOS_processado.csv\n",
      "   ‚úÖ Benignos:   60\n",
      "   ‚ùå Maliciosos: 901\n",
      "\n",
      "üìÑ DrDoS_NTP_processado.csv\n",
      "   ‚úÖ Benignos:   1770\n",
      "   ‚ùå Maliciosos: 2511\n",
      "\n",
      "üìÑ DrDoS_SNMP_processado.csv\n",
      "   ‚úÖ Benignos:   101\n",
      "   ‚ùå Maliciosos: 1275\n",
      "\n",
      "üìÑ DrDoS_SSDP_processado.csv\n",
      "   ‚úÖ Benignos:   3\n",
      "   ‚ùå Maliciosos: 986\n",
      "\n",
      "üìÑ DrDoS_UDP_processado.csv\n",
      "   ‚úÖ Benignos:   83\n",
      "   ‚ùå Maliciosos: 1773\n",
      "\n",
      "üìÑ Syn_processado.csv\n",
      "   ‚úÖ Benignos:   0\n",
      "   ‚ùå Maliciosos: 306\n",
      "\n",
      "üìÑ UDPLag_processado.csv\n",
      "   ‚úÖ Benignos:   473\n",
      "   ‚ùå Maliciosos: 1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def contar_labels(pasta_entrada):\n",
    "    arquivos_csv = glob(os.path.join(pasta_entrada, \"*.csv\"))\n",
    "    \n",
    "    if not arquivos_csv:\n",
    "        print(\"‚ùå Nenhum arquivo CSV encontrado na pasta especificada.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Analisando {len(arquivos_csv)} arquivos na pasta: {pasta_entrada}\\n\")\n",
    "    \n",
    "    for arquivo in arquivos_csv:\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, low_memory=False)\n",
    "            \n",
    "            if 'attack_label' not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è  Arquivo {os.path.basename(arquivo)} n√£o possui a coluna 'attack_label'. Ignorado.\")\n",
    "                continue\n",
    "            \n",
    "            benignos = (df['attack_label'] == 0).sum()\n",
    "            maliciosos = (df['attack_label'] == 1).sum()\n",
    "\n",
    "            print(f\"üìÑ {os.path.basename(arquivo)}\")\n",
    "            print(f\"   ‚úÖ Benignos:   {benignos}\")\n",
    "            print(f\"   ‚ùå Maliciosos: {maliciosos}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {arquivo}: {e}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "pasta_csv = \"data/01-12/attacks_labeled\"  # Altere aqui\n",
    "contar_labels(pasta_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Analisando 10 arquivos na pasta: data/01-12/attacks_labeled\n",
      "\n",
      "üìÑ DrDoS_DNS_processado.csv\n",
      "   ‚úÖ Benignos:   1126\n",
      "   ‚ùå Maliciosos: 1026\n",
      "\n",
      "üìÑ DrDoS_LDAP_processado.csv\n",
      "   ‚úÖ Benignos:   652\n",
      "   ‚ùå Maliciosos: 267\n",
      "\n",
      "üìÑ DrDoS_MSSQL_processado.csv\n",
      "   ‚úÖ Benignos:   538\n",
      "   ‚ùå Maliciosos: 447\n",
      "\n",
      "üìÑ DrDoS_NetBIOS_processado.csv\n",
      "   ‚úÖ Benignos:   456\n",
      "   ‚ùå Maliciosos: 505\n",
      "\n",
      "üìÑ DrDoS_NTP_processado.csv\n",
      "   ‚úÖ Benignos:   2626\n",
      "   ‚ùå Maliciosos: 1655\n",
      "\n",
      "üìÑ DrDoS_SNMP_processado.csv\n",
      "   ‚úÖ Benignos:   683\n",
      "   ‚ùå Maliciosos: 693\n",
      "\n",
      "üìÑ DrDoS_SSDP_processado.csv\n",
      "   ‚úÖ Benignos:   331\n",
      "   ‚ùå Maliciosos: 658\n",
      "\n",
      "üìÑ DrDoS_UDP_processado.csv\n",
      "   ‚úÖ Benignos:   683\n",
      "   ‚ùå Maliciosos: 1173\n",
      "\n",
      "üìÑ Syn_processado.csv\n",
      "   ‚úÖ Benignos:   136\n",
      "   ‚ùå Maliciosos: 170\n",
      "\n",
      "üìÑ UDPLag_processado.csv\n",
      "   ‚úÖ Benignos:   746\n",
      "   ‚ùå Maliciosos: 842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def contar_labels(pasta_entrada):\n",
    "    arquivos_csv = glob(os.path.join(pasta_entrada, \"*.csv\"))\n",
    "    \n",
    "    if not arquivos_csv:\n",
    "        print(\"‚ùå Nenhum arquivo CSV encontrado na pasta especificada.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Analisando {len(arquivos_csv)} arquivos na pasta: {pasta_entrada}\\n\")\n",
    "    \n",
    "    for arquivo in arquivos_csv:\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, low_memory=False)\n",
    "            \n",
    "            if 'definicao_ataque' not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è  Arquivo {os.path.basename(arquivo)} n√£o possui a coluna 'definicao_ataque'. Ignorado.\")\n",
    "                continue\n",
    "            \n",
    "            benignos = (df['definicao_ataque'] == 0).sum()\n",
    "            maliciosos = (df['definicao_ataque'] == 1).sum()\n",
    "\n",
    "            print(f\"üìÑ {os.path.basename(arquivo)}\")\n",
    "            print(f\"   ‚úÖ Benignos:   {benignos}\")\n",
    "            print(f\"   ‚ùå Maliciosos: {maliciosos}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {arquivo}: {e}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "pasta_csv = \"data/01-12/attacks_labeled\"  # Altere aqui\n",
    "contar_labels(pasta_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise de correla√ß√£o (Random Forest - RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_csv_with_feature_importance(file_paths, label_column='definicao_ataque', output_file='importancia_colunas_rf.csv'):\n",
    "    importance_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\nüîÑ Processando arquivo: {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "            if label_column not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Coluna '{label_column}' n√£o encontrada, pulando arquivo.\")\n",
    "                continue\n",
    "\n",
    "            X = df.drop(columns=[label_column])\n",
    "            y = df[label_column]\n",
    "\n",
    "            X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            X.dropna(inplace=True)\n",
    "            y = y.loc[X.index]\n",
    "\n",
    "            if len(X) < 10 or len(set(y)) < 2:\n",
    "                print(\"‚ö†Ô∏è Poucos dados ou classes √∫nicas, pulando arquivo.\")\n",
    "                continue\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "            model = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "            model.fit(X_scaled, y)\n",
    "\n",
    "            importances = model.feature_importances_\n",
    "            total_importance = importances.sum()\n",
    "\n",
    "            if total_importance == 0:\n",
    "                print(\"‚ö†Ô∏è Import√¢ncia total zero, pulando arquivo.\")\n",
    "                continue\n",
    "\n",
    "            importances_percent = 100.0 * (importances / total_importance)\n",
    "            file_name = os.path.basename(file_path)\n",
    "\n",
    "            row = {'arquivo': file_name}\n",
    "            for col, imp in zip(X.columns, importances_percent):\n",
    "                row[col] = imp\n",
    "\n",
    "            importance_data.append(row)\n",
    "            print(f\"‚úÖ Import√¢ncias salvas para: {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {file_path}: {e}\")\n",
    "\n",
    "    if importance_data:\n",
    "        df_output = pd.DataFrame(importance_data).fillna(0)\n",
    "        df_output.to_csv(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Arquivo final salvo como: {output_file}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum dado foi processado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/LDAP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: LDAP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/Mssql_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: Mssql_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/NetBIOS_processado.csv\n",
      "‚ö†Ô∏è Poucos dados ou classes √∫nicas, pulando arquivo.\n",
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/Syn_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: Syn_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/UDP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: UDP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/03-11/attacks_labeled/UDPLag_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: UDPLag_processado.csv\n",
      "\n",
      "‚úÖ Arquivo final salvo como: data/correlacao_03-11.csv\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    'data/03-11/attacks_labeled/LDAP_processado.csv',\n",
    "    'data/03-11/attacks_labeled/Mssql_processado.csv',\n",
    "    'data/03-11/attacks_labeled/NetBIOS_processado.csv',\n",
    "    'data/03-11/attacks_labeled/Syn_processado.csv',\n",
    "    'data/03-11/attacks_labeled/UDP_processado.csv',\n",
    "    'data/03-11/attacks_labeled/UDPLag_processado.csv'\n",
    "]\n",
    "\n",
    "analyze_csv_with_feature_importance(csv_files, output_file='data/correlacao_03-11.csv', label_column='definicao_ataque')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_DNS_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_DNS_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_LDAP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_LDAP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_MSSQL_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_MSSQL_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_NetBIOS_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_NetBIOS_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_NTP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_NTP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_SNMP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_SNMP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_SSDP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_SSDP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/DrDos_UDP_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: DrDos_UDP_processado.csv\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/Syn_processado.csv\n",
      "‚ö†Ô∏è Poucos dados ou classes √∫nicas, pulando arquivo.\n",
      "\n",
      "üîÑ Processando arquivo: data/01-12/attacks_labeled/UDPLag_processado.csv\n",
      "‚úÖ Import√¢ncias salvas para: UDPLag_processado.csv\n",
      "\n",
      "‚úÖ Arquivo final salvo como: data/correlacao_01-12.csv\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    'data/01-12/attacks_labeled/DrDos_DNS_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_LDAP_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_MSSQL_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NetBIOS_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NTP_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SNMP_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SSDP_processado.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_UDP_processado.csv',\n",
    "    'data/01-12/attacks_labeled/Syn_processado.csv',\n",
    "    'data/01-12/attacks_labeled/UDPLag_processado.csv'    \n",
    "]\n",
    "\n",
    "analyze_csv_with_feature_importance(csv_files, output_file='data/correlacao_01-12.csv', label_column='definicao_ataque')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, path, sequence_length, column_to_remove=None):\n",
    "        df = pd.read_csv(path, sep=';')\n",
    "\n",
    "        if column_to_remove and column_to_remove in df.columns:\n",
    "            df = df.drop(columns=[column_to_remove])\n",
    "\n",
    "        # Separar features e labels\n",
    "        features = df.iloc[:, :-1].values  # Assume que a √∫ltima coluna √© a label\n",
    "        labels = df.iloc[:, -1].values\n",
    "\n",
    "        # Normalizar os dados \n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "\n",
    "        # Criar sequ√™ncias\n",
    "        sequences = []\n",
    "        sequence_labels = []\n",
    "        for i in range(len(features) - sequence_length + 1):\n",
    "            seq = features[i:i+sequence_length]\n",
    "            label = labels[i+sequence_length-1]  \n",
    "            sequences.append(seq)\n",
    "            sequence_labels.append(label)\n",
    "\n",
    "        # Converter para tensores\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32).view(-1, sequence_length, features.shape[1])\n",
    "        self.labels = torch.tensor(sequence_labels, dtype=torch.long)\n",
    "        # self.labels = torch.tensor(sequence_labels, dtype=torch.float32).unsqueeze(1) # Para bin√°rio / nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n",
    "    \n",
    "    \n",
    "class LSTM_model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Primeira camada LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,\n",
    "                             hidden_size=hidden_size//2,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True\n",
    "                             \n",
    "                             )\n",
    "        \n",
    "        # Segunda camada LSTM\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size//2,\n",
    "                             hidden_size=hidden_size,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,  \n",
    "                             dropout=0.2  # Dropout entre as camadas LSTM                           \n",
    "                            )\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_size,\n",
    "                             hidden_size=hidden_size//2,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.2  # Dropout entre as camadas LSTM\n",
    "                             )\n",
    "        \n",
    "        # Camada fully-connected\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Para bin√°rio\n",
    "        # self.fc = torch.nn.Linear(hidden_size, 1) # nn.BCEWithLogitsLoss() / nn.BCELoss()\n",
    "        self.fc = torch.nn.Linear(hidden_size//2, output_size) # nn.CrossEntropyLoss()\n",
    "        # Camada de ativa√ß√£o softmax\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)  # Primeira camada LSTM\n",
    "      \n",
    "        out, _ = self.lstm2(out)  # Segunda camada LSTM\n",
    "        \n",
    "        out, _ = self.lstm3(out)  # Terceira camada LSTM\n",
    "        \n",
    "        out = torch.sigmoid(out)  # Aplicar sigmoid para obter probabilidades / Remover para nn.BCEWithLogitsLoss()\n",
    "        # out = self.softmax(out)  # Aplicar softmax para obter probabilidades / Remover para nn.BCEWithLogitsLoss()\n",
    "        out = self.fc(out[:, -1, :])  # Usar a √∫ltima sa√≠da do LSTM como entrada para fc\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        print(f\"Modelo salvo em: {filename}\")\n",
    "\n",
    "\n",
    "# out = self.dropout(out)  # Aplicar dropout se necess√°rio\n",
    "# out = self.batch_norm(out)  # Aplicar batch normalization se necess√°rio\n",
    "# out = self.relu(out)  # Aplicar ReLU se necess√°rio\n",
    "# out = self.tanh(out)  # Aplicar Tanh se necess√°rio\n",
    "# out = self.sigmoid(out)  # Aplicar Sigmoid se necess√°rio\n",
    "# out = self.leaky_relu(out)  # Aplicar Leaky ReLU se necess√°rio\n",
    "# out = self.prelu(out)  # Aplicar PReLU se necess√°rio\n",
    "# out = self.elu(out)  # Aplicar ELU se necess√°rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader - Inputs shape: torch.Size([64, 1, 9]), Labels shape: torch.Size([64])\n",
      "Test Loader - Inputs shape: torch.Size([64, 1, 9]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Configurar os par√¢metros da rede LSTM\n",
    "input_size = 9         # N√∫mero de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / N¬∫ de unidades ocultas por c√©lula\n",
    "num_layers = 2         # N√∫mero de camadas LSTM / N¬∫ de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 64        # Batch size / \n",
    "num_epochs = 100         # N√∫mero de epochs\n",
    "lr = 0.0011             # Learning rate\n",
    "sequence_length = 1   # Tamanho da sequ√™ncia de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida\n",
    "\n",
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove)\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Formato dos loaders\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Train Loader - Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Test Loader - Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_model(\n",
      "  (lstm1): LSTM(9, 128, num_layers=2, batch_first=True)\n",
      "  (lstm2): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (lstm3): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Dimens√£o da entrada: torch.Size([64, 1, 9])\n",
      "Dimens√£o da sa√≠da: torch.Size([64])\n",
      "Dimens√£o da sa√≠da do modelo: torch.Size([64, 2])\n",
      "Epoch [1/100], Loss: 0.4529, Val Loss: 0.8944, Accuracy: 0.4654\n",
      "Melhor modelo salvo!\n",
      "Epoch [2/100], Loss: 0.3712, Val Loss: 0.7187, Accuracy: 0.5864\n",
      "Melhor modelo salvo!\n",
      "Epoch [3/100], Loss: 0.4228, Val Loss: 0.5879, Accuracy: 0.7041\n",
      "Melhor modelo salvo!\n",
      "Epoch [4/100], Loss: 0.2935, Val Loss: 0.9191, Accuracy: 0.5445\n",
      "Epoch [5/100], Loss: 0.4467, Val Loss: 0.7766, Accuracy: 0.6225\n",
      "Epoch [6/100], Loss: 0.3432, Val Loss: 0.7228, Accuracy: 0.6538\n",
      "Epoch [7/100], Loss: 0.2184, Val Loss: 0.6046, Accuracy: 0.6659\n",
      "Epoch [8/100], Loss: 0.2359, Val Loss: 0.4953, Accuracy: 0.7686\n",
      "Melhor modelo salvo!\n",
      "Epoch [9/100], Loss: 0.2682, Val Loss: 0.5518, Accuracy: 0.7944\n",
      "Epoch [10/100], Loss: 0.2680, Val Loss: 0.5005, Accuracy: 0.7794\n",
      "Epoch [11/100], Loss: 0.1998, Val Loss: 0.5326, Accuracy: 0.8034\n",
      "Epoch [12/100], Loss: 0.3016, Val Loss: 0.5341, Accuracy: 0.7984\n",
      "Epoch [13/100], Loss: 0.2001, Val Loss: 0.4714, Accuracy: 0.8265\n",
      "Melhor modelo salvo!\n",
      "Epoch [14/100], Loss: 0.2007, Val Loss: 0.4505, Accuracy: 0.8265\n",
      "Melhor modelo salvo!\n",
      "Epoch [15/100], Loss: 0.2051, Val Loss: 0.4261, Accuracy: 0.8433\n",
      "Melhor modelo salvo!\n",
      "Epoch [16/100], Loss: 0.2579, Val Loss: 0.5191, Accuracy: 0.7983\n",
      "Epoch [17/100], Loss: 0.1826, Val Loss: 0.5191, Accuracy: 0.8379\n",
      "Epoch [18/100], Loss: 0.2479, Val Loss: 0.4665, Accuracy: 0.8290\n",
      "Epoch [19/100], Loss: 0.1912, Val Loss: 0.5088, Accuracy: 0.8285\n",
      "Epoch [20/100], Loss: 0.4203, Val Loss: 0.4472, Accuracy: 0.8525\n",
      "Epoch [21/100], Loss: 0.2189, Val Loss: 0.5225, Accuracy: 0.8451\n",
      "Epoch [22/100], Loss: 0.2013, Val Loss: 0.4905, Accuracy: 0.8449\n",
      "Epoch [23/100], Loss: 0.1649, Val Loss: 0.5073, Accuracy: 0.8319\n",
      "Epoch [24/100], Loss: 0.2598, Val Loss: 0.5309, Accuracy: 0.8355\n",
      "Epoch [25/100], Loss: 0.2067, Val Loss: 0.5012, Accuracy: 0.8301\n",
      "Epoch [26/100], Loss: 0.2301, Val Loss: 0.5490, Accuracy: 0.8286\n",
      "Epoch [27/100], Loss: 0.1349, Val Loss: 0.6600, Accuracy: 0.7743\n",
      "Epoch [28/100], Loss: 0.2314, Val Loss: 0.4987, Accuracy: 0.8172\n",
      "Epoch [29/100], Loss: 0.2482, Val Loss: 0.6339, Accuracy: 0.7474\n",
      "Epoch [30/100], Loss: 0.2281, Val Loss: 0.5454, Accuracy: 0.8143\n",
      "Epoch [31/100], Loss: 0.2001, Val Loss: 0.5154, Accuracy: 0.8195\n",
      "Epoch [32/100], Loss: 0.2861, Val Loss: 0.4970, Accuracy: 0.8085\n",
      "Epoch [33/100], Loss: 0.2350, Val Loss: 0.5891, Accuracy: 0.8006\n",
      "Epoch [34/100], Loss: 0.1649, Val Loss: 0.6676, Accuracy: 0.7746\n",
      "Epoch [35/100], Loss: 0.2630, Val Loss: 0.5974, Accuracy: 0.8006\n",
      "Epoch [36/100], Loss: 0.1488, Val Loss: 0.6979, Accuracy: 0.7628\n",
      "Epoch [37/100], Loss: 0.1134, Val Loss: 0.8045, Accuracy: 0.7554\n",
      "Epoch [38/100], Loss: 0.1802, Val Loss: 0.7584, Accuracy: 0.7753\n",
      "Epoch [39/100], Loss: 0.1743, Val Loss: 0.6824, Accuracy: 0.7809\n",
      "Epoch [40/100], Loss: 0.1232, Val Loss: 0.7726, Accuracy: 0.7838\n",
      "Epoch [41/100], Loss: 0.3256, Val Loss: 0.6867, Accuracy: 0.7653\n",
      "Epoch [42/100], Loss: 0.2321, Val Loss: 0.8652, Accuracy: 0.7674\n",
      "Epoch [43/100], Loss: 0.3512, Val Loss: 0.8105, Accuracy: 0.7681\n",
      "Epoch [44/100], Loss: 0.2419, Val Loss: 0.8076, Accuracy: 0.7487\n",
      "Epoch [45/100], Loss: 0.2782, Val Loss: 0.8433, Accuracy: 0.7556\n",
      "Epoch [46/100], Loss: 0.2519, Val Loss: 0.7498, Accuracy: 0.7718\n",
      "Epoch [47/100], Loss: 0.2783, Val Loss: 0.8171, Accuracy: 0.7652\n",
      "Epoch [48/100], Loss: 0.2934, Val Loss: 0.9520, Accuracy: 0.7521\n",
      "Epoch [49/100], Loss: 0.1380, Val Loss: 0.8541, Accuracy: 0.7563\n",
      "Epoch [50/100], Loss: 0.1832, Val Loss: 0.9031, Accuracy: 0.7595\n",
      "Epoch [51/100], Loss: 0.2300, Val Loss: 0.8494, Accuracy: 0.7666\n",
      "Epoch [52/100], Loss: 0.1675, Val Loss: 0.5982, Accuracy: 0.8185\n",
      "Epoch [53/100], Loss: 0.2019, Val Loss: 0.8313, Accuracy: 0.7560\n",
      "Epoch [54/100], Loss: 0.1762, Val Loss: 0.8372, Accuracy: 0.7652\n",
      "Epoch [55/100], Loss: 0.2996, Val Loss: 0.8138, Accuracy: 0.7609\n",
      "Epoch [56/100], Loss: 0.2089, Val Loss: 0.9199, Accuracy: 0.7557\n",
      "Epoch [57/100], Loss: 0.2739, Val Loss: 0.8316, Accuracy: 0.7671\n",
      "Epoch [58/100], Loss: 0.2469, Val Loss: 0.8775, Accuracy: 0.7651\n",
      "Epoch [59/100], Loss: 0.2369, Val Loss: 0.8349, Accuracy: 0.7630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels) \u001b[38;5;66;03m#  nn.BCELoss()\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Valida√ß√£o\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Criar o modelo\n",
    "model = LSTM_model(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "print(model)\n",
    "\n",
    "# Testar a dimens√£o da entrada e sa√≠da do modelo\n",
    "x, y = next(iter(train_loader))  \n",
    "output = model(x)\n",
    "print(f\"Dimens√£o da entrada: {x.size()}\")   # shape: [batch_size, sequence_length, input_size]\n",
    "print(f\"Dimens√£o da sa√≠da: {y.size()}\")    # shape: [batch_size]\n",
    "print(f\"Dimens√£o da sa√≠da do modelo: {output.size()}\") # shape: [batch_size, output_size]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss: Para classifica√ß√£o multi-classe (softmax j√° inclu√≠do)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # Adam: muito usado, bom para a maioria dos casos\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=lr) # RMSprop: bom para dados sequenciais e LSTM\n",
    "\n",
    "\n",
    "# Treinamento do modelo\n",
    "best_loss = float('inf') # Inicializa a melhor perda como infinito\n",
    "for epoch in range(num_epochs):\n",
    "    # Treinamento\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.permute(0, 1, 2)\n",
    "        outputs = model(inputs)   \n",
    "        # labels = labels.view(-1, 1).float() # Ajustar o formato de labels para [batch_size, 1] / nn.BCEWithLogitsLoss()\n",
    "        # loss = criterion(outputs.squeeze(1), labels.float()) # nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(outputs.squeeze(), labels) #  nn.BCELoss()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Valida√ß√£o\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.permute(0, 1, 2)\n",
    "            outputs = model(inputs)\n",
    "            # labels = labels.view(-1, 1).float() # Ajustar o formato de labels para [batch_size, 1] / nn.BCEWithLogitsLoss()\n",
    "            val_loss += criterion(outputs.squeeze(), labels).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Calcular a acur√°cia\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.permute(0, 1, 2)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "   \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {acc:.4f}')\n",
    "    \n",
    "    # Salvar o melhor modelo\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f'best_lstm_model_{acc:.2f}.pth')\n",
    "        print('Melhor modelo salvo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo em uso: cuda\n"
     ]
    }
   ],
   "source": [
    "# üì¶ C√âLULA 1 ‚Äì IMPORTA√á√ïES E CONFIGURA√á√ïES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo em uso: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ C√âLULA 2 ‚Äì DATASET COM REMO√á√ÉO DE COLUNA E DIVIS√ÉO EM SEQU√äNCIAS\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, path, sequence_length, column_to_remove=None):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Remover coluna opcional (como timestamp ou id)\n",
    "        if column_to_remove and column_to_remove in df.columns:\n",
    "            df = df.drop(columns=[column_to_remove])\n",
    "\n",
    "        # Separar features e labels\n",
    "        data = df.drop(columns=['label']).values\n",
    "        labels = df['label'].values\n",
    "\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i in range(len(data) - sequence_length + 1):\n",
    "            self.sequences.append(data[i:i+sequence_length])\n",
    "            self.labels.append(labels[i+sequence_length-1])\n",
    "\n",
    "        # Formato final: (batch, channels=1, time_steps)\n",
    "        self.sequences = torch.tensor(self.sequences, dtype=torch.float32).unsqueeze(1)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† C√âLULA 3 ‚Äì DEFINI√á√ÉO DO MODELO CNN 1D\n",
    "class CNN1DNet(nn.Module):\n",
    "    def __init__(self, input_length, output_size=2):\n",
    "        super(CNN1DNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_length // 4) * 64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "input_size = 9     \n",
    "output_size = 2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "column_to_remove = 'attack_name'\n",
    "\n",
    "train_dataset = SequenceDataset('data/cic_puro_enhanced/01-12-train.csv', sequence_length, column_to_remove)\n",
    "test_dataset  = SequenceDataset('data/cic_puro_enhanced/03-11-test.csv', sequence_length, column_to_remove)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 20, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     11\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mCNN1DNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 20, 9]"
     ]
    }
   ],
   "source": [
    "model = CNN1DNet(input_length=sequence_length, output_size=output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # üîç Valida√ß√£o\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predicted = torch.argmax(outputs, dim=1).cpu()\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(y_batch.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"√âpoca {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(\"‚úÖ Novo melhor modelo salvo com accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_model(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=64, num_layers=2, output_size=2):\n",
    "        super(CNN_LSTM_model, self).__init__()\n",
    "        \n",
    "        # Camada CNN para extrair caracter√≠sticas locais\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Camada LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # CamadaÂÖ®ËøûÊé• ap√≥s a LSTM\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # A CNN espera (batch_size, channels, sequence_length), ent√£o precisamos transpor\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, seq_length, input_size] -> [batch_size, input_size, seq_length]\n",
    "        \n",
    "        out = self.cnn(x)\n",
    "        out = out.permute(0, 2, 1)  # Volta para [batch_size, sequence_length, channels]\n",
    "        \n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])  # Pega o √∫ltimo timestep\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
