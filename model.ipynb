{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo. Abaixo, são listados os períodos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "DrDos_NTP.csv, 1217007, 88, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 5074413, 88, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 2181542, 88, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 4524498, 88, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 4094986, 88, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 5161377, 88, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 2611374, 88, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 3136802, 88, 12:45 - 13:09\n",
    "UDPLag.csv, 370605, 88, 13:11 - 13:15\n",
    "Syn.csv, 1582681, 88, 13:29 - 13:34\n",
    "TFTP.csv, 20107827, 88, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "NetBIOS.csv, 3455899, 88, 10:00 - 10:09\n",
    "LDAP.csv, 2113234, 88, 10:21 - 10:30\n",
    "MSSQL.csv, 5775786, 88, 10:33 - 10:42\n",
    "UDP.csv, 3782206, 88,  10:53 - 11:03\n",
    "UDPLag.csv, 725165, 88, 11:14 - 11:24\n",
    "Syn.csv, 4320541, 88, 11:28 - 17:35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento CICDDoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class DatasetBuilder:\n",
    "    def __init__(self, data_path, file_list, output_filename=\"combined_output.csv\", chunk_size=100_000):\n",
    "        self.data_path = data_path\n",
    "        self.file_list = file_list\n",
    "        self.chunk_size = chunk_size\n",
    "        self.output_file = os.path.join(data_path, output_filename)\n",
    "\n",
    "    def inspect_files(self):\n",
    "        for file_name in self.file_list:\n",
    "            file_path = os.path.join(self.data_path, file_name)\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    total_rows = 0\n",
    "                    column_count = None\n",
    "\n",
    "                    for chunk in enumerate(pd.read_csv(file_path, chunksize=self.chunk_size, low_memory=False)):\n",
    "                        chunk.columns = chunk.columns.str.strip()\n",
    "                        total_rows += len(chunk)\n",
    "                        if column_count is None:\n",
    "                            column_count = len(chunk.columns)\n",
    "\n",
    "                    print(f\"✅ {file_name}: {total_rows} linhas, {column_count} colunas\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erro ao inspecionar {file_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "    def concatenate_selected_columns(self, selected_columns):\n",
    "        if os.path.exists(self.output_file):\n",
    "            os.remove(self.output_file)\n",
    "\n",
    "        first_chunk = True\n",
    "\n",
    "        for file_name in self.file_list:\n",
    "            file_path = os.path.join(self.data_path, file_name)\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=self.chunk_size, usecols=selected_columns, low_memory=False):\n",
    "                        chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "                        if \"Timestamp\" in chunk.columns:\n",
    "                            chunk[\"Timestamp\"] = pd.to_datetime(chunk[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "                        chunk.to_csv(self.output_file, mode='a', header=first_chunk, index=False)\n",
    "                        first_chunk = False\n",
    "\n",
    "                    print(f\"✅ {file_name} processado e salvo com colunas selecionadas.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erro ao processar {file_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "    def sort_output_by_timestamp(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.output_file, low_memory=False)\n",
    "            if \"Timestamp\" in df.columns:\n",
    "                df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "                df = df.sort_values(by=\"Timestamp\")\n",
    "                df.to_csv(self.output_file, index=False)\n",
    "                print(f\"✅ Arquivo ordenado por Timestamp salvo em: {self.output_file}\")\n",
    "            else:\n",
    "                print(\"⚠️ Coluna 'Timestamp' não encontrada para ordenação.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao ordenar o arquivo: {e}\")\n",
    "            \n",
    "    def label_anomaly_period(self, file_name, start_time_str, end_time_str):\n",
    "        file_path = os.path.join(self.data_path, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ Arquivo não encontrado: {file_name}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "            if \" Timestamp\" not in df.columns:\n",
    "                print(\"❌ Coluna 'Timestamp' não encontrada.\")\n",
    "                return\n",
    "\n",
    "            df[\" Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "            df[\"TimeOnly\"] = df[\" Timestamp\"].dt.time\n",
    "\n",
    "            from datetime import time\n",
    "\n",
    "            start_time = time.fromisoformat(start_time_str)\n",
    "            end_time = time.fromisoformat(end_time_str)\n",
    "\n",
    "            # Criação da coluna 'label'\n",
    "            df[\"label\"] = df[\"TimeOnly\"].apply(lambda t: 1 if start_time <= t <= end_time else 0)\n",
    "\n",
    "            df.drop(columns=[\"TimeOnly\"], inplace=True)\n",
    "\n",
    "            # Novo nome de arquivo com \"_labeled\"\n",
    "            labeled_file_path = os.path.join(self.data_path, file_name.replace(\".csv\", \"_labeled.csv\"))\n",
    "            df.to_csv(labeled_file_path, index=False)\n",
    "\n",
    "            print(f\"✅ Rotulagem aplicada e salva em: {labeled_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao rotular {file_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de arquivos\n",
    "files_1day = [\n",
    "    \"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\",\n",
    "    \"Syn.csv\", \"UDP.csv\", \"UDPLag.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "files_2day = [\n",
    "    \"DrDos_DNS.csv\", \"DrDos_LDAP.csv\", \"DrDos_MSSQL.csv\",\n",
    "    \"DrDos_NetBIOS.csv\", \"DrDos_NTP.csv\", \"DrDos_SNMP.csv\",\n",
    "    \"DrDos_SSDP.csv\", \"DrDos_UDP.csv\", \"Syn.csv\",\n",
    "    \"TFTP.csv\", \"UDPLag.csv\"\n",
    "]\n",
    "\n",
    "selected_columns = ['Timestamp', 'Flow Duration', 'Total Fwd Packets', 'Label']\n",
    "\n",
    "# Caminho para os arquivos\n",
    "data_path_1day = \"data/03-11\"\n",
    "data_path_2day = \"data/01-12\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rotulagem aplicada e salva em: data/01-12\\DrDos_NTP_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "# Criando o objeto\n",
    "builder = DatasetBuilder(data_path_2day, files_2day, output_filename=\"output_2day.csv\")\n",
    "builder.label_anomaly_period(\"DrDos_NTP.csv\", \"10:35:00\", \"10:45:00\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def analyze_rfe_feature_importance(file_path, label_column='label', top_k=15, sample_frac=1.0, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Amostragem dos dados\n",
    "    if sample_frac < 1.0:\n",
    "        df = df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Salvar colunas categóricas (ex: IPs)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    preserved_cols = df[categorical_cols]\n",
    "\n",
    "    # Limpar dados\n",
    "    df.dropna(axis=1, how='all', inplace=True)  # remove colunas 100% nulas\n",
    "\n",
    "    # Trabalhar só com numéricas\n",
    "    df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "    if label_column not in df_numeric.columns:\n",
    "        raise ValueError(f\"Coluna '{label_column}' não encontrada no dataset.\")\n",
    "\n",
    "    # X e y\n",
    "    X = df_numeric.drop(columns=[label_column])\n",
    "    y = df_numeric[label_column]\n",
    "\n",
    "    # Remover infs e nulos\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.dropna(inplace=True)\n",
    "    y = y.loc[X.index]\n",
    "    preserved_cols = preserved_cols.loc[X.index]\n",
    "\n",
    "    # Normalização (opcional para RF, mas ajuda no desempenho)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Estimador\n",
    "    estimator = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "\n",
    "    # RFE com o número de features que queremos ranquear\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=top_k)\n",
    "    rfe.fit(X_scaled, y)\n",
    "\n",
    "    # Obter os nomes das colunas selecionadas\n",
    "    selected_features = X.columns[rfe.support_]\n",
    "    feature_ranking = pd.Series(rfe.ranking_, index=X.columns)\n",
    "\n",
    "    # Plotar as top features selecionadas\n",
    "    top_features = feature_ranking[feature_ranking == 1].index\n",
    "    importances = estimator.fit(X, y).feature_importances_\n",
    "    importance_series = pd.Series(importances, index=X.columns).loc[top_features]\n",
    "\n",
    "    # Gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=importance_series.values, y=importance_series.index, palette='magma')\n",
    "    plt.title(f'Top {top_k} Features Relevantes (RFE + Random Forest)')\n",
    "    plt.xlabel(\"Importância\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return top_features, preserved_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DrDos_NTP_labeled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_rfe_features, preserved_data \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_rfe_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDrDos_NTP_labeled.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usa apenas 10% dos dados\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36manalyze_rfe_feature_importance\u001b[1;34m(file_path, label_column, top_k, sample_frac, random_state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalyze_rfe_feature_importance\u001b[39m(file_path, label_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, sample_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Amostragem dos dados\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_frac \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DrDos_NTP_labeled.csv'"
     ]
    }
   ],
   "source": [
    "top_rfe_features, preserved_data = analyze_rfe_feature_importance(\n",
    "    data_path_2day + \"DrDos_NTP_labeled.csv\",\n",
    "    top_k=20,\n",
    "    sample_frac=0.1  # Usa apenas 10% dos dados\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos - dataset UEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Usando dispositivo: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]  # [1, seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # última saída da sequência\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = NetworkDataset(X_train, y_train)\n",
    "test_dataset = NetworkDataset(X_test, y_test)\n",
    "\n",
    "# Split em treino e validação\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(input_size=X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 30\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss += criterion(val_outputs, y_val).item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_lstm_model.pt\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Parando cedo (early stopping)\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o melhor modelo\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = (outputs > 0.5).int().cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y_batch.numpy().astype(int))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos CSVs já normalizados\n",
    "train_df = pd.read_csv('train_final.csv', sep=';')\n",
    "test_df = pd.read_csv('test_final.csv', sep=';')\n",
    "\n",
    "# Features selecionadas\n",
    "features = ['bytes', 'src_ip_entropy', 'dst_ip_entropy', 'src_port_entropy',\n",
    "            'dst_port_entropy', 'packets', 'flow_count', 'bytes_mean', 'packets_mean']\n",
    "\n",
    "# Separação de variáveis e labels\n",
    "X_train = train_df[features].values.astype(np.float32)\n",
    "y_train = train_df['label'].values.astype(np.float32)\n",
    "\n",
    "X_test = test_df[features].values.astype(np.float32)\n",
    "y_test = test_df['label'].values.astype(np.float32)\n",
    "\n",
    "# Ajuste para CNN: (batch_size, channels, length)\n",
    "X_train_tensor = torch.tensor(X_train).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Criação de DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 2, 32)  # 9 -> pool(4) -> pool(2) ≈ 2 posições\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Instancia e envia para GPU\n",
    "model = CNN1D().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "best_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Época {epoch+1}/{num_epochs} - Loss treino: {avg_loss:.4f} - Loss validação: {val_loss:.4f}\")\n",
    "\n",
    "    # Salva o melhor modelo\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"melhor_cnn_modelo.pth\")\n",
    "        print(\"✅ Modelo salvo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação nos dados de teste\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "        y_preds.extend(preds)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "print(\"\\n📊 Relatório de Classificação:\")\n",
    "print(classification_report(y_true, y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Abrir aquivo e fazer o plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(\"data\", \"01-12\")\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    plt.plot(df[\"Timestamp\"], df[\" Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Pacotes Enviados\")\n",
    "    plt.title(f\"Tráfego de Pacotes - {file_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
