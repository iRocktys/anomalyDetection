{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contém múltiplos cenários de ataques, registrados em arquivos CSV, com detalhes sobre tráfego malicioso e legítimo. Abaixo, são listados os períodos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "DrDos_NTP.csv, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 12:45 - 13:09\n",
    "UDPLag.csv, 13:11 - 13:15\n",
    "Syn.csv, 13:29 - 13:34\n",
    "TFTP.csv, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```csv\n",
    "PortMap.csv, 09:43 - 09:51\n",
    "DrDos_NetBIOS.csv, 10:00 - 10:09\n",
    "DrDos_LDAP.csv, 10:21 - 10:30\n",
    "DrDos_MSSQL.csv, 10:33 - 10:42\n",
    "DrDos_UDP.csv, 10:53 - 11:03\n",
    "DrDos_UDP-Lag.csv, 11:14 - 11:24\n",
    "Syn.csv, 11:28 - 17:35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatena os dias da coleta em um único arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erro ao processar DrDos_DNS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "✔ DrDos_LDAP.csv processado com 2181542 linhas.\n",
      "✔ DrDos_MSSQL.csv processado com 4524498 linhas.\n",
      "❌ Erro ao processar DrDos_NetBIOS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "✔ DrDos_NTP.csv processado com 1217007 linhas.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# data_path = \"data/01-12\"\n",
    "\n",
    "# files = [\n",
    "#     \"DrDos_DNS.csv\", \"DrDos_LDAP.csv\", \"DrDos_MSSQL.csv\",\n",
    "#     \"DrDos_NetBIOS.csv\", \"DrDos_NTP.csv\", \"DrDos_SNMP.csv\",\n",
    "#     \"DrDos_SSDP.csv\", \"DrDos_UDP.csv\", \"Syn.csv\",\n",
    "#     \"TFTP.csv\", \"UDPLag.csv\"\n",
    "# ]\n",
    "\n",
    "# selected_columns = [\n",
    "#     \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "#     \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "#     \" Total Fwd Packets\"\n",
    "# ]\n",
    "\n",
    "# all_data = []\n",
    "\n",
    "# for file_name in files:\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "#     if os.path.exists(file_path):\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "#             df = df.rename(columns=lambda x: x.strip())\n",
    "#             df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "#             all_data.append(df)\n",
    "#             print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erro ao processar {file_name}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "# if all_data:\n",
    "#     final_df = pd.concat(all_data, ignore_index=True)\n",
    "#     final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "#     output_file = os.path.join(data_path, \"combined_attacks_01_12.csv\")\n",
    "#     final_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Arquivo combinado salvo corretamente em ordem cronológica: {output_file}\")\n",
    "# else:\n",
    "#     print(\"Nenhum dado válido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/01-12/combined_attacks_01_12.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# data_path = \"data/03-11\"\n",
    "\n",
    "# files = [\n",
    "#     \"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\",\n",
    "#     \"Syn.csv\", \"UDP.csv\", \"UDPLag.csv\"\n",
    "# ]\n",
    "\n",
    "# selected_columns = [\n",
    "#     \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "#     \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "#     \" Total Fwd Packets\"\n",
    "# ]\n",
    "\n",
    "# all_data = []\n",
    "\n",
    "# for file_name in files:\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "#     if os.path.exists(file_path):\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "#             df = df.rename(columns=lambda x: x.strip())\n",
    "#             df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "#             all_data.append(df)\n",
    "#             print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erro ao processar {file_name}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Arquivo não encontrado: {file_name}\")\n",
    "\n",
    "# if all_data:\n",
    "#     final_df = pd.concat(all_data, ignore_index=True)\n",
    "#     final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "#     output_file = os.path.join(data_path, \"combined_attacks_03_11.csv\")\n",
    "#     final_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Arquivo combinado salvo corretamente em ordem cronológica: {output_file}\")\n",
    "# else:\n",
    "#     print(\"Nenhum dado válido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# combined_df = pd.read_csv(\"data/03-11/combined_attacks_03_11.csv\")\n",
    "# combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "# combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "# combined_df = combined_df.sort_index()\n",
    "# print(len(combined_df))\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "# plt.xlabel(\"Timestamp\")\n",
    "# plt.ylabel(\"Total Fwd Packets\")\n",
    "# plt.title(\"Combined Attacks Time Series\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização e criar sequência "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza e adiciona labels binárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "DATA_PATH = \"data/01-12/combined_attacks_01_12.csv\"\n",
    "FINAL_FILE = \"data/01-12/01_12_final.csv\"\n",
    "# DATA_PATH = \"data/03-11/combined_attacks_03_11.csv\"\n",
    "# FINAL_FILE = \"data/03-11/03_11_final.parquet\"\n",
    "CHUNK_SIZE = 10000  # Definir o tamanho do lote\n",
    "SEQ_LENGTH = 10\n",
    "FEATURES = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]\n",
    "\n",
    "# Definir os períodos de ataque\n",
    "ATTACK_PERIODS = {\n",
    "    \"DrDoS_NTP\": (\"10:35\", \"10:45\"),\n",
    "    \"DrDoS_DNS\": (\"10:52\", \"11:05\"),\n",
    "    \"DrDoS_LDAP\": (\"11:22\", \"11:32\"),\n",
    "    \"DrDoS_MSSQL\": (\"11:36\", \"11:45\"),\n",
    "    \"DrDoS_NetBIOS\": (\"11:50\", \"12:00\"),\n",
    "    \"DrDoS_SNMP\": (\"12:12\", \"12:23\"),\n",
    "    \"DrDoS_SSDP\": (\"12:27\", \"12:37\"),\n",
    "    \"DrDoS_UDP\": (\"12:45\", \"13:09\"),\n",
    "    \"UDPLag\": (\"13:11\", \"13:15\"),\n",
    "    \"SYN\": (\"13:29\", \"13:34\"),\n",
    "    \"TFTP\": (\"13:35\", \"17:15\"),\n",
    "}\n",
    "\n",
    "# ATTACK_PERIODS_EXTRA = {\n",
    "#     \"PortMap\": (\"09:43\", \"09:51\"),\n",
    "#     \"DrDoS_NetBIOS\": (\"10:00\", \"10:09\"),\n",
    "#     \"DrDoS_LDAP\": (\"10:21\", \"10:30\"),\n",
    "#     \"DrDoS_MSSQL\": (\"10:33\", \"10:42\"),\n",
    "#     \"DrDoS_UDP\": (\"10:53\", \"11:03\"),\n",
    "#     \"DrDoS_UDP-Lag\": (\"11:14\", \"11:24\"),\n",
    "#     \"SYN\": (\"11:28\", \"17:35\"),\n",
    "# }\n",
    "\n",
    "\n",
    "# Processamento por chunks\n",
    "def process_chunk(chunk, scaler):\n",
    "    chunk = chunk[[\"Timestamp\"] + FEATURES].copy()  # Manter apenas colunas necessárias\n",
    "    chunk[\"Label\"] = 0  # Iniciar rótulo como 0 (normal)\n",
    "\n",
    "    for _, (start, end) in ATTACK_PERIODS.items():\n",
    "        mask = (chunk[\"Timestamp\"].astype(str).str[11:16] >= start) & (chunk[\"Timestamp\"].astype(str).str[11:16] <= end)\n",
    "        chunk.loc[mask, \"Label\"] = 1\n",
    "\n",
    "    chunk.fillna(0, inplace=True)\n",
    "    chunk[FEATURES] = scaler.transform(chunk[FEATURES])\n",
    "    chunk[FEATURES] = chunk[FEATURES].astype(np.float16)  # Reduzindo a precisão\n",
    "    chunk[\"Label\"] = chunk[\"Label\"].astype(np.uint8)  # Compactando o rótulo\n",
    "    return chunk\n",
    "\n",
    "# Criar arquivo processado acumulando os chunks\n",
    "first_chunk = True\n",
    "scaler = MinMaxScaler()\n",
    "processed_data = []\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE):\n",
    "    if first_chunk:\n",
    "        scaler.fit(chunk[FEATURES])  # Ajustar o scaler na primeira iteração\n",
    "        first_chunk = False\n",
    "\n",
    "    processed_data.append(process_chunk(chunk, scaler))\n",
    "\n",
    "# Concatenar todos os chunks e salvar em CSV\n",
    "df_final = pd.concat(processed_data, ignore_index=True)\n",
    "df_final.to_csv(FINAL_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Flow ID       Source IP  Source Port  \\\n",
      "0     172.16.0.5-192.168.50.1-60675-80-6      172.16.0.5        60675   \n",
      "1     172.16.0.5-192.168.50.1-60676-80-6      172.16.0.5        60676   \n",
      "2  192.168.50.7-65.55.163.78-50458-443-6    65.55.163.78          443   \n",
      "3  192.168.50.7-65.55.163.78-50465-443-6    65.55.163.78          443   \n",
      "4         192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0   192.168.50.1                80         6  2018-12-01 09:17:11.183810   \n",
      "1   192.168.50.1                80         6  2018-12-01 09:17:11.205636   \n",
      "2   192.168.50.7             50458         6  2018-12-01 09:17:12.634569   \n",
      "3   192.168.50.7             50465         6  2018-12-01 09:17:13.458370   \n",
      "4      224.0.0.5                 0         0  2018-12-01 09:17:13.470913   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0        5220876                 12  \n",
      "1       12644252                  5  \n",
      "2              3                  2  \n",
      "3              3                  2  \n",
      "4      114329232                 52  \n",
      "                                 Flow ID       Source IP  Source Port  \\\n",
      "0     172.16.0.5-192.168.50.1-60675-80-6      172.16.0.5        60675   \n",
      "1     172.16.0.5-192.168.50.1-60676-80-6      172.16.0.5        60676   \n",
      "2  192.168.50.7-65.55.163.78-50458-443-6    65.55.163.78          443   \n",
      "3  192.168.50.7-65.55.163.78-50465-443-6    65.55.163.78          443   \n",
      "4         192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0   192.168.50.1                80         6  2018-12-01 09:17:11.183810   \n",
      "1   192.168.50.1                80         6  2018-12-01 09:17:11.205636   \n",
      "2   192.168.50.7             50458         6  2018-12-01 09:17:12.634569   \n",
      "3   192.168.50.7             50465         6  2018-12-01 09:17:13.458370   \n",
      "4      224.0.0.5                 0         0  2018-12-01 09:17:13.470913   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0        5220876                 12  \n",
      "1       12644252                  5  \n",
      "2              3                  2  \n",
      "3              3                  2  \n",
      "4      114329232                 52  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"data/01-12/combined_attacks_01_12.csv\"\n",
    "DATA_FILE = \"data/01-12/01_12_final.csv\"\n",
    "\n",
    "df_sample_data = pd.read_csv(DATA_PATH, nrows=5)\n",
    "print(pd.len(df_sample_data))\n",
    "print(df_sample_data.columns)\n",
    "print(df_sample_data.dtypes)\n",
    "print(df_sample_data)\n",
    "\n",
    "df_sample_file = pd.read_csv(DATA_FILE, nrows=5)\n",
    "print(pd.len(df_sample_file))\n",
    "print(df_sample_file.columns)\n",
    "print(df_sample_file.dtypes)\n",
    "print(df_sample_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# 📌 1️⃣ Carregar Dataset Já Processado (sequenciado e normalizado)\n",
    "data_path = \"data/01-12/03_11_processado.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 📌 2️⃣ Definir Features e Rótulos\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]  \n",
    "label = \"Attack Type\"  \n",
    "\n",
    "X = df[features].values  \n",
    "y = df[label].values  \n",
    "\n",
    "# 📌 3️⃣ Separar Treino e Teste (80% treino, 20% teste)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# 📌 4️⃣ Criar Modelo Híbrido CNN + LSTM\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 📌 5️⃣ Treinar Modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 📌 6️⃣ Extração de Características da CNN+LSTM para o SVM\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "# 📌 7️⃣ Treinar o SVM\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# 📌 8️⃣ Fazer Predições\n",
    "y_pred = svm.predict(X_test_features)\n",
    "\n",
    "# 📌 9️⃣ Avaliação do Modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"🔹 Precisão do Modelo Híbrido: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n🔹 Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 🔟 Matriz de Confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Ataque\"], yticklabels=[\"Normal\", \"Ataque\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusão - CNN+LSTM+SVM\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 1️⃣1️⃣ Gráfico: Comparação Predições vs Reais\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test[:100], label=\"Real\", linestyle=\"dashed\")\n",
    "plt.plot(y_pred[:100], label=\"Previsto\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"🔹 Predições vs Valores Reais\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 1️⃣2️⃣ Salvar Modelos\n",
    "joblib.dump(svm, \"svm_model.pkl\")\n",
    "model.save(\"cnn_lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Abrir aquivo e fazer o plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(\"data\", \"01-12\")\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    plt.plot(df[\"Timestamp\"], df[\" Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Pacotes Enviados\")\n",
    "    plt.title(f\"Tráfego de Pacotes - {file_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
