{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset cont√©m m√∫ltiplos cen√°rios de ataques, registrados em arquivos CSV, com detalhes sobre tr√°fego malicioso e leg√≠timo. Abaixo, s√£o listados os per√≠odos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "DrDos_NTP.csv, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 12:45 - 13:09\n",
    "UDPLag.csv, 13:11 - 13:15\n",
    "Syn.csv, 13:29 - 13:34\n",
    "TFTP.csv, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```csv\n",
    "PortMap.csv, 09:43 - 09:51\n",
    "DrDos_NetBIOS.csv, 10:00 - 10:09\n",
    "DrDos_LDAP.csv, 10:21 - 10:30\n",
    "DrDos_MSSQL.csv, 10:33 - 10:42\n",
    "DrDos_UDP.csv, 10:53 - 11:03\n",
    "DrDos_UDP-Lag.csv, 11:14 - 11:24\n",
    "Syn.csv, 11:28 - 17:35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatena os dias da coleta em um √∫nico arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao processar DrDos_DNS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "‚úî DrDos_LDAP.csv processado com 2181542 linhas.\n",
      "‚úî DrDos_MSSQL.csv processado com 4524498 linhas.\n",
      "‚ùå Erro ao processar DrDos_NetBIOS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "‚úî DrDos_NTP.csv processado com 1217007 linhas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = \"data/01-12\"\n",
    "\n",
    "files = [\n",
    "    \"DrDos_DNS.csv\", \"DrDos_LDAP.csv\", \"DrDos_MSSQL.csv\",\n",
    "    \"DrDos_NetBIOS.csv\", \"DrDos_NTP.csv\", \"DrDos_SNMP.csv\",\n",
    "    \"DrDos_SSDP.csv\", \"DrDos_UDP.csv\", \"Syn.csv\",\n",
    "    \"TFTP.csv\", \"UDPLag.csv\"\n",
    "]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "            df = df.rename(columns=lambda x: x.strip())\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "            all_data.append(df)\n",
    "            print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Arquivo n√£o encontrado: {file_name}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "    output_file = os.path.join(data_path, \"combined_attacks_01_12.csv\")\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Arquivo combinado salvo corretamente em ordem cronol√≥gica: {output_file}\")\n",
    "else:\n",
    "    print(\"Nenhum dado v√°lido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/01-12/combined_attacks_01_12.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = \"data/03-11\"\n",
    "\n",
    "files = [\n",
    "    \"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\",\n",
    "    \"Syn.csv\", \"UDP.csv\", \"UDPLag.csv\"\n",
    "]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "            df = df.rename(columns=lambda x: x.strip())\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "            all_data.append(df)\n",
    "            print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Arquivo n√£o encontrado: {file_name}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "    output_file = os.path.join(data_path, \"combined_attacks_03_11.csv\")\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Arquivo combinado salvo corretamente em ordem cronol√≥gica: {output_file}\")\n",
    "else:\n",
    "    print(\"Nenhum dado v√°lido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/03-11/combined_attacks_03_11.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliza√ß√£o e criar sequ√™ncia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados preparados para LSTM salvos em: data/03-11/03_11_processado.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_path = \"data/03-11/combined_attacks_03_11.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "def create_sequences(data, seq_length=10):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        labels.append(data[i+seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "data_values = df[features].values\n",
    "seq_length = 10\n",
    "X, y = create_sequences(data_values, seq_length)\n",
    "\n",
    "processed_file = \"data/03-11/03_11_processado.csv\"\n",
    "pd.DataFrame(X.reshape(X.shape[0], -1)).to_csv(processed_file, index=False)\n",
    "\n",
    "print(f\"Dados preparados para LSTM salvos em: {processed_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas:\n",
      "['Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packets']\n",
      "                                  Flow ID       Source IP  Source Port  \\\n",
      "0          192.168.50.254-224.0.0.5-0-0-0  192.168.50.254            0   \n",
      "1          192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "2  172.217.10.98-192.168.50.6-443-54799-6    192.168.50.6        54799   \n",
      "3    172.217.7.2-192.168.50.6-443-54800-6    192.168.50.6        54800   \n",
      "4  172.217.10.98-192.168.50.6-443-54801-6    192.168.50.6        54801   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0      224.0.0.5                 0  0.000000  2018-11-03 09:18:16.964447   \n",
      "1      224.0.0.5                 0  0.000000  2018-11-03 09:18:18.506537   \n",
      "2  172.217.10.98               443  0.352941  2018-11-03 09:18:18.610576   \n",
      "3    172.217.7.2               443  0.352941  2018-11-03 09:18:18.610579   \n",
      "4  172.217.10.98               443  0.352941  2018-11-03 09:18:18.610581   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0       0.953828           0.000505  \n",
      "1       0.952916           0.000631  \n",
      "2       0.303635           0.000057  \n",
      "3       0.303629           0.000057  \n",
      "4       0.303628           0.000057  \n"
     ]
    }
   ],
   "source": [
    "print(\"Nomes das colunas:\")\n",
    "print(df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# üìå 1Ô∏è‚É£ Carregar Dataset J√° Processado (sequenciado e normalizado)\n",
    "data_path = \"data/01-12/03_11_processado.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# üìå 2Ô∏è‚É£ Definir Features e R√≥tulos\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]  \n",
    "label = \"Attack Type\"  \n",
    "\n",
    "X = df[features].values  \n",
    "y = df[label].values  \n",
    "\n",
    "# üìå 3Ô∏è‚É£ Separar Treino e Teste (80% treino, 20% teste)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# üìå 4Ô∏è‚É£ Criar Modelo H√≠brido CNN + LSTM\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# üìå 5Ô∏è‚É£ Treinar Modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# üìå 6Ô∏è‚É£ Extra√ß√£o de Caracter√≠sticas da CNN+LSTM para o SVM\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "# üìå 7Ô∏è‚É£ Treinar o SVM\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# üìå 8Ô∏è‚É£ Fazer Predi√ß√µes\n",
    "y_pred = svm.predict(X_test_features)\n",
    "\n",
    "# üìå 9Ô∏è‚É£ Avalia√ß√£o do Modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üîπ Precis√£o do Modelo H√≠brido: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nüîπ Relat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# üìå üîü Matriz de Confus√£o\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Ataque\"], yticklabels=[\"Normal\", \"Ataque\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confus√£o - CNN+LSTM+SVM\")\n",
    "plt.show()\n",
    "\n",
    "# üìå 1Ô∏è‚É£1Ô∏è‚É£ Gr√°fico: Compara√ß√£o Predi√ß√µes vs Reais\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test[:100], label=\"Real\", linestyle=\"dashed\")\n",
    "plt.plot(y_pred[:100], label=\"Previsto\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"üîπ Predi√ß√µes vs Valores Reais\")\n",
    "plt.show()\n",
    "\n",
    "# üìå 1Ô∏è‚É£2Ô∏è‚É£ Salvar Modelos\n",
    "joblib.dump(svm, \"svm_model.pkl\")\n",
    "model.save(\"cnn_lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Abrir aquivo e fazer o plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(\"data\", \"01-12\")\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    plt.plot(df[\"Timestamp\"], df[\" Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Pacotes Enviados\")\n",
    "    plt.title(f\"Tr√°fego de Pacotes - {file_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
