{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntroduÃ§Ã£o aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset contÃ©m mÃºltiplos cenÃ¡rios de ataques, registrados em arquivos CSV, com detalhes sobre trÃ¡fego malicioso e legÃ­timo. Abaixo, sÃ£o listados os perÃ­odos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "DrDos_NTP.csv, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 12:45 - 13:09\n",
    "UDPLag.csv, 13:11 - 13:15\n",
    "Syn.csv, 13:29 - 13:34\n",
    "TFTP.csv, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```csv\n",
    "PortMap.csv, 09:43 - 09:51\n",
    "DrDos_NetBIOS.csv, 10:00 - 10:09\n",
    "DrDos_LDAP.csv, 10:21 - 10:30\n",
    "DrDos_MSSQL.csv, 10:33 - 10:42\n",
    "DrDos_UDP.csv, 10:53 - 11:03\n",
    "DrDos_UDP-Lag.csv, 11:14 - 11:24\n",
    "Syn.csv, 11:28 - 17:35\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrÃ©-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatena os dias da coleta em um Ãºnico arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erro ao processar DrDos_DNS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "âœ” DrDos_LDAP.csv processado com 2181542 linhas.\n",
      "âœ” DrDos_MSSQL.csv processado com 4524498 linhas.\n",
      "âŒ Erro ao processar DrDos_NetBIOS.csv: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "âœ” DrDos_NTP.csv processado com 1217007 linhas.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# data_path = \"data/01-12\"\n",
    "\n",
    "# files = [\n",
    "#     \"DrDos_DNS.csv\", \"DrDos_LDAP.csv\", \"DrDos_MSSQL.csv\",\n",
    "#     \"DrDos_NetBIOS.csv\", \"DrDos_NTP.csv\", \"DrDos_SNMP.csv\",\n",
    "#     \"DrDos_SSDP.csv\", \"DrDos_UDP.csv\", \"Syn.csv\",\n",
    "#     \"TFTP.csv\", \"UDPLag.csv\"\n",
    "# ]\n",
    "\n",
    "# selected_columns = [\n",
    "#     \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "#     \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "#     \" Total Fwd Packets\"\n",
    "# ]\n",
    "\n",
    "# all_data = []\n",
    "\n",
    "# for file_name in files:\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "#     if os.path.exists(file_path):\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "#             df = df.rename(columns=lambda x: x.strip())\n",
    "#             df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "#             all_data.append(df)\n",
    "#             print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erro ao processar {file_name}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Arquivo nÃ£o encontrado: {file_name}\")\n",
    "\n",
    "# if all_data:\n",
    "#     final_df = pd.concat(all_data, ignore_index=True)\n",
    "#     final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "#     output_file = os.path.join(data_path, \"combined_attacks_01_12.csv\")\n",
    "#     final_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Arquivo combinado salvo corretamente em ordem cronolÃ³gica: {output_file}\")\n",
    "# else:\n",
    "#     print(\"Nenhum dado vÃ¡lido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"data/01-12/combined_attacks_01_12.csv\")\n",
    "combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "combined_df = combined_df.sort_index()\n",
    "print(len(combined_df))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Total Fwd Packets\")\n",
    "plt.title(\"Combined Attacks Time Series\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# data_path = \"data/03-11\"\n",
    "\n",
    "# files = [\n",
    "#     \"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\",\n",
    "#     \"Syn.csv\", \"UDP.csv\", \"UDPLag.csv\"\n",
    "# ]\n",
    "\n",
    "# selected_columns = [\n",
    "#     \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "#     \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "#     \" Total Fwd Packets\"\n",
    "# ]\n",
    "\n",
    "# all_data = []\n",
    "\n",
    "# for file_name in files:\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "    \n",
    "#     if os.path.exists(file_path):\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "#             df = df.rename(columns=lambda x: x.strip())\n",
    "#             df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "#             all_data.append(df)\n",
    "#             print(f\"{file_name} processado com {len(df)} linhas.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erro ao processar {file_name}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Arquivo nÃ£o encontrado: {file_name}\")\n",
    "\n",
    "# if all_data:\n",
    "#     final_df = pd.concat(all_data, ignore_index=True)\n",
    "#     final_df = final_df.sort_values(by=\"Timestamp\")\n",
    "#     output_file = os.path.join(data_path, \"combined_attacks_03_11.csv\")\n",
    "#     final_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Arquivo combinado salvo corretamente em ordem cronolÃ³gica: {output_file}\")\n",
    "# else:\n",
    "#     print(\"Nenhum dado vÃ¡lido encontrado para gerar o arquivo combinado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# combined_df = pd.read_csv(\"data/03-11/combined_attacks_03_11.csv\")\n",
    "# combined_df[\"Timestamp\"] = pd.to_datetime(combined_df[\"Timestamp\"])\n",
    "# combined_df.set_index(\"Timestamp\", inplace=True)\n",
    "# combined_df = combined_df.sort_index()\n",
    "# print(len(combined_df))\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(combined_df.index, combined_df[\"Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "\n",
    "# plt.xlabel(\"Timestamp\")\n",
    "# plt.ylabel(\"Total Fwd Packets\")\n",
    "# plt.title(\"Combined Attacks Time Series\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NormalizaÃ§Ã£o e criar sequÃªncia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza e adiciona labels binÃ¡rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "DATA_PATH = \"data/01-12/combined_attacks_01_12.csv\"\n",
    "FINAL_FILE = \"data/01-12/01_12_final.csv\"\n",
    "# DATA_PATH = \"data/03-11/combined_attacks_03_11.csv\"\n",
    "# FINAL_FILE = \"data/03-11/03_11_final.parquet\"\n",
    "CHUNK_SIZE = 10000  # Definir o tamanho do lote\n",
    "SEQ_LENGTH = 10\n",
    "FEATURES = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]\n",
    "\n",
    "# Definir os perÃ­odos de ataque\n",
    "ATTACK_PERIODS = {\n",
    "    \"DrDoS_NTP\": (\"10:35\", \"10:45\"),\n",
    "    \"DrDoS_DNS\": (\"10:52\", \"11:05\"),\n",
    "    \"DrDoS_LDAP\": (\"11:22\", \"11:32\"),\n",
    "    \"DrDoS_MSSQL\": (\"11:36\", \"11:45\"),\n",
    "    \"DrDoS_NetBIOS\": (\"11:50\", \"12:00\"),\n",
    "    \"DrDoS_SNMP\": (\"12:12\", \"12:23\"),\n",
    "    \"DrDoS_SSDP\": (\"12:27\", \"12:37\"),\n",
    "    \"DrDoS_UDP\": (\"12:45\", \"13:09\"),\n",
    "    \"UDPLag\": (\"13:11\", \"13:15\"),\n",
    "    \"SYN\": (\"13:29\", \"13:34\"),\n",
    "    \"TFTP\": (\"13:35\", \"17:15\"),\n",
    "}\n",
    "\n",
    "# ATTACK_PERIODS_EXTRA = {\n",
    "#     \"PortMap\": (\"09:43\", \"09:51\"),\n",
    "#     \"DrDoS_NetBIOS\": (\"10:00\", \"10:09\"),\n",
    "#     \"DrDoS_LDAP\": (\"10:21\", \"10:30\"),\n",
    "#     \"DrDoS_MSSQL\": (\"10:33\", \"10:42\"),\n",
    "#     \"DrDoS_UDP\": (\"10:53\", \"11:03\"),\n",
    "#     \"DrDoS_UDP-Lag\": (\"11:14\", \"11:24\"),\n",
    "#     \"SYN\": (\"11:28\", \"17:35\"),\n",
    "# }\n",
    "\n",
    "\n",
    "# Processamento por chunks\n",
    "def process_chunk(chunk, scaler):\n",
    "    chunk = chunk[[\"Timestamp\"] + FEATURES].copy()  # Manter apenas colunas necessÃ¡rias\n",
    "    chunk[\"Label\"] = 0  # Iniciar rÃ³tulo como 0 (normal)\n",
    "\n",
    "    for _, (start, end) in ATTACK_PERIODS.items():\n",
    "        mask = (chunk[\"Timestamp\"].astype(str).str[11:16] >= start) & (chunk[\"Timestamp\"].astype(str).str[11:16] <= end)\n",
    "        chunk.loc[mask, \"Label\"] = 1\n",
    "\n",
    "    chunk.fillna(0, inplace=True)\n",
    "    chunk[FEATURES] = scaler.transform(chunk[FEATURES])\n",
    "    chunk[FEATURES] = chunk[FEATURES].astype(np.float16)  # Reduzindo a precisÃ£o\n",
    "    chunk[\"Label\"] = chunk[\"Label\"].astype(np.uint8)  # Compactando o rÃ³tulo\n",
    "    return chunk\n",
    "\n",
    "# Criar arquivo processado acumulando os chunks\n",
    "first_chunk = True\n",
    "scaler = MinMaxScaler()\n",
    "processed_data = []\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE):\n",
    "    if first_chunk:\n",
    "        scaler.fit(chunk[FEATURES])  # Ajustar o scaler na primeira iteraÃ§Ã£o\n",
    "        first_chunk = False\n",
    "\n",
    "    processed_data.append(process_chunk(chunk, scaler))\n",
    "\n",
    "# Concatenar todos os chunks e salvar em CSV\n",
    "df_final = pd.concat(processed_data, ignore_index=True)\n",
    "df_final.to_csv(FINAL_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Flow ID       Source IP  Source Port  \\\n",
      "0     172.16.0.5-192.168.50.1-60675-80-6      172.16.0.5        60675   \n",
      "1     172.16.0.5-192.168.50.1-60676-80-6      172.16.0.5        60676   \n",
      "2  192.168.50.7-65.55.163.78-50458-443-6    65.55.163.78          443   \n",
      "3  192.168.50.7-65.55.163.78-50465-443-6    65.55.163.78          443   \n",
      "4         192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0   192.168.50.1                80         6  2018-12-01 09:17:11.183810   \n",
      "1   192.168.50.1                80         6  2018-12-01 09:17:11.205636   \n",
      "2   192.168.50.7             50458         6  2018-12-01 09:17:12.634569   \n",
      "3   192.168.50.7             50465         6  2018-12-01 09:17:13.458370   \n",
      "4      224.0.0.5                 0         0  2018-12-01 09:17:13.470913   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0        5220876                 12  \n",
      "1       12644252                  5  \n",
      "2              3                  2  \n",
      "3              3                  2  \n",
      "4      114329232                 52  \n",
      "                                 Flow ID       Source IP  Source Port  \\\n",
      "0     172.16.0.5-192.168.50.1-60675-80-6      172.16.0.5        60675   \n",
      "1     172.16.0.5-192.168.50.1-60676-80-6      172.16.0.5        60676   \n",
      "2  192.168.50.7-65.55.163.78-50458-443-6    65.55.163.78          443   \n",
      "3  192.168.50.7-65.55.163.78-50465-443-6    65.55.163.78          443   \n",
      "4         192.168.50.253-224.0.0.5-0-0-0  192.168.50.253            0   \n",
      "\n",
      "  Destination IP  Destination Port  Protocol                   Timestamp  \\\n",
      "0   192.168.50.1                80         6  2018-12-01 09:17:11.183810   \n",
      "1   192.168.50.1                80         6  2018-12-01 09:17:11.205636   \n",
      "2   192.168.50.7             50458         6  2018-12-01 09:17:12.634569   \n",
      "3   192.168.50.7             50465         6  2018-12-01 09:17:13.458370   \n",
      "4      224.0.0.5                 0         0  2018-12-01 09:17:13.470913   \n",
      "\n",
      "   Flow Duration  Total Fwd Packets  \n",
      "0        5220876                 12  \n",
      "1       12644252                  5  \n",
      "2              3                  2  \n",
      "3              3                  2  \n",
      "4      114329232                 52  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"data/01-12/combined_attacks_01_12.csv\"\n",
    "DATA_FILE = \"data/01-12/01_12_final.csv\"\n",
    "\n",
    "df_sample_data = pd.read_csv(DATA_PATH, nrows=5)\n",
    "print(pd.len(df_sample_data))\n",
    "print(df_sample_data.columns)\n",
    "print(df_sample_data.dtypes)\n",
    "print(df_sample_data)\n",
    "\n",
    "df_sample_file = pd.read_csv(DATA_FILE, nrows=5)\n",
    "print(pd.len(df_sample_file))\n",
    "print(df_sample_file.columns)\n",
    "print(df_sample_file.dtypes)\n",
    "print(df_sample_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# ğŸ“Œ 1ï¸âƒ£ Carregar Dataset JÃ¡ Processado (sequenciado e normalizado)\n",
    "data_path = \"data/01-12/03_11_processado.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ğŸ“Œ 2ï¸âƒ£ Definir Features e RÃ³tulos\n",
    "features = [\"Flow Duration\", \"Total Fwd Packets\", \"Protocol\"]  \n",
    "label = \"Attack Type\"  \n",
    "\n",
    "X = df[features].values  \n",
    "y = df[label].values  \n",
    "\n",
    "# ğŸ“Œ 3ï¸âƒ£ Separar Treino e Teste (80% treino, 20% teste)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# ğŸ“Œ 4ï¸âƒ£ Criar Modelo HÃ­brido CNN + LSTM\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ğŸ“Œ 5ï¸âƒ£ Treinar Modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# ğŸ“Œ 6ï¸âƒ£ ExtraÃ§Ã£o de CaracterÃ­sticas da CNN+LSTM para o SVM\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "# ğŸ“Œ 7ï¸âƒ£ Treinar o SVM\n",
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# ğŸ“Œ 8ï¸âƒ£ Fazer PrediÃ§Ãµes\n",
    "y_pred = svm.predict(X_test_features)\n",
    "\n",
    "# ğŸ“Œ 9ï¸âƒ£ AvaliaÃ§Ã£o do Modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"ğŸ”¹ PrecisÃ£o do Modelo HÃ­brido: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”¹ RelatÃ³rio de ClassificaÃ§Ã£o:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ğŸ“Œ ğŸ”Ÿ Matriz de ConfusÃ£o\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Ataque\"], yticklabels=[\"Normal\", \"Ataque\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de ConfusÃ£o - CNN+LSTM+SVM\")\n",
    "plt.show()\n",
    "\n",
    "# ğŸ“Œ 1ï¸âƒ£1ï¸âƒ£ GrÃ¡fico: ComparaÃ§Ã£o PrediÃ§Ãµes vs Reais\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test[:100], label=\"Real\", linestyle=\"dashed\")\n",
    "plt.plot(y_pred[:100], label=\"Previsto\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"ğŸ”¹ PrediÃ§Ãµes vs Valores Reais\")\n",
    "plt.show()\n",
    "\n",
    "# ğŸ“Œ 1ï¸âƒ£2ï¸âƒ£ Salvar Modelos\n",
    "joblib.dump(svm, \"svm_model.pkl\")\n",
    "model.save(\"cnn_lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Abrir aquivo e fazer o plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = os.path.join(\"data\", \"01-12\")\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "\n",
    "selected_columns = [\n",
    "    \"Flow ID\", \" Source IP\", \" Source Port\", \" Destination IP\", \n",
    "    \" Destination Port\", \" Protocol\", \" Timestamp\", \" Flow Duration\",\n",
    "    \" Total Fwd Packets\"\n",
    "]\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, usecols=selected_columns)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\" Timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "    \n",
    "    plt.plot(df[\"Timestamp\"], df[\" Total Fwd Packets\"], label=\"Total Fwd Packets\", color=\"blue\")\n",
    "    plt.xlabel(\"Tempo\")\n",
    "    plt.ylabel(\"Pacotes Enviados\")\n",
    "    plt.title(f\"TrÃ¡fego de Pacotes - {file_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
