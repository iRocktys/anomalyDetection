{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdu√ß√£o aos Ataques DDoS no Dataset CICDDoS2019\n",
    "\n",
    "O dataset cont√©m m√∫ltiplos cen√°rios de ataques, registrados em arquivos CSV, com detalhes sobre tr√°fego malicioso e leg√≠timo. Abaixo, s√£o listados os per√≠odos de tempo (em horas e minutos) em que os ataques ocorreram, organizados por dia e tipo de ataque.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (01/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "DrDos_NTP.csv, 1217007, 88, 10:35 - 10:45\n",
    "DrDos_DNS.csv, 5074413, 88, 10:52 - 11:05\n",
    "DrDos_LDAP.csv, 2181542, 88, 11:22 - 11:32\n",
    "DrDos_MSSQL.csv, 4524498, 88, 11:36 - 11:45\n",
    "DrDos_NetBIOS.csv, 4094986, 88, 11:50 - 12:00\n",
    "DrDos_SNMP.csv, 5161377, 88, 12:12 - 12:23\n",
    "DrDos_SSDP.csv, 2611374, 88, 12:27 - 12:37\n",
    "DrDos_UDP.csv, 3136802, 88, 12:45 - 13:09\n",
    "UDPLag.csv, 370605, 88, 13:11 - 13:15\n",
    "Syn.csv, 1582681, 88, 13:29 - 13:34\n",
    "TFTP.csv, 20107827, 88, 13:35 - 17:15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataques coloetados no dia (03/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```csv\n",
    "data_path, length, columns, hour\n",
    "NetBIOS.csv, 3455899, 88, 10:00 - 10:09\n",
    "LDAP.csv, 2113234, 88, 10:21 - 10:30\n",
    "MSSQL.csv, 5775786, 88, 10:33 - 10:42\n",
    "UDP.csv, 3782206, 88,  10:53 - 11:03\n",
    "UDPLag.csv, 725165, 88, 11:14 - 11:24\n",
    "Syn.csv, 4320541, 88, 11:28 - 17:35\n",
    "```\n",
    "\n",
    "Portmap.csv n√£o tem per√≠odo de ataque "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-Processamento UEL - Gerando dados para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequ√™ncias aleat√≥rias!\n"
     ]
    }
   ],
   "source": [
    "# selected_columns = [\n",
    "#     ' Source IP',\n",
    "#     ' Source Port',\n",
    "#     ' Destination IP',\n",
    "#     ' Destination Port',\n",
    "#     ' Timestamp',\n",
    "#     ' Flow Duration',\n",
    "#     ' Total Fwd Packets',\n",
    "#     ' Total Backward Packets',\n",
    "#     'Total Length of Fwd Packets',\n",
    "#     ' Total Length of Bwd Packets',\n",
    "#     'Flow Bytes/s',\n",
    "#     ' Flow Packets/s',\n",
    "#     'Fwd Packets/s',\n",
    "#     ' Bwd Packets/s',\n",
    "# ]\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import cycle\n",
    "import random\n",
    "\n",
    "# 1. Carregar os arquivos\n",
    "teste_ataque = pd.read_csv('data/cic_puro/teste_ataque_ordenado.csv', sep=';')\n",
    "teste_normal = pd.read_csv('data/cic_puro/teste_sem_ataque_ordenado.csv', sep=';')\n",
    "treino_ataque = pd.read_csv('data/cic_puro/treino_ataque_ordenado.csv', sep=';')\n",
    "treino_normal = pd.read_csv('data/cic_puro/treino_sem_ataque_ordenado.csv', sep=';')\n",
    "\n",
    "\n",
    "# 2. Concatenar para treino e teste\n",
    "teste_full = pd.concat([teste_normal, teste_ataque], ignore_index=True)\n",
    "treino_full = pd.concat([treino_normal, treino_ataque], ignore_index=True)\n",
    "\n",
    "# 3. Separar normais e ataques\n",
    "def prepare_data(df, max_per_attack=1000, max_normal=5000):\n",
    "    normal = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)  # embaralhar normais\n",
    "    attacks = df[df['label'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Agora limitar por tipo de ataque\n",
    "    attack_types = {}\n",
    "    for name, group in attacks.groupby('attack_name'):\n",
    "        attack_types[name] = group.sample(n=min(len(group), max_per_attack)).reset_index(drop=True)\n",
    "\n",
    "    # Limitar normais\n",
    "    if max_normal is not None:\n",
    "        normal = normal.sample(n=min(len(normal), max_normal)).reset_index(drop=True)\n",
    "\n",
    "    return normal, attack_types\n",
    "\n",
    "train_normal, train_attacks = prepare_data(treino_full, max_per_attack=1000, max_normal=10000)\n",
    "test_normal, test_attacks = prepare_data(teste_full, max_per_attack=500, max_normal=5000)\n",
    "\n",
    "# 4. Fun√ß√£o para criar sequ√™ncias aleat√≥rias\n",
    "def create_random_sequences(normal_df, attack_dict, min_seq=30, max_seq=150):\n",
    "    final_rows = []\n",
    "    \n",
    "    normal_iter = normal_df.iterrows()\n",
    "    attack_iters = {k: v.iterrows() for k, v in attack_dict.items()}\n",
    "    attack_cycle = cycle(list(attack_iters.keys()))\n",
    "    \n",
    "    normal_remaining = True\n",
    "    attack_remaining = True\n",
    "\n",
    "    while normal_remaining or attack_remaining:\n",
    "        choice = random.choice(['normal', 'attack'])  # Aleatoriamente decidir normal ou ataque primeiro\n",
    "        \n",
    "        if choice == 'normal' and normal_remaining:\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(normal_iter)\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    normal_remaining = False\n",
    "                    break\n",
    "        \n",
    "        elif choice == 'attack' and attack_remaining:\n",
    "            attack_type = next(attack_cycle)\n",
    "            seq_len = random.randint(min_seq, max_seq)\n",
    "            for _ in range(seq_len):\n",
    "                try:\n",
    "                    idx, row = next(attack_iters[attack_type])\n",
    "                    final_rows.append(row)\n",
    "                except StopIteration:\n",
    "                    # Se esgotar ataques desse tipo, remover do ciclo\n",
    "                    del attack_iters[attack_type]\n",
    "                    if attack_iters:\n",
    "                        attack_cycle = cycle(list(attack_iters.keys()))\n",
    "                    else:\n",
    "                        attack_remaining = False\n",
    "                    break\n",
    "        else:\n",
    "            # Se o tipo escolhido acabou, tenta o outro\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "# 5. Criar datasets\n",
    "train_final = create_random_sequences(train_normal, train_attacks, min_seq=30, max_seq=120)\n",
    "test_final = create_random_sequences(test_normal, test_attacks, min_seq=30, max_seq=120)\n",
    "\n",
    "# 6. Salvar\n",
    "train_final.to_csv('treino_final_estratificado_random.csv', sep=';', index=False)\n",
    "test_final.to_csv('teste_final_estratificado_random.csv', sep=';', index=False)\n",
    "\n",
    "print('Arquivos treino_final_estratificado_random.csv e teste_final_estratificado_random.csv gerados com sequ√™ncias aleat√≥rias!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho: 13 Treino: attack_name\n",
      "normal           8074\n",
      "DrDoS_DNS        1000\n",
      "DrDoS_NTP        1000\n",
      "DrDoS_SNMP       1000\n",
      "DrDoS_UDP        1000\n",
      "TFTP             1000\n",
      "UDP-lag           885\n",
      "DrDoS_SSDP        822\n",
      "DrDoS_NetBIOS     726\n",
      "DrDoS_MSSQL       687\n",
      "DrDoS_LDAP        592\n",
      "Syn               237\n",
      "WebDDoS           125\n",
      "Name: count, dtype: int64\n",
      "Tamanho: 8 Teste: attack_name\n",
      "normal     5000\n",
      "LDAP        500\n",
      "MSSQL       500\n",
      "NetBIOS     500\n",
      "Syn         500\n",
      "UDP         500\n",
      "UDPLag      470\n",
      "Portmap     449\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar a quantidade de cada valor na coluna 'attack_name'\n",
    "attack_counts_train = train_final['attack_name'].value_counts()\n",
    "attack_counts_test = test_final['attack_name'].value_counts()\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Tamanho:', len(train_final), 'Treino:', attack_counts_train)\n",
    "print('Total de linhas no conjunto de treino:', len(train_final))\n",
    "\n",
    "print('Tamanho:', len(test_final), 'Teste:', attack_counts_test)\n",
    "print('Total de linhas no conjunto de teste:', len(test_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes CIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def contar_rotulos_csv(lista_arquivos, coluna_label=' Label'):\n",
    "    for arquivo in lista_arquivos:\n",
    "        try:\n",
    "            print(f\"\\nüìÑ Analisando: {arquivo}\")\n",
    "            df = pd.read_csv(arquivo, usecols=[coluna_label], low_memory=False)\n",
    "            \n",
    "            total_linhas = len(df)\n",
    "            contagem_rotulos = df[coluna_label].value_counts(dropna=False)\n",
    "            soma_rotulos = contagem_rotulos.sum()\n",
    "\n",
    "            print(f\"  ‚û§ Total de linhas: {total_linhas}\")\n",
    "            print(\"  ‚û§ Contagem de r√≥tulos:\")\n",
    "            for rotulo, qtd in contagem_rotulos.items():\n",
    "                print(f\"     - {rotulo}: {qtd}\")\n",
    "\n",
    "            if soma_rotulos == total_linhas:\n",
    "                print(\"  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\")\n",
    "            else:\n",
    "                print(\"  ‚ùå A soma de todos os r√≥tulos N√ÉO √© igual ao total de linhas.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao ler {arquivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/LDAP.csv\n",
      "  ‚û§ Total de linhas: 2113234\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - LDAP: 1905191\n",
      "     - NetBIOS: 202919\n",
      "     - BENIGN: 5124\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/Mssql.csv\n",
      "  ‚û§ Total de linhas: 5775786\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - MSSQL: 5763061\n",
      "     - LDAP: 9931\n",
      "     - BENIGN: 2794\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/NetBIOS.csv\n",
      "  ‚û§ Total de linhas: 3455899\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - NetBIOS: 3454578\n",
      "     - BENIGN: 1321\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/Syn.csv\n",
      "  ‚û§ Total de linhas: 4320541\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - Syn: 4284751\n",
      "     - BENIGN: 35790\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/UDP.csv\n",
      "  ‚û§ Total de linhas: 3782206\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - UDP: 3754680\n",
      "     - MSSQL: 24392\n",
      "     - BENIGN: 3134\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n",
      "\n",
      "üìÑ Analisando: data/Full_dataset/03-11/UDPLag.csv\n",
      "  ‚û§ Total de linhas: 725165\n",
      "  ‚û§ Contagem de r√≥tulos:\n",
      "     - Syn: 606749\n",
      "     - UDP: 112475\n",
      "     - BENIGN: 4068\n",
      "     - UDPLag: 1873\n",
      "  ‚úÖ A soma de todos os r√≥tulos √© igual ao total de linhas.\n"
     ]
    }
   ],
   "source": [
    "csv_files1 = [\n",
    "    'data/03-11/attacks_labeled/LDAP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Mssql_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/NetBIOS_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Syn_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDPLag_labeled.csv'\n",
    "]\n",
    "\n",
    "csv_files2 = [\n",
    "    'data/01-12/attacks_labeled/DrDos_DNS_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_LDAP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_MSSQL_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NetBIOS_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_NTP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SNMP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_SSDP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/DrDos_UDP_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/Syn_labeled.csv',\n",
    "    'data/01-12/attacks_labeled/UDPLag_labeled.csv'   \n",
    "]\n",
    "\n",
    "csv_files3 = [\n",
    "    'data/Full_dataset/03-11/LDAP.csv',\n",
    "    'data/Full_dataset/03-11/Mssql.csv',\n",
    "    'data/Full_dataset/03-11/NetBIOS.csv',\n",
    "    'data/Full_dataset/03-11/Syn.csv',\n",
    "    'data/Full_dataset/03-11/UDP.csv',\n",
    "    'data/Full_dataset/03-11/UDPLag.csv'\n",
    "]\n",
    "\n",
    "csv_files4 = [\n",
    "    'data/Full_dataset/01-12/DrDos_DNS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_LDAP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_MSSQL.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NetBIOS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NTP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SNMP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SSDP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_UDP.csv',\n",
    "    'data/Full_dataset/01-12/Syn.csv',\n",
    "    'data/Full_dataset/01-12/UDPLag.csv'   \n",
    "]\n",
    "    \n",
    "contar_rotulos_csv(csv_files3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def visualizar_primeiras_linhas_formatado(caminho_csv, n_linhas=3):\n",
    "    try:\n",
    "        print(f\"\\nüìÇ Lendo primeiras {n_linhas} linhas do arquivo: {caminho_csv}\")\n",
    "        df = pd.read_csv(caminho_csv, nrows=n_linhas, low_memory=False)\n",
    "\n",
    "        for idx, linha in df.iterrows():\n",
    "            print(f\"\\nüßæ Linha {idx + 1}:\")\n",
    "            for coluna, valor in linha.items():\n",
    "                print(f\"  {coluna:<30} ‚ûú {valor}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao ler o arquivo {caminho_csv}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Lendo primeiras 1 linhas do arquivo: data/Full_dataset/01-12/DrDos_LDAP.csv\n",
      "\n",
      "üßæ Linha 1:\n",
      "  Unnamed: 0                     ‚ûú 21010\n",
      "  Flow ID                        ‚ûú 172.16.0.5-192.168.50.1-0-0-0\n",
      "   Source IP                     ‚ûú 172.16.0.5\n",
      "   Source Port                   ‚ûú 0\n",
      "   Destination IP                ‚ûú 192.168.50.1\n",
      "   Destination Port              ‚ûú 0\n",
      "   Protocol                      ‚ûú 0\n",
      "   Timestamp                     ‚ûú 2018-12-01 11:22:40.254769\n",
      "   Flow Duration                 ‚ûú 9141643\n",
      "   Total Fwd Packets             ‚ûú 85894\n",
      "   Total Backward Packets        ‚ûú 28\n",
      "  Total Length of Fwd Packets    ‚ûú 0.0\n",
      "   Total Length of Bwd Packets   ‚ûú 0.0\n",
      "   Fwd Packet Length Max         ‚ûú 0.0\n",
      "   Fwd Packet Length Min         ‚ûú 0.0\n",
      "   Fwd Packet Length Mean        ‚ûú 0.0\n",
      "   Fwd Packet Length Std         ‚ûú 0.0\n",
      "  Bwd Packet Length Max          ‚ûú 0.0\n",
      "   Bwd Packet Length Min         ‚ûú 0.0\n",
      "   Bwd Packet Length Mean        ‚ûú 0.0\n",
      "   Bwd Packet Length Std         ‚ûú 0.0\n",
      "  Flow Bytes/s                   ‚ûú 0.0\n",
      "   Flow Packets/s                ‚ûú 9398.966903433004\n",
      "   Flow IAT Mean                 ‚ûú 106.39591019657458\n",
      "   Flow IAT Std                  ‚ûú 209.9051587990212\n",
      "   Flow IAT Max                  ‚ûú 2968.0\n",
      "   Flow IAT Min                  ‚ûú 0.0\n",
      "  Fwd IAT Total                  ‚ûú 9141643.0\n",
      "   Fwd IAT Mean                  ‚ûú 106.4305938784295\n",
      "   Fwd IAT Std                   ‚ûú 209.9465295818024\n",
      "   Fwd IAT Max                   ‚ûú 2968.0\n",
      "   Fwd IAT Min                   ‚ûú 0.0\n",
      "  Bwd IAT Total                  ‚ûú 8487477.0\n",
      "   Bwd IAT Mean                  ‚ûú 314351.0\n",
      "   Bwd IAT Std                   ‚ûú 1147262.6944003915\n",
      "   Bwd IAT Max                   ‚ûú 5975703.0\n",
      "   Bwd IAT Min                   ‚ûú 0.0\n",
      "  Fwd PSH Flags                  ‚ûú 0\n",
      "   Bwd PSH Flags                 ‚ûú 0\n",
      "   Fwd URG Flags                 ‚ûú 0\n",
      "   Bwd URG Flags                 ‚ûú 0\n",
      "   Fwd Header Length             ‚ûú 0\n",
      "   Bwd Header Length             ‚ûú 0\n",
      "  Fwd Packets/s                  ‚ûú 9395.90399668856\n",
      "   Bwd Packets/s                 ‚ûú 3.0629067444440787\n",
      "   Min Packet Length             ‚ûú 0.0\n",
      "   Max Packet Length             ‚ûú 0.0\n",
      "   Packet Length Mean            ‚ûú 0.0\n",
      "   Packet Length Std             ‚ûú 0.0\n",
      "   Packet Length Variance        ‚ûú 0.0\n",
      "  FIN Flag Count                 ‚ûú 0\n",
      "   SYN Flag Count                ‚ûú 0\n",
      "   RST Flag Count                ‚ûú 0\n",
      "   PSH Flag Count                ‚ûú 0\n",
      "   ACK Flag Count                ‚ûú 0\n",
      "   URG Flag Count                ‚ûú 0\n",
      "   CWE Flag Count                ‚ûú 0\n",
      "   ECE Flag Count                ‚ûú 0\n",
      "   Down/Up Ratio                 ‚ûú 0.0\n",
      "   Average Packet Size           ‚ûú 0.0\n",
      "   Avg Fwd Segment Size          ‚ûú 0.0\n",
      "   Avg Bwd Segment Size          ‚ûú 0.0\n",
      "   Fwd Header Length.1           ‚ûú 0\n",
      "  Fwd Avg Bytes/Bulk             ‚ûú 0\n",
      "   Fwd Avg Packets/Bulk          ‚ûú 0\n",
      "   Fwd Avg Bulk Rate             ‚ûú 0\n",
      "   Bwd Avg Bytes/Bulk            ‚ûú 0\n",
      "   Bwd Avg Packets/Bulk          ‚ûú 0\n",
      "  Bwd Avg Bulk Rate              ‚ûú 0\n",
      "  Subflow Fwd Packets            ‚ûú 85894\n",
      "   Subflow Fwd Bytes             ‚ûú 0\n",
      "   Subflow Bwd Packets           ‚ûú 28\n",
      "   Subflow Bwd Bytes             ‚ûú 0\n",
      "  Init_Win_bytes_forward         ‚ûú -1\n",
      "   Init_Win_bytes_backward       ‚ûú -1\n",
      "   act_data_pkt_fwd              ‚ûú 0\n",
      "   min_seg_size_forward          ‚ûú 0\n",
      "  Active Mean                    ‚ûú 0.0\n",
      "   Active Std                    ‚ûú 0.0\n",
      "   Active Max                    ‚ûú 0.0\n",
      "   Active Min                    ‚ûú 0.0\n",
      "  Idle Mean                      ‚ûú 0.0\n",
      "   Idle Std                      ‚ûú 0.0\n",
      "   Idle Max                      ‚ûú 0.0\n",
      "   Idle Min                      ‚ûú 0.0\n",
      "  SimillarHTTP                   ‚ûú 0\n",
      "   Inbound                       ‚ûú 1\n",
      "   Label                         ‚ûú DrDoS_LDAP\n"
     ]
    }
   ],
   "source": [
    "csv_files4 = [\n",
    "    'data/Full_dataset/01-12/DrDos_DNS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_LDAP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_MSSQL.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NetBIOS.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_NTP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SNMP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_SSDP.csv',\n",
    "    'data/Full_dataset/01-12/DrDos_UDP.csv',\n",
    "    'data/Full_dataset/01-12/Syn.csv',\n",
    "    'data/Full_dataset/01-12/UDPLag.csv'   \n",
    "]\n",
    "\n",
    "visualizar_primeiras_linhas_formatado('data/Full_dataset/01-12/DrDos_LDAP.csv', n_linhas=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corre√ß√£o do CICDDoS - Calculo da entropia e concatena√ß√£o de tempo em 1 segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise de correla√ß√£o (Random Forest - RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_csv_with_feature_importance(file_paths, label_column='label', chunk_size=5000, output_file='importancia_colunas_todas.csv'):\n",
    "    importance_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\nüîÑ Processando arquivo: {file_path}\")\n",
    "        try:\n",
    "            reader = pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
    "            importance_sum = {}\n",
    "            total_chunks = 0\n",
    "\n",
    "            for i, chunk in enumerate(reader):\n",
    "                print(f\"  üì¶ Chunk {i+1}\")\n",
    "                chunk.dropna(axis=1, how='all', inplace=True)\n",
    "                numeric_chunk = chunk.select_dtypes(include=[np.number])\n",
    "\n",
    "                if label_column not in numeric_chunk.columns:\n",
    "                    print(f\"  ‚ö†Ô∏è Coluna '{label_column}' n√£o encontrada, pulando chunk.\")\n",
    "                    continue\n",
    "\n",
    "                X = numeric_chunk.drop(columns=[label_column])\n",
    "                y = numeric_chunk[label_column]\n",
    "\n",
    "                X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                X.dropna(inplace=True)\n",
    "                y = y.loc[X.index]\n",
    "\n",
    "                if len(X) < 10 or len(set(y)) < 2:\n",
    "                    print(\"  ‚ö†Ô∏è Poucos dados ou classes √∫nicas, pulando chunk.\")\n",
    "                    continue\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "                model = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "                model.fit(X_scaled, y)\n",
    "\n",
    "                importances = model.feature_importances_\n",
    "                total_importance = importances.sum()\n",
    "                if total_importance == 0:\n",
    "                    print(\"  ‚ö†Ô∏è Import√¢ncia total zero, pulando.\")\n",
    "                    continue\n",
    "\n",
    "                importances_percent = 100.0 * (importances / total_importance)\n",
    "\n",
    "                for col, imp in zip(X.columns, importances_percent):\n",
    "                    importance_sum[col] = importance_sum.get(col, 0) + imp\n",
    "\n",
    "                print(f\"    ‚úÖ Chunk {i+1} processado com {len(X.columns)} colunas.\")\n",
    "                total_chunks += 1\n",
    "\n",
    "            if total_chunks == 0:\n",
    "                print(\"‚ö†Ô∏è Nenhum chunk v√°lido neste arquivo.\")\n",
    "                continue\n",
    "\n",
    "            avg_importances = {col: imp / total_chunks for col, imp in importance_sum.items()}\n",
    "            all_columns = sorted(avg_importances.keys())\n",
    "            file_name = os.path.basename(file_path)\n",
    "\n",
    "            row = {'arquivo': file_name}\n",
    "            for col in all_columns:\n",
    "                row[col] = avg_importances.get(col, 0.0)\n",
    "\n",
    "            importance_data.append(row)\n",
    "            print(f\"üìä Import√¢ncias m√©dias para '{file_name}':\")\n",
    "            for k, v in row.items():\n",
    "                if k != 'arquivo':\n",
    "                    print(f\"    {k}: {v:.2f}%\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {file_path}: {e}\")\n",
    "\n",
    "    if importance_data:\n",
    "        df_output = pd.DataFrame(importance_data).fillna(0)\n",
    "        df_output.to_csv(output_file, index=False)\n",
    "        print(f\"\\n‚úÖ Arquivo final salvo como: {output_file}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum dado foi processado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: data/03-11/attacks_labeled/LDAP_labeled.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/03-11/attacks_labeled/LDAP_labeled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/03-11/attacks_labeled/Mssql_labeled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/03-11/attacks_labeled/UDPLag_labeled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m \u001b[43manalyze_csv_with_rfe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimportancia_colunas_percentual.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m, in \u001b[0;36manalyze_csv_with_rfe\u001b[1;34m(file_paths, label_column, top_k, chunk_size, output_file)\u001b[0m\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     42\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(estimator\u001b[38;5;241m=\u001b[39mmodel, n_features_to_select\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns[rfe\u001b[38;5;241m.\u001b[39msupport_]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Re-treinar com colunas selecionadas para obter import√¢ncias\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:276\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m Bunch(estimator\u001b[38;5;241m=\u001b[39mBunch(fit\u001b[38;5;241m=\u001b[39mfit_params))\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:332\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 332\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    335\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    336\u001b[0m     estimator,\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    338\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    198\u001b[0m         X,\n\u001b[0;32m    199\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    203\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:239\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    238\u001b[0m ):\n\u001b[1;32m--> 239\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_random_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;66;03m# Need to validate separately here.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# We can't pass multi_output=True because that would allow y to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask will check for finite values and\u001b[39;00m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;66;03m# compute the missing mask if the tree supports missing values\u001b[39;00m\n\u001b[0;32m    248\u001b[0m         check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    249\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    250\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1515\u001b[0m, in \u001b[0;36mcheck_random_state\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmtrand\u001b[38;5;241m.\u001b[39m_rand\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[1;32m-> 1515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState):\n\u001b[0;32m   1517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seed\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:184\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_mt19937.pyx:132\u001b[0m, in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    'data/03-11/attacks_labeled/LDAP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Mssql_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/NetBIOS_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/Syn_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDP_labeled.csv',\n",
    "    'data/03-11/attacks_labeled/UDPLag_labeled.csv'\n",
    "]\n",
    "\n",
    "\n",
    "analyze_csv_with_rfe(csv_files, output_file='importancia_colunas_percentual.csv', label_column='label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, path, sequence_length, column_to_remove=None):\n",
    "        df = pd.read_csv(path, sep=';')\n",
    "\n",
    "        if column_to_remove and column_to_remove in df.columns:\n",
    "            df = df.drop(columns=[column_to_remove])\n",
    "\n",
    "        # Separar features e labels\n",
    "        features = df.iloc[:, :-1].values  # Assume que a √∫ltima coluna √© a label\n",
    "        labels = df.iloc[:, -1].values\n",
    "\n",
    "        # Normalizar os dados \n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "\n",
    "        # Criar sequ√™ncias\n",
    "        sequences = []\n",
    "        sequence_labels = []\n",
    "        for i in range(len(features) - sequence_length + 1):\n",
    "            seq = features[i:i+sequence_length]\n",
    "            label = labels[i+sequence_length-1]  \n",
    "            sequences.append(seq)\n",
    "            sequence_labels.append(label)\n",
    "\n",
    "        # Converter para tensores\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32).view(-1, sequence_length, features.shape[1])\n",
    "        self.labels = torch.tensor(sequence_labels, dtype=torch.long)\n",
    "        # self.labels = torch.tensor(sequence_labels, dtype=torch.float32).unsqueeze(1) # Para bin√°rio / nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n",
    "    \n",
    "    \n",
    "class LSTM_model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Primeira camada LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,\n",
    "                             hidden_size=hidden_size//2,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True\n",
    "                             \n",
    "                             )\n",
    "        \n",
    "        # Segunda camada LSTM\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size//2,\n",
    "                             hidden_size=hidden_size,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,  \n",
    "                             dropout=0.2  # Dropout entre as camadas LSTM                           \n",
    "                            )\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_size,\n",
    "                             hidden_size=hidden_size//2,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.2  # Dropout entre as camadas LSTM\n",
    "                             )\n",
    "        \n",
    "        # Camada fully-connected\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Para bin√°rio\n",
    "        # self.fc = torch.nn.Linear(hidden_size, 1) # nn.BCEWithLogitsLoss() / nn.BCELoss()\n",
    "        self.fc = torch.nn.Linear(hidden_size//2, output_size) # nn.CrossEntropyLoss()\n",
    "        # Camada de ativa√ß√£o softmax\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)  # Primeira camada LSTM\n",
    "      \n",
    "        out, _ = self.lstm2(out)  # Segunda camada LSTM\n",
    "        \n",
    "        out, _ = self.lstm3(out)  # Terceira camada LSTM\n",
    "        \n",
    "        out = torch.sigmoid(out)  # Aplicar sigmoid para obter probabilidades / Remover para nn.BCEWithLogitsLoss()\n",
    "        # out = self.softmax(out)  # Aplicar softmax para obter probabilidades / Remover para nn.BCEWithLogitsLoss()\n",
    "        out = self.fc(out[:, -1, :])  # Usar a √∫ltima sa√≠da do LSTM como entrada para fc\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "        print(f\"Modelo salvo em: {filename}\")\n",
    "\n",
    "\n",
    "# out = self.dropout(out)  # Aplicar dropout se necess√°rio\n",
    "# out = self.batch_norm(out)  # Aplicar batch normalization se necess√°rio\n",
    "# out = self.relu(out)  # Aplicar ReLU se necess√°rio\n",
    "# out = self.tanh(out)  # Aplicar Tanh se necess√°rio\n",
    "# out = self.sigmoid(out)  # Aplicar Sigmoid se necess√°rio\n",
    "# out = self.leaky_relu(out)  # Aplicar Leaky ReLU se necess√°rio\n",
    "# out = self.prelu(out)  # Aplicar PReLU se necess√°rio\n",
    "# out = self.elu(out)  # Aplicar ELU se necess√°rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader - Inputs shape: torch.Size([64, 1, 9]), Labels shape: torch.Size([64])\n",
      "Test Loader - Inputs shape: torch.Size([64, 1, 9]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Configurar os par√¢metros da rede LSTM\n",
    "input_size = 9         # N√∫mero de features no dataset / Tamanho do vetor de entrada por tempo\n",
    "hidden_size = 256       # Tamanho do hidden state / N¬∫ de unidades ocultas por c√©lula\n",
    "num_layers = 2         # N√∫mero de camadas LSTM / N¬∫ de camadas LSTM empilhadas\n",
    "output_size = 2        # Classes: normal (0), anomalia (1) \n",
    "batch_size = 64        # Batch size / \n",
    "num_epochs = 100         # N√∫mero de epochs\n",
    "lr = 0.0011             # Learning rate\n",
    "sequence_length = 1   # Tamanho da sequ√™ncia de entrada para a LSTM\n",
    "column_to_remove = 'attack_name'  # Coluna a ser removida\n",
    "\n",
    "# Criar os datasets\n",
    "train_dataset = SequenceDataset('data/cic_puro/treino_final_estratificado_random.csv', sequence_length, column_to_remove)\n",
    "test_dataset = SequenceDataset('data/cic_puro/teste_final_estratificado_random.csv', sequence_length, column_to_remove)\n",
    "\n",
    "# Criar os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Formato dos loaders\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Train Loader - Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Test Loader - Inputs shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_model(\n",
      "  (lstm1): LSTM(9, 128, num_layers=2, batch_first=True)\n",
      "  (lstm2): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (lstm3): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Dimens√£o da entrada: torch.Size([64, 1, 9])\n",
      "Dimens√£o da sa√≠da: torch.Size([64])\n",
      "Dimens√£o da sa√≠da do modelo: torch.Size([64, 2])\n",
      "Epoch [1/100], Loss: 0.4529, Val Loss: 0.8944, Accuracy: 0.4654\n",
      "Melhor modelo salvo!\n",
      "Epoch [2/100], Loss: 0.3712, Val Loss: 0.7187, Accuracy: 0.5864\n",
      "Melhor modelo salvo!\n",
      "Epoch [3/100], Loss: 0.4228, Val Loss: 0.5879, Accuracy: 0.7041\n",
      "Melhor modelo salvo!\n",
      "Epoch [4/100], Loss: 0.2935, Val Loss: 0.9191, Accuracy: 0.5445\n",
      "Epoch [5/100], Loss: 0.4467, Val Loss: 0.7766, Accuracy: 0.6225\n",
      "Epoch [6/100], Loss: 0.3432, Val Loss: 0.7228, Accuracy: 0.6538\n",
      "Epoch [7/100], Loss: 0.2184, Val Loss: 0.6046, Accuracy: 0.6659\n",
      "Epoch [8/100], Loss: 0.2359, Val Loss: 0.4953, Accuracy: 0.7686\n",
      "Melhor modelo salvo!\n",
      "Epoch [9/100], Loss: 0.2682, Val Loss: 0.5518, Accuracy: 0.7944\n",
      "Epoch [10/100], Loss: 0.2680, Val Loss: 0.5005, Accuracy: 0.7794\n",
      "Epoch [11/100], Loss: 0.1998, Val Loss: 0.5326, Accuracy: 0.8034\n",
      "Epoch [12/100], Loss: 0.3016, Val Loss: 0.5341, Accuracy: 0.7984\n",
      "Epoch [13/100], Loss: 0.2001, Val Loss: 0.4714, Accuracy: 0.8265\n",
      "Melhor modelo salvo!\n",
      "Epoch [14/100], Loss: 0.2007, Val Loss: 0.4505, Accuracy: 0.8265\n",
      "Melhor modelo salvo!\n",
      "Epoch [15/100], Loss: 0.2051, Val Loss: 0.4261, Accuracy: 0.8433\n",
      "Melhor modelo salvo!\n",
      "Epoch [16/100], Loss: 0.2579, Val Loss: 0.5191, Accuracy: 0.7983\n",
      "Epoch [17/100], Loss: 0.1826, Val Loss: 0.5191, Accuracy: 0.8379\n",
      "Epoch [18/100], Loss: 0.2479, Val Loss: 0.4665, Accuracy: 0.8290\n",
      "Epoch [19/100], Loss: 0.1912, Val Loss: 0.5088, Accuracy: 0.8285\n",
      "Epoch [20/100], Loss: 0.4203, Val Loss: 0.4472, Accuracy: 0.8525\n",
      "Epoch [21/100], Loss: 0.2189, Val Loss: 0.5225, Accuracy: 0.8451\n",
      "Epoch [22/100], Loss: 0.2013, Val Loss: 0.4905, Accuracy: 0.8449\n",
      "Epoch [23/100], Loss: 0.1649, Val Loss: 0.5073, Accuracy: 0.8319\n",
      "Epoch [24/100], Loss: 0.2598, Val Loss: 0.5309, Accuracy: 0.8355\n",
      "Epoch [25/100], Loss: 0.2067, Val Loss: 0.5012, Accuracy: 0.8301\n",
      "Epoch [26/100], Loss: 0.2301, Val Loss: 0.5490, Accuracy: 0.8286\n",
      "Epoch [27/100], Loss: 0.1349, Val Loss: 0.6600, Accuracy: 0.7743\n",
      "Epoch [28/100], Loss: 0.2314, Val Loss: 0.4987, Accuracy: 0.8172\n",
      "Epoch [29/100], Loss: 0.2482, Val Loss: 0.6339, Accuracy: 0.7474\n",
      "Epoch [30/100], Loss: 0.2281, Val Loss: 0.5454, Accuracy: 0.8143\n",
      "Epoch [31/100], Loss: 0.2001, Val Loss: 0.5154, Accuracy: 0.8195\n",
      "Epoch [32/100], Loss: 0.2861, Val Loss: 0.4970, Accuracy: 0.8085\n",
      "Epoch [33/100], Loss: 0.2350, Val Loss: 0.5891, Accuracy: 0.8006\n",
      "Epoch [34/100], Loss: 0.1649, Val Loss: 0.6676, Accuracy: 0.7746\n",
      "Epoch [35/100], Loss: 0.2630, Val Loss: 0.5974, Accuracy: 0.8006\n",
      "Epoch [36/100], Loss: 0.1488, Val Loss: 0.6979, Accuracy: 0.7628\n",
      "Epoch [37/100], Loss: 0.1134, Val Loss: 0.8045, Accuracy: 0.7554\n",
      "Epoch [38/100], Loss: 0.1802, Val Loss: 0.7584, Accuracy: 0.7753\n",
      "Epoch [39/100], Loss: 0.1743, Val Loss: 0.6824, Accuracy: 0.7809\n",
      "Epoch [40/100], Loss: 0.1232, Val Loss: 0.7726, Accuracy: 0.7838\n",
      "Epoch [41/100], Loss: 0.3256, Val Loss: 0.6867, Accuracy: 0.7653\n",
      "Epoch [42/100], Loss: 0.2321, Val Loss: 0.8652, Accuracy: 0.7674\n",
      "Epoch [43/100], Loss: 0.3512, Val Loss: 0.8105, Accuracy: 0.7681\n",
      "Epoch [44/100], Loss: 0.2419, Val Loss: 0.8076, Accuracy: 0.7487\n",
      "Epoch [45/100], Loss: 0.2782, Val Loss: 0.8433, Accuracy: 0.7556\n",
      "Epoch [46/100], Loss: 0.2519, Val Loss: 0.7498, Accuracy: 0.7718\n",
      "Epoch [47/100], Loss: 0.2783, Val Loss: 0.8171, Accuracy: 0.7652\n",
      "Epoch [48/100], Loss: 0.2934, Val Loss: 0.9520, Accuracy: 0.7521\n",
      "Epoch [49/100], Loss: 0.1380, Val Loss: 0.8541, Accuracy: 0.7563\n",
      "Epoch [50/100], Loss: 0.1832, Val Loss: 0.9031, Accuracy: 0.7595\n",
      "Epoch [51/100], Loss: 0.2300, Val Loss: 0.8494, Accuracy: 0.7666\n",
      "Epoch [52/100], Loss: 0.1675, Val Loss: 0.5982, Accuracy: 0.8185\n",
      "Epoch [53/100], Loss: 0.2019, Val Loss: 0.8313, Accuracy: 0.7560\n",
      "Epoch [54/100], Loss: 0.1762, Val Loss: 0.8372, Accuracy: 0.7652\n",
      "Epoch [55/100], Loss: 0.2996, Val Loss: 0.8138, Accuracy: 0.7609\n",
      "Epoch [56/100], Loss: 0.2089, Val Loss: 0.9199, Accuracy: 0.7557\n",
      "Epoch [57/100], Loss: 0.2739, Val Loss: 0.8316, Accuracy: 0.7671\n",
      "Epoch [58/100], Loss: 0.2469, Val Loss: 0.8775, Accuracy: 0.7651\n",
      "Epoch [59/100], Loss: 0.2369, Val Loss: 0.8349, Accuracy: 0.7630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels) \u001b[38;5;66;03m#  nn.BCELoss()\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Valida√ß√£o\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Criar o modelo\n",
    "model = LSTM_model(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "print(model)\n",
    "\n",
    "# Testar a dimens√£o da entrada e sa√≠da do modelo\n",
    "x, y = next(iter(train_loader))  \n",
    "output = model(x)\n",
    "print(f\"Dimens√£o da entrada: {x.size()}\")   # shape: [batch_size, sequence_length, input_size]\n",
    "print(f\"Dimens√£o da sa√≠da: {y.size()}\")    # shape: [batch_size]\n",
    "print(f\"Dimens√£o da sa√≠da do modelo: {output.size()}\") # shape: [batch_size, output_size]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss: Para classifica√ß√£o multi-classe (softmax j√° inclu√≠do)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # Adam: muito usado, bom para a maioria dos casos\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=lr) # RMSprop: bom para dados sequenciais e LSTM\n",
    "\n",
    "\n",
    "# Treinamento do modelo\n",
    "best_loss = float('inf') # Inicializa a melhor perda como infinito\n",
    "for epoch in range(num_epochs):\n",
    "    # Treinamento\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.permute(0, 1, 2)\n",
    "        outputs = model(inputs)   \n",
    "        # labels = labels.view(-1, 1).float() # Ajustar o formato de labels para [batch_size, 1] / nn.BCEWithLogitsLoss()\n",
    "        # loss = criterion(outputs.squeeze(1), labels.float()) # nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(outputs.squeeze(), labels) #  nn.BCELoss()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Valida√ß√£o\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.permute(0, 1, 2)\n",
    "            outputs = model(inputs)\n",
    "            # labels = labels.view(-1, 1).float() # Ajustar o formato de labels para [batch_size, 1] / nn.BCEWithLogitsLoss()\n",
    "            val_loss += criterion(outputs.squeeze(), labels).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    # Calcular a acur√°cia\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.permute(0, 1, 2)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "   \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {acc:.4f}')\n",
    "    \n",
    "    # Salvar o melhor modelo\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f'best_lstm_model_{acc:.2f}.pth')\n",
    "        print('Melhor modelo salvo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo em uso: cuda\n"
     ]
    }
   ],
   "source": [
    "# üì¶ C√âLULA 1 ‚Äì IMPORTA√á√ïES E CONFIGURA√á√ïES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo em uso: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ C√âLULA 2 ‚Äì DATASET COM REMO√á√ÉO DE COLUNA E DIVIS√ÉO EM SEQU√äNCIAS\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, path, sequence_length, column_to_remove=None):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Remover coluna opcional (como timestamp ou id)\n",
    "        if column_to_remove and column_to_remove in df.columns:\n",
    "            df = df.drop(columns=[column_to_remove])\n",
    "\n",
    "        # Separar features e labels\n",
    "        data = df.drop(columns=['label']).values\n",
    "        labels = df['label'].values\n",
    "\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i in range(len(data) - sequence_length + 1):\n",
    "            self.sequences.append(data[i:i+sequence_length])\n",
    "            self.labels.append(labels[i+sequence_length-1])\n",
    "\n",
    "        # Formato final: (batch, channels=1, time_steps)\n",
    "        self.sequences = torch.tensor(self.sequences, dtype=torch.float32).unsqueeze(1)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† C√âLULA 3 ‚Äì DEFINI√á√ÉO DO MODELO CNN 1D\n",
    "class CNN1DNet(nn.Module):\n",
    "    def __init__(self, input_length, output_size=2):\n",
    "        super(CNN1DNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_length // 4) * 64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "input_size = 9     \n",
    "output_size = 2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "lr = 0.001\n",
    "column_to_remove = 'attack_name'\n",
    "\n",
    "train_dataset = SequenceDataset('data/cic_puro_enhanced/01-12-train.csv', sequence_length, column_to_remove)\n",
    "test_dataset  = SequenceDataset('data/cic_puro_enhanced/03-11-test.csv', sequence_length, column_to_remove)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 20, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     11\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mCNN1DNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leand\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 20, 9]"
     ]
    }
   ],
   "source": [
    "model = CNN1DNet(input_length=sequence_length, output_size=output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # üîç Valida√ß√£o\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predicted = torch.argmax(outputs, dim=1).cpu()\n",
    "            y_pred.extend(predicted.numpy())\n",
    "            y_true.extend(y_batch.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"√âpoca {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(\"‚úÖ Novo melhor modelo salvo com accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-CNN-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_model(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=64, num_layers=2, output_size=2):\n",
    "        super(CNN_LSTM_model, self).__init__()\n",
    "        \n",
    "        # Camada CNN para extrair caracter√≠sticas locais\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Camada LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # CamadaÂÖ®ËøûÊé• ap√≥s a LSTM\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # A CNN espera (batch_size, channels, sequence_length), ent√£o precisamos transpor\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, seq_length, input_size] -> [batch_size, input_size, seq_length]\n",
    "        \n",
    "        out = self.cnn(x)\n",
    "        out = out.permute(0, 2, 1)  # Volta para [batch_size, sequence_length, channels]\n",
    "        \n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc(out[:, -1, :])  # Pega o √∫ltimo timestep\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
